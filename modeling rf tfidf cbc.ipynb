{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = pd.read_csv(\"df_cbc_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'tfidf_0',\n",
       " 'tfidf_1',\n",
       " 'tfidf_2',\n",
       " 'tfidf_3',\n",
       " 'tfidf_4',\n",
       " 'tfidf_5',\n",
       " 'tfidf_6',\n",
       " 'tfidf_7',\n",
       " 'tfidf_8',\n",
       " 'tfidf_9',\n",
       " 'tfidf_10',\n",
       " 'tfidf_11',\n",
       " 'tfidf_12',\n",
       " 'tfidf_13',\n",
       " 'tfidf_14',\n",
       " 'tfidf_15',\n",
       " 'tfidf_16',\n",
       " 'tfidf_17',\n",
       " 'tfidf_18',\n",
       " 'tfidf_19',\n",
       " 'tfidf_20',\n",
       " 'tfidf_21',\n",
       " 'tfidf_22',\n",
       " 'tfidf_23',\n",
       " 'tfidf_24',\n",
       " 'tfidf_25',\n",
       " 'tfidf_26',\n",
       " 'tfidf_27',\n",
       " 'tfidf_28',\n",
       " 'tfidf_29',\n",
       " 'tfidf_30',\n",
       " 'tfidf_31',\n",
       " 'tfidf_32',\n",
       " 'tfidf_33',\n",
       " 'tfidf_34',\n",
       " 'tfidf_35',\n",
       " 'tfidf_36',\n",
       " 'tfidf_37',\n",
       " 'tfidf_38',\n",
       " 'tfidf_39',\n",
       " 'tfidf_40',\n",
       " 'tfidf_41',\n",
       " 'tfidf_42',\n",
       " 'tfidf_43',\n",
       " 'tfidf_44',\n",
       " 'tfidf_45',\n",
       " 'tfidf_46',\n",
       " 'tfidf_47',\n",
       " 'tfidf_48',\n",
       " 'tfidf_49',\n",
       " 'tfidf_50',\n",
       " 'tfidf_51',\n",
       " 'tfidf_52',\n",
       " 'tfidf_53',\n",
       " 'tfidf_54',\n",
       " 'tfidf_55',\n",
       " 'tfidf_56',\n",
       " 'tfidf_57',\n",
       " 'tfidf_58',\n",
       " 'tfidf_59',\n",
       " 'tfidf_60',\n",
       " 'tfidf_61',\n",
       " 'tfidf_62',\n",
       " 'tfidf_63',\n",
       " 'tfidf_64',\n",
       " 'tfidf_65',\n",
       " 'tfidf_66',\n",
       " 'tfidf_67',\n",
       " 'tfidf_68',\n",
       " 'tfidf_69',\n",
       " 'tfidf_70',\n",
       " 'tfidf_71',\n",
       " 'tfidf_72',\n",
       " 'tfidf_73',\n",
       " 'tfidf_74',\n",
       " 'tfidf_75',\n",
       " 'tfidf_76',\n",
       " 'tfidf_77',\n",
       " 'tfidf_78',\n",
       " 'tfidf_79',\n",
       " 'tfidf_80',\n",
       " 'tfidf_81',\n",
       " 'tfidf_82',\n",
       " 'tfidf_83',\n",
       " 'tfidf_84',\n",
       " 'tfidf_85',\n",
       " 'tfidf_86',\n",
       " 'tfidf_87',\n",
       " 'tfidf_88',\n",
       " 'tfidf_89',\n",
       " 'tfidf_90',\n",
       " 'tfidf_91',\n",
       " 'tfidf_92',\n",
       " 'tfidf_93',\n",
       " 'tfidf_94',\n",
       " 'tfidf_95',\n",
       " 'tfidf_96',\n",
       " 'tfidf_97',\n",
       " 'tfidf_98',\n",
       " 'tfidf_99',\n",
       " 'tfidf_100',\n",
       " 'tfidf_101',\n",
       " 'tfidf_102',\n",
       " 'tfidf_103',\n",
       " 'tfidf_104',\n",
       " 'tfidf_105',\n",
       " 'tfidf_106',\n",
       " 'tfidf_107',\n",
       " 'tfidf_108',\n",
       " 'tfidf_109',\n",
       " 'tfidf_110',\n",
       " 'tfidf_111',\n",
       " 'tfidf_112',\n",
       " 'tfidf_113',\n",
       " 'tfidf_114',\n",
       " 'tfidf_115',\n",
       " 'tfidf_116',\n",
       " 'tfidf_117',\n",
       " 'tfidf_118',\n",
       " 'tfidf_119',\n",
       " 'tfidf_120',\n",
       " 'tfidf_121',\n",
       " 'tfidf_122',\n",
       " 'tfidf_123',\n",
       " 'tfidf_124',\n",
       " 'tfidf_125',\n",
       " 'tfidf_126',\n",
       " 'tfidf_127',\n",
       " 'tfidf_128',\n",
       " 'tfidf_129',\n",
       " 'tfidf_130',\n",
       " 'tfidf_131',\n",
       " 'tfidf_132',\n",
       " 'tfidf_133',\n",
       " 'tfidf_134',\n",
       " 'tfidf_135',\n",
       " 'tfidf_136',\n",
       " 'tfidf_137',\n",
       " 'tfidf_138',\n",
       " 'tfidf_139',\n",
       " 'tfidf_140',\n",
       " 'tfidf_141',\n",
       " 'tfidf_142',\n",
       " 'tfidf_143',\n",
       " 'tfidf_144',\n",
       " 'tfidf_145',\n",
       " 'tfidf_146',\n",
       " 'tfidf_147',\n",
       " 'tfidf_148',\n",
       " 'tfidf_149',\n",
       " 'tfidf_150',\n",
       " 'tfidf_151',\n",
       " 'tfidf_152',\n",
       " 'tfidf_153',\n",
       " 'tfidf_154',\n",
       " 'tfidf_155',\n",
       " 'tfidf_156',\n",
       " 'tfidf_157',\n",
       " 'tfidf_158',\n",
       " 'tfidf_159',\n",
       " 'tfidf_160',\n",
       " 'tfidf_161',\n",
       " 'tfidf_162',\n",
       " 'tfidf_163',\n",
       " 'tfidf_164',\n",
       " 'tfidf_165',\n",
       " 'tfidf_166',\n",
       " 'tfidf_167',\n",
       " 'tfidf_168',\n",
       " 'tfidf_169',\n",
       " 'tfidf_170',\n",
       " 'tfidf_171',\n",
       " 'tfidf_172',\n",
       " 'tfidf_173',\n",
       " 'tfidf_174',\n",
       " 'tfidf_175',\n",
       " 'tfidf_176',\n",
       " 'tfidf_177',\n",
       " 'tfidf_178',\n",
       " 'tfidf_179',\n",
       " 'tfidf_180',\n",
       " 'tfidf_181',\n",
       " 'tfidf_182',\n",
       " 'tfidf_183',\n",
       " 'tfidf_184',\n",
       " 'tfidf_185',\n",
       " 'tfidf_186',\n",
       " 'tfidf_187',\n",
       " 'tfidf_188',\n",
       " 'tfidf_189',\n",
       " 'tfidf_190',\n",
       " 'tfidf_191',\n",
       " 'tfidf_192',\n",
       " 'tfidf_193',\n",
       " 'tfidf_194',\n",
       " 'tfidf_195',\n",
       " 'tfidf_196',\n",
       " 'tfidf_197',\n",
       " 'tfidf_198',\n",
       " 'tfidf_199',\n",
       " 'tfidf_200',\n",
       " 'tfidf_201',\n",
       " 'tfidf_202',\n",
       " 'tfidf_203',\n",
       " 'tfidf_204',\n",
       " 'tfidf_205',\n",
       " 'tfidf_206',\n",
       " 'tfidf_207',\n",
       " 'tfidf_208',\n",
       " 'tfidf_209',\n",
       " 'tfidf_210',\n",
       " 'tfidf_211',\n",
       " 'tfidf_212',\n",
       " 'tfidf_213',\n",
       " 'tfidf_214',\n",
       " 'tfidf_215',\n",
       " 'tfidf_216',\n",
       " 'tfidf_217',\n",
       " 'tfidf_218',\n",
       " 'tfidf_219',\n",
       " 'tfidf_220',\n",
       " 'tfidf_221',\n",
       " 'tfidf_222',\n",
       " 'tfidf_223',\n",
       " 'tfidf_224',\n",
       " 'tfidf_225',\n",
       " 'tfidf_226',\n",
       " 'tfidf_227',\n",
       " 'tfidf_228',\n",
       " 'tfidf_229',\n",
       " 'tfidf_230',\n",
       " 'tfidf_231',\n",
       " 'tfidf_232',\n",
       " 'tfidf_233',\n",
       " 'tfidf_234',\n",
       " 'tfidf_235',\n",
       " 'tfidf_236',\n",
       " 'tfidf_237',\n",
       " 'tfidf_238',\n",
       " 'tfidf_239',\n",
       " 'tfidf_240',\n",
       " 'tfidf_241',\n",
       " 'tfidf_242',\n",
       " 'tfidf_243',\n",
       " 'tfidf_244',\n",
       " 'tfidf_245',\n",
       " 'tfidf_246',\n",
       " 'tfidf_247',\n",
       " 'tfidf_248',\n",
       " 'tfidf_249',\n",
       " 'tfidf_250',\n",
       " 'tfidf_251',\n",
       " 'tfidf_252',\n",
       " 'tfidf_253',\n",
       " 'tfidf_254',\n",
       " 'tfidf_255',\n",
       " 'tfidf_256',\n",
       " 'tfidf_257',\n",
       " 'tfidf_258',\n",
       " 'tfidf_259',\n",
       " 'tfidf_260',\n",
       " 'tfidf_261',\n",
       " 'tfidf_262',\n",
       " 'tfidf_263',\n",
       " 'tfidf_264',\n",
       " 'tfidf_265',\n",
       " 'tfidf_266',\n",
       " 'tfidf_267',\n",
       " 'tfidf_268',\n",
       " 'tfidf_269',\n",
       " 'tfidf_270',\n",
       " 'tfidf_271',\n",
       " 'tfidf_272',\n",
       " 'tfidf_273',\n",
       " 'tfidf_274',\n",
       " 'tfidf_275',\n",
       " 'tfidf_276',\n",
       " 'tfidf_277',\n",
       " 'tfidf_278',\n",
       " 'tfidf_279',\n",
       " 'tfidf_280',\n",
       " 'tfidf_281',\n",
       " 'tfidf_282',\n",
       " 'tfidf_283',\n",
       " 'tfidf_284',\n",
       " 'tfidf_285',\n",
       " 'tfidf_286',\n",
       " 'tfidf_287',\n",
       " 'tfidf_288',\n",
       " 'tfidf_289',\n",
       " 'tfidf_290',\n",
       " 'tfidf_291',\n",
       " 'tfidf_292',\n",
       " 'tfidf_293',\n",
       " 'tfidf_294',\n",
       " 'tfidf_295',\n",
       " 'tfidf_296',\n",
       " 'tfidf_297',\n",
       " 'tfidf_298',\n",
       " 'tfidf_299',\n",
       " 'tfidf_300',\n",
       " 'tfidf_301',\n",
       " 'tfidf_302',\n",
       " 'tfidf_303',\n",
       " 'tfidf_304',\n",
       " 'tfidf_305',\n",
       " 'tfidf_306',\n",
       " 'tfidf_307',\n",
       " 'tfidf_308',\n",
       " 'tfidf_309',\n",
       " 'tfidf_310',\n",
       " 'tfidf_311',\n",
       " 'tfidf_312',\n",
       " 'tfidf_313',\n",
       " 'tfidf_314',\n",
       " 'tfidf_315',\n",
       " 'tfidf_316',\n",
       " 'tfidf_317',\n",
       " 'tfidf_318',\n",
       " 'tfidf_319',\n",
       " 'tfidf_320',\n",
       " 'tfidf_321',\n",
       " 'tfidf_322',\n",
       " 'tfidf_323',\n",
       " 'tfidf_324',\n",
       " 'tfidf_325',\n",
       " 'tfidf_326',\n",
       " 'tfidf_327',\n",
       " 'tfidf_328',\n",
       " 'tfidf_329',\n",
       " 'tfidf_330',\n",
       " 'tfidf_331',\n",
       " 'tfidf_332',\n",
       " 'tfidf_333',\n",
       " 'tfidf_334',\n",
       " 'tfidf_335',\n",
       " 'tfidf_336',\n",
       " 'tfidf_337',\n",
       " 'tfidf_338',\n",
       " 'tfidf_339',\n",
       " 'tfidf_340',\n",
       " 'tfidf_341',\n",
       " 'tfidf_342',\n",
       " 'tfidf_343',\n",
       " 'tfidf_344',\n",
       " 'tfidf_345',\n",
       " 'tfidf_346',\n",
       " 'tfidf_347',\n",
       " 'tfidf_348',\n",
       " 'tfidf_349',\n",
       " 'tfidf_350',\n",
       " 'tfidf_351',\n",
       " 'tfidf_352',\n",
       " 'tfidf_353',\n",
       " 'tfidf_354',\n",
       " 'tfidf_355',\n",
       " 'tfidf_356',\n",
       " 'tfidf_357',\n",
       " 'tfidf_358',\n",
       " 'tfidf_359',\n",
       " 'tfidf_360',\n",
       " 'tfidf_361',\n",
       " 'tfidf_362',\n",
       " 'tfidf_363',\n",
       " 'tfidf_364',\n",
       " 'tfidf_365',\n",
       " 'tfidf_366',\n",
       " 'tfidf_367',\n",
       " 'tfidf_368',\n",
       " 'tfidf_369',\n",
       " 'tfidf_370',\n",
       " 'tfidf_371',\n",
       " 'tfidf_372',\n",
       " 'tfidf_373',\n",
       " 'tfidf_374',\n",
       " 'tfidf_375',\n",
       " 'tfidf_376',\n",
       " 'tfidf_377',\n",
       " 'tfidf_378',\n",
       " 'tfidf_379',\n",
       " 'tfidf_380',\n",
       " 'tfidf_381',\n",
       " 'tfidf_382',\n",
       " 'tfidf_383',\n",
       " 'tfidf_384',\n",
       " 'tfidf_385',\n",
       " 'tfidf_386',\n",
       " 'tfidf_387',\n",
       " 'hybrid',\n",
       " 'indica',\n",
       " 'sativa',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'aroused',\n",
       " 'arthritis',\n",
       " 'creative',\n",
       " 'depression',\n",
       " 'dizzy',\n",
       " 'dry eyes',\n",
       " 'dry mouth',\n",
       " 'energetic',\n",
       " 'epilepsy',\n",
       " 'euphoric',\n",
       " 'eye pressure',\n",
       " 'fatigue',\n",
       " 'focused',\n",
       " 'giggly',\n",
       " 'happy',\n",
       " 'headache',\n",
       " 'hungry',\n",
       " 'migraines',\n",
       " 'pain',\n",
       " 'paranoid',\n",
       " 'relaxed',\n",
       " 'seizures',\n",
       " 'sleepy',\n",
       " 'spasticity',\n",
       " 'stress',\n",
       " 'talkative',\n",
       " 'tingly',\n",
       " 'uplifted',\n",
       " 'ammonia',\n",
       " 'apple',\n",
       " 'apricot',\n",
       " 'berry',\n",
       " 'blue cheese',\n",
       " 'blueberry',\n",
       " 'butter',\n",
       " 'cheese',\n",
       " 'chemical',\n",
       " 'chestnut',\n",
       " 'citrus',\n",
       " 'coffee',\n",
       " 'diesel',\n",
       " 'earthy',\n",
       " 'flowery',\n",
       " 'fruit',\n",
       " 'grape',\n",
       " 'grapefruit',\n",
       " 'honey',\n",
       " 'lavender',\n",
       " 'lemon',\n",
       " 'lime',\n",
       " 'mango',\n",
       " 'menthol',\n",
       " 'mint',\n",
       " 'nutty',\n",
       " 'orange',\n",
       " 'peach',\n",
       " 'pear',\n",
       " 'pepper',\n",
       " 'pine',\n",
       " 'pineapple',\n",
       " 'plum',\n",
       " 'pungent',\n",
       " 'rose',\n",
       " 'sage',\n",
       " 'skunk',\n",
       " 'spicy/herbal',\n",
       " 'strawberry',\n",
       " 'sweet',\n",
       " 'tar',\n",
       " 'tea',\n",
       " 'tobacco',\n",
       " 'tree',\n",
       " 'tropical',\n",
       " 'vanilla',\n",
       " 'violet',\n",
       " 'woody',\n",
       " 'X..CBC']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_rf.drop(['index', 'X..CBC'], axis = 1)\n",
    "y = df_rf[['X..CBC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting histograms on target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1754386 ],\n",
       "       [0.1754386 ],\n",
       "       [0.1754386 ],\n",
       "       ...,\n",
       "       [0.61403509],\n",
       "       [0.61403509],\n",
       "       [0.61403509]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array = y.to_numpy()\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(y, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF modeling (before Feature selection and Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_8091/350139188.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04664025149163046"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010441796879713486"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10218511085140283"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699557518853386"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627783533284515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots for each target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame({\n",
    "    \"features\": X_train1.columns,\n",
    "    \"score\": rfreg.feature_importances_\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.000861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.017930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     features     score\n",
       "0     tfidf_0  0.001070\n",
       "1     tfidf_1  0.002246\n",
       "2     tfidf_2  0.000447\n",
       "3     tfidf_3  0.002112\n",
       "4     tfidf_4  0.000861\n",
       "..        ...       ...\n",
       "464      tree  0.000299\n",
       "465  tropical  0.000567\n",
       "466   vanilla  0.017930\n",
       "467    violet  0.000163\n",
       "468     woody  0.001993\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_ranked = df_feat.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>diesel</td>\n",
       "      <td>0.045981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>sativa</td>\n",
       "      <td>0.028979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>tfidf_253</td>\n",
       "      <td>0.023822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>0.021857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>tfidf_329</td>\n",
       "      <td>0.020850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.017930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>orange</td>\n",
       "      <td>0.016473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>tfidf_285</td>\n",
       "      <td>0.012630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>tfidf_168</td>\n",
       "      <td>0.012605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>tfidf_145</td>\n",
       "      <td>0.012457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>lemon</td>\n",
       "      <td>0.012307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>tfidf_149</td>\n",
       "      <td>0.011801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>blueberry</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>tfidf_312</td>\n",
       "      <td>0.010385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>tfidf_239</td>\n",
       "      <td>0.010106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>tfidf_345</td>\n",
       "      <td>0.009958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>tfidf_245</td>\n",
       "      <td>0.009891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>tfidf_199</td>\n",
       "      <td>0.009245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>tfidf_141</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>tfidf_309</td>\n",
       "      <td>0.008320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>tfidf_121</td>\n",
       "      <td>0.007838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.007825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tfidf_11</td>\n",
       "      <td>0.007257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>tfidf_207</td>\n",
       "      <td>0.006915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>0.006857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>euphoric</td>\n",
       "      <td>0.006698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>tfidf_281</td>\n",
       "      <td>0.006577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>earthy</td>\n",
       "      <td>0.006515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>tfidf_367</td>\n",
       "      <td>0.006182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>relaxed</td>\n",
       "      <td>0.005832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>tfidf_210</td>\n",
       "      <td>0.005684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tfidf_37</td>\n",
       "      <td>0.005669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>pine</td>\n",
       "      <td>0.005647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>cheese</td>\n",
       "      <td>0.005578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>tfidf_209</td>\n",
       "      <td>0.005314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>tfidf_342</td>\n",
       "      <td>0.005309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tfidf_30</td>\n",
       "      <td>0.005297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>indica</td>\n",
       "      <td>0.005189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.005084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>tfidf_357</td>\n",
       "      <td>0.005021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>tfidf_314</td>\n",
       "      <td>0.005007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>tfidf_73</td>\n",
       "      <td>0.004875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>tfidf_93</td>\n",
       "      <td>0.004869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>mint</td>\n",
       "      <td>0.004804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>uplifted</td>\n",
       "      <td>0.004631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tfidf_35</td>\n",
       "      <td>0.004566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_7</td>\n",
       "      <td>0.004485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>tfidf_162</td>\n",
       "      <td>0.004402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>grape</td>\n",
       "      <td>0.004370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>tfidf_119</td>\n",
       "      <td>0.004139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>tfidf_158</td>\n",
       "      <td>0.004133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>creative</td>\n",
       "      <td>0.004082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>tfidf_78</td>\n",
       "      <td>0.004025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>tfidf_303</td>\n",
       "      <td>0.004022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>tfidf_178</td>\n",
       "      <td>0.003943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_43</td>\n",
       "      <td>0.003893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>hungry</td>\n",
       "      <td>0.003796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>sweet</td>\n",
       "      <td>0.003795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>energetic</td>\n",
       "      <td>0.003729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>tfidf_230</td>\n",
       "      <td>0.003714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>tfidf_362</td>\n",
       "      <td>0.003629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tfidf_20</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>tfidf_167</td>\n",
       "      <td>0.003471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>focused</td>\n",
       "      <td>0.003445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>dry mouth</td>\n",
       "      <td>0.003414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>tfidf_257</td>\n",
       "      <td>0.003365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>tfidf_144</td>\n",
       "      <td>0.003331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>tfidf_200</td>\n",
       "      <td>0.003318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>tfidf_258</td>\n",
       "      <td>0.003225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>tfidf_382</td>\n",
       "      <td>0.003194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tfidf_39</td>\n",
       "      <td>0.003170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>tfidf_128</td>\n",
       "      <td>0.003080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>tfidf_173</td>\n",
       "      <td>0.003077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>sleepy</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>tfidf_291</td>\n",
       "      <td>0.003052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>tfidf_374</td>\n",
       "      <td>0.003037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>tfidf_337</td>\n",
       "      <td>0.003036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>tfidf_184</td>\n",
       "      <td>0.003025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>tfidf_205</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>talkative</td>\n",
       "      <td>0.002960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tfidf_21</td>\n",
       "      <td>0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>pungent</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>tfidf_117</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>tfidf_166</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>tfidf_131</td>\n",
       "      <td>0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>tfidf_289</td>\n",
       "      <td>0.002703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>tfidf_340</td>\n",
       "      <td>0.002698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>butter</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>tfidf_124</td>\n",
       "      <td>0.002631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>tfidf_154</td>\n",
       "      <td>0.002623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>citrus</td>\n",
       "      <td>0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>tfidf_369</td>\n",
       "      <td>0.002603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>tfidf_283</td>\n",
       "      <td>0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>tfidf_237</td>\n",
       "      <td>0.002570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_5</td>\n",
       "      <td>0.002537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>tingly</td>\n",
       "      <td>0.002519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>flowery</td>\n",
       "      <td>0.002472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>tfidf_130</td>\n",
       "      <td>0.002458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>tfidf_380</td>\n",
       "      <td>0.002443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>tfidf_266</td>\n",
       "      <td>0.002430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>tfidf_304</td>\n",
       "      <td>0.002426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>giggly</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>tfidf_343</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>dry eyes</td>\n",
       "      <td>0.002350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>tfidf_104</td>\n",
       "      <td>0.002348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tfidf_48</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>tfidf_376</td>\n",
       "      <td>0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>tfidf_360</td>\n",
       "      <td>0.002303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>tfidf_240</td>\n",
       "      <td>0.002296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>tfidf_216</td>\n",
       "      <td>0.002292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>tfidf_361</td>\n",
       "      <td>0.002291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>tfidf_366</td>\n",
       "      <td>0.002283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>tfidf_129</td>\n",
       "      <td>0.002280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>aroused</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tfidf_46</td>\n",
       "      <td>0.002234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>rose</td>\n",
       "      <td>0.002221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>skunk</td>\n",
       "      <td>0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>tfidf_278</td>\n",
       "      <td>0.002202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tfidf_36</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>tfidf_246</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>tfidf_336</td>\n",
       "      <td>0.002158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>tfidf_375</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>tfidf_103</td>\n",
       "      <td>0.002104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>tfidf_247</td>\n",
       "      <td>0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>tfidf_151</td>\n",
       "      <td>0.002085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>tfidf_371</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>tfidf_354</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>tfidf_137</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>tfidf_126</td>\n",
       "      <td>0.002046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tfidf_12</td>\n",
       "      <td>0.002040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>tfidf_142</td>\n",
       "      <td>0.002037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>tfidf_344</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>tfidf_318</td>\n",
       "      <td>0.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>tfidf_264</td>\n",
       "      <td>0.001990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>tfidf_203</td>\n",
       "      <td>0.001983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>tfidf_217</td>\n",
       "      <td>0.001960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>tfidf_97</td>\n",
       "      <td>0.001952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tfidf_34</td>\n",
       "      <td>0.001951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>tfidf_81</td>\n",
       "      <td>0.001936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>tfidf_338</td>\n",
       "      <td>0.001927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>tfidf_221</td>\n",
       "      <td>0.001921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tfidf_79</td>\n",
       "      <td>0.001918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>tfidf_140</td>\n",
       "      <td>0.001889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>tfidf_295</td>\n",
       "      <td>0.001855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>tfidf_373</td>\n",
       "      <td>0.001841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>tfidf_348</td>\n",
       "      <td>0.001839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>tfidf_339</td>\n",
       "      <td>0.001825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>tfidf_341</td>\n",
       "      <td>0.001798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>tfidf_123</td>\n",
       "      <td>0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>tfidf_286</td>\n",
       "      <td>0.001785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>tfidf_324</td>\n",
       "      <td>0.001760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>tfidf_350</td>\n",
       "      <td>0.001743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tfidf_98</td>\n",
       "      <td>0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>tfidf_75</td>\n",
       "      <td>0.001716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tfidf_38</td>\n",
       "      <td>0.001684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tfidf_54</td>\n",
       "      <td>0.001681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tfidf_32</td>\n",
       "      <td>0.001645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>tfidf_259</td>\n",
       "      <td>0.001644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>tfidf_122</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>tfidf_267</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tfidf_16</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>tfidf_287</td>\n",
       "      <td>0.001635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>tfidf_190</td>\n",
       "      <td>0.001622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>tfidf_273</td>\n",
       "      <td>0.001613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tfidf_17</td>\n",
       "      <td>0.001607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tfidf_26</td>\n",
       "      <td>0.001601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>tfidf_159</td>\n",
       "      <td>0.001591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>nutty</td>\n",
       "      <td>0.001580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>tfidf_120</td>\n",
       "      <td>0.001571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>tfidf_321</td>\n",
       "      <td>0.001570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>tfidf_139</td>\n",
       "      <td>0.001553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>tfidf_223</td>\n",
       "      <td>0.001541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tfidf_55</td>\n",
       "      <td>0.001503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tfidf_22</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>tfidf_107</td>\n",
       "      <td>0.001482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>tfidf_61</td>\n",
       "      <td>0.001476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>tfidf_163</td>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>tfidf_185</td>\n",
       "      <td>0.001466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>tfidf_272</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>tfidf_335</td>\n",
       "      <td>0.001451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tfidf_58</td>\n",
       "      <td>0.001431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>tfidf_90</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>tfidf_96</td>\n",
       "      <td>0.001423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>tfidf_215</td>\n",
       "      <td>0.001417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tfidf_332</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>tfidf_275</td>\n",
       "      <td>0.001398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>tfidf_206</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>tfidf_225</td>\n",
       "      <td>0.001390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>tfidf_355</td>\n",
       "      <td>0.001386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>tfidf_251</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>tfidf_153</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>tfidf_232</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tfidf_24</td>\n",
       "      <td>0.001365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tfidf_53</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>tfidf_260</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tfidf_41</td>\n",
       "      <td>0.001353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>tfidf_319</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>tfidf_325</td>\n",
       "      <td>0.001349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>tfidf_77</td>\n",
       "      <td>0.001341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>tfidf_353</td>\n",
       "      <td>0.001333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>tfidf_101</td>\n",
       "      <td>0.001321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>tfidf_64</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tfidf_45</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tfidf_175</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>tfidf_198</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>tfidf_288</td>\n",
       "      <td>0.001248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tfidf_299</td>\n",
       "      <td>0.001245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>tfidf_326</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>mango</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>tfidf_161</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>tfidf_171</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfidf_6</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tfidf_69</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>tfidf_370</td>\n",
       "      <td>0.001222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>tfidf_211</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>tfidf_180</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>tfidf_136</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tfidf_99</td>\n",
       "      <td>0.001199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tfidf_44</td>\n",
       "      <td>0.001178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>dizzy</td>\n",
       "      <td>0.001170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>tfidf_193</td>\n",
       "      <td>0.001159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>tfidf_110</td>\n",
       "      <td>0.001158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>tfidf_224</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>tfidf_87</td>\n",
       "      <td>0.001155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>tfidf_243</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tfidf_28</td>\n",
       "      <td>0.001140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>tfidf_359</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>tfidf_189</td>\n",
       "      <td>0.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>tfidf_102</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>tfidf_164</td>\n",
       "      <td>0.001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tfidf_29</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>tfidf_152</td>\n",
       "      <td>0.001104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>tfidf_300</td>\n",
       "      <td>0.001104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>tfidf_271</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>tfidf_76</td>\n",
       "      <td>0.001101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>tfidf_294</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>tfidf_57</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>tfidf_80</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>tfidf_244</td>\n",
       "      <td>0.001095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>tfidf_222</td>\n",
       "      <td>0.001060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>tfidf_331</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tfidf_9</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>tfidf_183</td>\n",
       "      <td>0.001040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tfidf_19</td>\n",
       "      <td>0.001035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>tfidf_265</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>tfidf_372</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>spicy/herbal</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>tfidf_227</td>\n",
       "      <td>0.001009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>tfidf_74</td>\n",
       "      <td>0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tfidf_208</td>\n",
       "      <td>0.001003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>tfidf_118</td>\n",
       "      <td>0.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tfidf_52</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>tfidf_316</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>tfidf_328</td>\n",
       "      <td>0.000975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>tfidf_284</td>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>tfidf_146</td>\n",
       "      <td>0.000965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>tfidf_150</td>\n",
       "      <td>0.000960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>tfidf_368</td>\n",
       "      <td>0.000958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>tfidf_71</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>tfidf_88</td>\n",
       "      <td>0.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tfidf_27</td>\n",
       "      <td>0.000938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>tfidf_386</td>\n",
       "      <td>0.000938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>tfidf_179</td>\n",
       "      <td>0.000937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>tfidf_282</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>tfidf_231</td>\n",
       "      <td>0.000916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>tfidf_170</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tfidf_8</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>apple</td>\n",
       "      <td>0.000909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>tfidf_301</td>\n",
       "      <td>0.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>tfidf_377</td>\n",
       "      <td>0.000899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>tfidf_317</td>\n",
       "      <td>0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tfidf_68</td>\n",
       "      <td>0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>tfidf_82</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tfidf_31</td>\n",
       "      <td>0.000884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>tfidf_381</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>tfidf_297</td>\n",
       "      <td>0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>sage</td>\n",
       "      <td>0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>paranoid</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>tfidf_280</td>\n",
       "      <td>0.000862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.000861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>tfidf_255</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tfidf_10</td>\n",
       "      <td>0.000849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tfidf_14</td>\n",
       "      <td>0.000848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>tfidf_92</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>tfidf_95</td>\n",
       "      <td>0.000844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>tfidf_320</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>tfidf_186</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>tfidf_387</td>\n",
       "      <td>0.000825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tfidf_105</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>tfidf_138</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>tfidf_263</td>\n",
       "      <td>0.000809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>tfidf_125</td>\n",
       "      <td>0.000806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>tfidf_148</td>\n",
       "      <td>0.000803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tfidf_296</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>tfidf_60</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tfidf_236</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>tfidf_115</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>tfidf_384</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>tfidf_59</td>\n",
       "      <td>0.000769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tfidf_65</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>tfidf_220</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>tfidf_308</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>tfidf_192</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>tfidf_254</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>tfidf_315</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>tfidf_157</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>tfidf_174</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>tfidf_111</td>\n",
       "      <td>0.000728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>tfidf_363</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>tfidf_196</td>\n",
       "      <td>0.000711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>tfidf_91</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>tfidf_112</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>tfidf_116</td>\n",
       "      <td>0.000683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>tfidf_212</td>\n",
       "      <td>0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>tfidf_290</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>tfidf_385</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>tfidf_187</td>\n",
       "      <td>0.000669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>tfidf_277</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>tfidf_89</td>\n",
       "      <td>0.000666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>tfidf_160</td>\n",
       "      <td>0.000659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>tfidf_169</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>tfidf_177</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tfidf_49</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>tfidf_165</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>tfidf_202</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>tfidf_181</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>tfidf_63</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>tfidf_323</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>tfidf_135</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>tfidf_322</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>tfidf_147</td>\n",
       "      <td>0.000627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>tfidf_293</td>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tfidf_67</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>tfidf_188</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>tfidf_197</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>tfidf_305</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>tfidf_268</td>\n",
       "      <td>0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>tfidf_114</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tfidf_42</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tfidf_66</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>anxious</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tfidf_56</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>tfidf_219</td>\n",
       "      <td>0.000598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>tfidf_84</td>\n",
       "      <td>0.000595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>lime</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tfidf_40</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tfidf_106</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>tfidf_85</td>\n",
       "      <td>0.000577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>tfidf_298</td>\n",
       "      <td>0.000577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>headache</td>\n",
       "      <td>0.000571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tfidf_13</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>tfidf_233</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>tfidf_133</td>\n",
       "      <td>0.000545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>tfidf_270</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>tfidf_333</td>\n",
       "      <td>0.000533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>tfidf_274</td>\n",
       "      <td>0.000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>tfidf_235</td>\n",
       "      <td>0.000518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>tfidf_261</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tfidf_33</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>tfidf_269</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>tfidf_83</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>tfidf_310</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>tfidf_182</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tfidf_94</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>tfidf_358</td>\n",
       "      <td>0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>tfidf_109</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tfidf_51</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>tfidf_346</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>tfidf_172</td>\n",
       "      <td>0.000481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>tfidf_334</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tfidf_86</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tfidf_18</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>tfidf_127</td>\n",
       "      <td>0.000463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>tfidf_365</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>tfidf_249</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>tfidf_234</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>tfidf_100</td>\n",
       "      <td>0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>tfidf_252</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>tfidf_248</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>honey</td>\n",
       "      <td>0.000443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>fruit</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>tfidf_311</td>\n",
       "      <td>0.000429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>tfidf_313</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tfidf_191</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>tfidf_347</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>tfidf_356</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tfidf_15</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>tfidf_195</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tfidf_23</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>tfidf_378</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>tfidf_241</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>pear</td>\n",
       "      <td>0.000406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>tfidf_62</td>\n",
       "      <td>0.000389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>chemical</td>\n",
       "      <td>0.000389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>tfidf_226</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>tfidf_204</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>tfidf_276</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>tfidf_238</td>\n",
       "      <td>0.000373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>tfidf_228</td>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tfidf_25</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tfidf_50</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>tea</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>tfidf_134</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>tfidf_72</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>tfidf_307</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>tfidf_352</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>tfidf_279</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>tfidf_176</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>tfidf_262</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>pineapple</td>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>tfidf_218</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>tfidf_194</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>tfidf_70</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>tfidf_113</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>tfidf_156</td>\n",
       "      <td>0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>tfidf_327</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>tfidf_155</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>ammonia</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>pepper</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>tfidf_108</td>\n",
       "      <td>0.000307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>tfidf_379</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>tfidf_143</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>tfidf_214</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>lavender</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>tfidf_213</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>tfidf_383</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>tfidf_256</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>tfidf_292</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>tfidf_349</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>tfidf_364</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>tfidf_351</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tfidf_47</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>tar</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>tfidf_306</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>tfidf_201</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>tfidf_330</td>\n",
       "      <td>0.000211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>peach</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>chestnut</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>tfidf_132</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>tfidf_242</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tfidf_302</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>menthol</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>tfidf_229</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>apricot</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>tfidf_250</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>blue cheese</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>plum</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>depression</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>migraines</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>pain</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>fatigue</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>eye pressure</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>spasticity</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>stress</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>epilepsy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>arthritis</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>seizures</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         features     score\n",
       "433        diesel  0.045981\n",
       "390        sativa  0.028979\n",
       "253     tfidf_253  0.023822\n",
       "388        hybrid  0.021857\n",
       "329     tfidf_329  0.020850\n",
       "466       vanilla  0.017930\n",
       "447        orange  0.016473\n",
       "285     tfidf_285  0.012630\n",
       "168     tfidf_168  0.012605\n",
       "145     tfidf_145  0.012457\n",
       "441         lemon  0.012307\n",
       "149     tfidf_149  0.011801\n",
       "426     blueberry  0.011200\n",
       "312     tfidf_312  0.010385\n",
       "239     tfidf_239  0.010106\n",
       "345     tfidf_345  0.009958\n",
       "245     tfidf_245  0.009891\n",
       "199     tfidf_199  0.009245\n",
       "141     tfidf_141  0.008789\n",
       "309     tfidf_309  0.008320\n",
       "121     tfidf_121  0.007838\n",
       "424         berry  0.007825\n",
       "11       tfidf_11  0.007257\n",
       "207     tfidf_207  0.006915\n",
       "438    grapefruit  0.006857\n",
       "402      euphoric  0.006698\n",
       "281     tfidf_281  0.006577\n",
       "434        earthy  0.006515\n",
       "367     tfidf_367  0.006182\n",
       "413       relaxed  0.005832\n",
       "210     tfidf_210  0.005684\n",
       "37       tfidf_37  0.005669\n",
       "451          pine  0.005647\n",
       "428        cheese  0.005578\n",
       "209     tfidf_209  0.005314\n",
       "342     tfidf_342  0.005309\n",
       "30       tfidf_30  0.005297\n",
       "389        indica  0.005189\n",
       "407         happy  0.005084\n",
       "357     tfidf_357  0.005021\n",
       "314     tfidf_314  0.005007\n",
       "73       tfidf_73  0.004875\n",
       "93       tfidf_93  0.004869\n",
       "445          mint  0.004804\n",
       "420      uplifted  0.004631\n",
       "35       tfidf_35  0.004566\n",
       "7         tfidf_7  0.004485\n",
       "162     tfidf_162  0.004402\n",
       "437         grape  0.004370\n",
       "119     tfidf_119  0.004139\n",
       "158     tfidf_158  0.004133\n",
       "395      creative  0.004082\n",
       "78       tfidf_78  0.004025\n",
       "303     tfidf_303  0.004022\n",
       "178     tfidf_178  0.003943\n",
       "43       tfidf_43  0.003893\n",
       "409        hungry  0.003796\n",
       "460         sweet  0.003795\n",
       "400     energetic  0.003729\n",
       "230     tfidf_230  0.003714\n",
       "362     tfidf_362  0.003629\n",
       "20       tfidf_20  0.003598\n",
       "167     tfidf_167  0.003471\n",
       "405       focused  0.003445\n",
       "399     dry mouth  0.003414\n",
       "257     tfidf_257  0.003365\n",
       "144     tfidf_144  0.003331\n",
       "200     tfidf_200  0.003318\n",
       "258     tfidf_258  0.003225\n",
       "382     tfidf_382  0.003194\n",
       "39       tfidf_39  0.003170\n",
       "128     tfidf_128  0.003080\n",
       "173     tfidf_173  0.003077\n",
       "415        sleepy  0.003066\n",
       "291     tfidf_291  0.003052\n",
       "374     tfidf_374  0.003037\n",
       "337     tfidf_337  0.003036\n",
       "184     tfidf_184  0.003025\n",
       "205     tfidf_205  0.003000\n",
       "418     talkative  0.002960\n",
       "21       tfidf_21  0.002851\n",
       "454       pungent  0.002847\n",
       "117     tfidf_117  0.002800\n",
       "166     tfidf_166  0.002720\n",
       "131     tfidf_131  0.002708\n",
       "289     tfidf_289  0.002703\n",
       "340     tfidf_340  0.002698\n",
       "427        butter  0.002659\n",
       "124     tfidf_124  0.002631\n",
       "154     tfidf_154  0.002623\n",
       "431        citrus  0.002611\n",
       "369     tfidf_369  0.002603\n",
       "283     tfidf_283  0.002591\n",
       "237     tfidf_237  0.002570\n",
       "5         tfidf_5  0.002537\n",
       "419        tingly  0.002519\n",
       "435       flowery  0.002472\n",
       "130     tfidf_130  0.002458\n",
       "380     tfidf_380  0.002443\n",
       "266     tfidf_266  0.002430\n",
       "304     tfidf_304  0.002426\n",
       "406        giggly  0.002425\n",
       "343     tfidf_343  0.002398\n",
       "398      dry eyes  0.002350\n",
       "104     tfidf_104  0.002348\n",
       "48       tfidf_48  0.002332\n",
       "376     tfidf_376  0.002321\n",
       "360     tfidf_360  0.002303\n",
       "240     tfidf_240  0.002296\n",
       "216     tfidf_216  0.002292\n",
       "361     tfidf_361  0.002291\n",
       "366     tfidf_366  0.002283\n",
       "129     tfidf_129  0.002280\n",
       "1         tfidf_1  0.002246\n",
       "393       aroused  0.002246\n",
       "46       tfidf_46  0.002234\n",
       "455          rose  0.002221\n",
       "457         skunk  0.002214\n",
       "278     tfidf_278  0.002202\n",
       "36       tfidf_36  0.002188\n",
       "246     tfidf_246  0.002168\n",
       "336     tfidf_336  0.002158\n",
       "375     tfidf_375  0.002115\n",
       "3         tfidf_3  0.002112\n",
       "103     tfidf_103  0.002104\n",
       "247     tfidf_247  0.002091\n",
       "151     tfidf_151  0.002085\n",
       "371     tfidf_371  0.002074\n",
       "354     tfidf_354  0.002054\n",
       "137     tfidf_137  0.002054\n",
       "126     tfidf_126  0.002046\n",
       "12       tfidf_12  0.002040\n",
       "142     tfidf_142  0.002037\n",
       "344     tfidf_344  0.002029\n",
       "318     tfidf_318  0.002020\n",
       "468         woody  0.001993\n",
       "264     tfidf_264  0.001990\n",
       "203     tfidf_203  0.001983\n",
       "217     tfidf_217  0.001960\n",
       "97       tfidf_97  0.001952\n",
       "34       tfidf_34  0.001951\n",
       "81       tfidf_81  0.001936\n",
       "338     tfidf_338  0.001927\n",
       "221     tfidf_221  0.001921\n",
       "79       tfidf_79  0.001918\n",
       "140     tfidf_140  0.001889\n",
       "295     tfidf_295  0.001855\n",
       "373     tfidf_373  0.001841\n",
       "348     tfidf_348  0.001839\n",
       "339     tfidf_339  0.001825\n",
       "341     tfidf_341  0.001798\n",
       "123     tfidf_123  0.001788\n",
       "286     tfidf_286  0.001785\n",
       "324     tfidf_324  0.001760\n",
       "350     tfidf_350  0.001743\n",
       "98       tfidf_98  0.001731\n",
       "75       tfidf_75  0.001716\n",
       "38       tfidf_38  0.001684\n",
       "54       tfidf_54  0.001681\n",
       "32       tfidf_32  0.001645\n",
       "259     tfidf_259  0.001644\n",
       "122     tfidf_122  0.001639\n",
       "267     tfidf_267  0.001636\n",
       "16       tfidf_16  0.001636\n",
       "287     tfidf_287  0.001635\n",
       "190     tfidf_190  0.001622\n",
       "273     tfidf_273  0.001613\n",
       "17       tfidf_17  0.001607\n",
       "26       tfidf_26  0.001601\n",
       "159     tfidf_159  0.001591\n",
       "446         nutty  0.001580\n",
       "120     tfidf_120  0.001571\n",
       "321     tfidf_321  0.001570\n",
       "139     tfidf_139  0.001553\n",
       "223     tfidf_223  0.001541\n",
       "55       tfidf_55  0.001503\n",
       "22       tfidf_22  0.001487\n",
       "107     tfidf_107  0.001482\n",
       "61       tfidf_61  0.001476\n",
       "163     tfidf_163  0.001472\n",
       "185     tfidf_185  0.001466\n",
       "272     tfidf_272  0.001465\n",
       "335     tfidf_335  0.001451\n",
       "58       tfidf_58  0.001431\n",
       "90       tfidf_90  0.001427\n",
       "96       tfidf_96  0.001423\n",
       "215     tfidf_215  0.001417\n",
       "332     tfidf_332  0.001409\n",
       "275     tfidf_275  0.001398\n",
       "206     tfidf_206  0.001395\n",
       "225     tfidf_225  0.001390\n",
       "355     tfidf_355  0.001386\n",
       "251     tfidf_251  0.001383\n",
       "153     tfidf_153  0.001381\n",
       "232     tfidf_232  0.001374\n",
       "24       tfidf_24  0.001365\n",
       "53       tfidf_53  0.001364\n",
       "260     tfidf_260  0.001362\n",
       "41       tfidf_41  0.001353\n",
       "319     tfidf_319  0.001351\n",
       "325     tfidf_325  0.001349\n",
       "77       tfidf_77  0.001341\n",
       "353     tfidf_353  0.001333\n",
       "101     tfidf_101  0.001321\n",
       "64       tfidf_64  0.001306\n",
       "45       tfidf_45  0.001285\n",
       "175     tfidf_175  0.001282\n",
       "198     tfidf_198  0.001256\n",
       "288     tfidf_288  0.001248\n",
       "299     tfidf_299  0.001245\n",
       "326     tfidf_326  0.001244\n",
       "443         mango  0.001240\n",
       "161     tfidf_161  0.001237\n",
       "171     tfidf_171  0.001226\n",
       "6         tfidf_6  0.001226\n",
       "69       tfidf_69  0.001224\n",
       "370     tfidf_370  0.001222\n",
       "211     tfidf_211  0.001214\n",
       "180     tfidf_180  0.001211\n",
       "136     tfidf_136  0.001211\n",
       "99       tfidf_99  0.001199\n",
       "44       tfidf_44  0.001178\n",
       "397         dizzy  0.001170\n",
       "193     tfidf_193  0.001159\n",
       "110     tfidf_110  0.001158\n",
       "224     tfidf_224  0.001156\n",
       "87       tfidf_87  0.001155\n",
       "243     tfidf_243  0.001153\n",
       "28       tfidf_28  0.001140\n",
       "359     tfidf_359  0.001139\n",
       "189     tfidf_189  0.001127\n",
       "102     tfidf_102  0.001124\n",
       "164     tfidf_164  0.001116\n",
       "29       tfidf_29  0.001114\n",
       "152     tfidf_152  0.001104\n",
       "300     tfidf_300  0.001104\n",
       "271     tfidf_271  0.001102\n",
       "76       tfidf_76  0.001101\n",
       "294     tfidf_294  0.001098\n",
       "57       tfidf_57  0.001096\n",
       "80       tfidf_80  0.001096\n",
       "244     tfidf_244  0.001095\n",
       "0         tfidf_0  0.001070\n",
       "222     tfidf_222  0.001060\n",
       "331     tfidf_331  0.001058\n",
       "9         tfidf_9  0.001057\n",
       "183     tfidf_183  0.001040\n",
       "19       tfidf_19  0.001035\n",
       "265     tfidf_265  0.001032\n",
       "372     tfidf_372  0.001026\n",
       "458  spicy/herbal  0.001018\n",
       "227     tfidf_227  0.001009\n",
       "74       tfidf_74  0.001007\n",
       "208     tfidf_208  0.001003\n",
       "118     tfidf_118  0.001001\n",
       "52       tfidf_52  0.001000\n",
       "316     tfidf_316  0.000997\n",
       "328     tfidf_328  0.000975\n",
       "284     tfidf_284  0.000966\n",
       "146     tfidf_146  0.000965\n",
       "150     tfidf_150  0.000960\n",
       "368     tfidf_368  0.000958\n",
       "71       tfidf_71  0.000945\n",
       "88       tfidf_88  0.000942\n",
       "27       tfidf_27  0.000938\n",
       "386     tfidf_386  0.000938\n",
       "179     tfidf_179  0.000937\n",
       "282     tfidf_282  0.000931\n",
       "231     tfidf_231  0.000916\n",
       "170     tfidf_170  0.000915\n",
       "8         tfidf_8  0.000911\n",
       "422         apple  0.000909\n",
       "301     tfidf_301  0.000906\n",
       "377     tfidf_377  0.000899\n",
       "317     tfidf_317  0.000890\n",
       "68       tfidf_68  0.000890\n",
       "82       tfidf_82  0.000885\n",
       "31       tfidf_31  0.000884\n",
       "381     tfidf_381  0.000871\n",
       "297     tfidf_297  0.000866\n",
       "456          sage  0.000866\n",
       "412      paranoid  0.000865\n",
       "280     tfidf_280  0.000862\n",
       "4         tfidf_4  0.000861\n",
       "255     tfidf_255  0.000850\n",
       "10       tfidf_10  0.000849\n",
       "14       tfidf_14  0.000848\n",
       "92       tfidf_92  0.000846\n",
       "95       tfidf_95  0.000844\n",
       "320     tfidf_320  0.000835\n",
       "186     tfidf_186  0.000829\n",
       "387     tfidf_387  0.000825\n",
       "105     tfidf_105  0.000819\n",
       "138     tfidf_138  0.000818\n",
       "263     tfidf_263  0.000809\n",
       "125     tfidf_125  0.000806\n",
       "148     tfidf_148  0.000803\n",
       "296     tfidf_296  0.000797\n",
       "60       tfidf_60  0.000792\n",
       "236     tfidf_236  0.000777\n",
       "115     tfidf_115  0.000774\n",
       "384     tfidf_384  0.000771\n",
       "59       tfidf_59  0.000769\n",
       "65       tfidf_65  0.000764\n",
       "220     tfidf_220  0.000762\n",
       "308     tfidf_308  0.000762\n",
       "192     tfidf_192  0.000760\n",
       "254     tfidf_254  0.000756\n",
       "315     tfidf_315  0.000748\n",
       "157     tfidf_157  0.000748\n",
       "174     tfidf_174  0.000745\n",
       "432        coffee  0.000730\n",
       "111     tfidf_111  0.000728\n",
       "363     tfidf_363  0.000719\n",
       "196     tfidf_196  0.000711\n",
       "91       tfidf_91  0.000699\n",
       "112     tfidf_112  0.000698\n",
       "116     tfidf_116  0.000683\n",
       "212     tfidf_212  0.000680\n",
       "290     tfidf_290  0.000673\n",
       "385     tfidf_385  0.000670\n",
       "187     tfidf_187  0.000669\n",
       "277     tfidf_277  0.000668\n",
       "89       tfidf_89  0.000666\n",
       "160     tfidf_160  0.000659\n",
       "169     tfidf_169  0.000658\n",
       "177     tfidf_177  0.000658\n",
       "49       tfidf_49  0.000651\n",
       "165     tfidf_165  0.000648\n",
       "202     tfidf_202  0.000643\n",
       "181     tfidf_181  0.000639\n",
       "63       tfidf_63  0.000639\n",
       "323     tfidf_323  0.000635\n",
       "135     tfidf_135  0.000635\n",
       "322     tfidf_322  0.000633\n",
       "147     tfidf_147  0.000627\n",
       "293     tfidf_293  0.000626\n",
       "67       tfidf_67  0.000619\n",
       "188     tfidf_188  0.000617\n",
       "197     tfidf_197  0.000617\n",
       "305     tfidf_305  0.000608\n",
       "268     tfidf_268  0.000607\n",
       "114     tfidf_114  0.000605\n",
       "42       tfidf_42  0.000605\n",
       "66       tfidf_66  0.000604\n",
       "392       anxious  0.000602\n",
       "56       tfidf_56  0.000600\n",
       "219     tfidf_219  0.000598\n",
       "84       tfidf_84  0.000595\n",
       "442          lime  0.000592\n",
       "40       tfidf_40  0.000589\n",
       "106     tfidf_106  0.000583\n",
       "85       tfidf_85  0.000577\n",
       "298     tfidf_298  0.000577\n",
       "408      headache  0.000571\n",
       "13       tfidf_13  0.000569\n",
       "465      tropical  0.000567\n",
       "233     tfidf_233  0.000563\n",
       "133     tfidf_133  0.000545\n",
       "270     tfidf_270  0.000542\n",
       "333     tfidf_333  0.000533\n",
       "274     tfidf_274  0.000529\n",
       "235     tfidf_235  0.000518\n",
       "261     tfidf_261  0.000514\n",
       "33       tfidf_33  0.000513\n",
       "269     tfidf_269  0.000511\n",
       "83       tfidf_83  0.000507\n",
       "310     tfidf_310  0.000507\n",
       "182     tfidf_182  0.000505\n",
       "94       tfidf_94  0.000501\n",
       "358     tfidf_358  0.000495\n",
       "109     tfidf_109  0.000493\n",
       "51       tfidf_51  0.000489\n",
       "346     tfidf_346  0.000482\n",
       "172     tfidf_172  0.000481\n",
       "334     tfidf_334  0.000472\n",
       "86       tfidf_86  0.000470\n",
       "18       tfidf_18  0.000467\n",
       "127     tfidf_127  0.000463\n",
       "365     tfidf_365  0.000452\n",
       "249     tfidf_249  0.000450\n",
       "234     tfidf_234  0.000449\n",
       "100     tfidf_100  0.000448\n",
       "2         tfidf_2  0.000447\n",
       "252     tfidf_252  0.000446\n",
       "248     tfidf_248  0.000446\n",
       "439         honey  0.000443\n",
       "436         fruit  0.000432\n",
       "311     tfidf_311  0.000429\n",
       "313     tfidf_313  0.000427\n",
       "191     tfidf_191  0.000424\n",
       "347     tfidf_347  0.000418\n",
       "356     tfidf_356  0.000417\n",
       "15       tfidf_15  0.000416\n",
       "195     tfidf_195  0.000414\n",
       "23       tfidf_23  0.000414\n",
       "378     tfidf_378  0.000413\n",
       "241     tfidf_241  0.000411\n",
       "449          pear  0.000406\n",
       "463       tobacco  0.000395\n",
       "62       tfidf_62  0.000389\n",
       "429      chemical  0.000389\n",
       "226     tfidf_226  0.000384\n",
       "204     tfidf_204  0.000380\n",
       "276     tfidf_276  0.000375\n",
       "238     tfidf_238  0.000373\n",
       "228     tfidf_228  0.000368\n",
       "25       tfidf_25  0.000366\n",
       "50       tfidf_50  0.000361\n",
       "462           tea  0.000356\n",
       "134     tfidf_134  0.000355\n",
       "72       tfidf_72  0.000353\n",
       "307     tfidf_307  0.000353\n",
       "352     tfidf_352  0.000350\n",
       "279     tfidf_279  0.000348\n",
       "176     tfidf_176  0.000348\n",
       "262     tfidf_262  0.000347\n",
       "452     pineapple  0.000344\n",
       "218     tfidf_218  0.000343\n",
       "194     tfidf_194  0.000343\n",
       "70       tfidf_70  0.000339\n",
       "113     tfidf_113  0.000337\n",
       "459    strawberry  0.000336\n",
       "156     tfidf_156  0.000328\n",
       "327     tfidf_327  0.000323\n",
       "155     tfidf_155  0.000321\n",
       "421       ammonia  0.000320\n",
       "450        pepper  0.000314\n",
       "108     tfidf_108  0.000307\n",
       "379     tfidf_379  0.000305\n",
       "143     tfidf_143  0.000302\n",
       "464          tree  0.000299\n",
       "214     tfidf_214  0.000286\n",
       "440      lavender  0.000276\n",
       "213     tfidf_213  0.000271\n",
       "383     tfidf_383  0.000269\n",
       "256     tfidf_256  0.000247\n",
       "292     tfidf_292  0.000247\n",
       "349     tfidf_349  0.000244\n",
       "364     tfidf_364  0.000242\n",
       "351     tfidf_351  0.000241\n",
       "47       tfidf_47  0.000241\n",
       "461           tar  0.000230\n",
       "306     tfidf_306  0.000230\n",
       "201     tfidf_201  0.000221\n",
       "330     tfidf_330  0.000211\n",
       "448         peach  0.000185\n",
       "430      chestnut  0.000168\n",
       "132     tfidf_132  0.000168\n",
       "467        violet  0.000163\n",
       "242     tfidf_242  0.000148\n",
       "302     tfidf_302  0.000136\n",
       "444       menthol  0.000127\n",
       "229     tfidf_229  0.000123\n",
       "423       apricot  0.000120\n",
       "250     tfidf_250  0.000102\n",
       "425   blue cheese  0.000091\n",
       "453          plum  0.000080\n",
       "396    depression  0.000027\n",
       "410     migraines  0.000024\n",
       "391       anxiety  0.000016\n",
       "411          pain  0.000016\n",
       "404       fatigue  0.000009\n",
       "403  eye pressure  0.000007\n",
       "416    spasticity  0.000000\n",
       "417        stress  0.000000\n",
       "401      epilepsy  0.000000\n",
       "394     arthritis  0.000000\n",
       "414      seizures  0.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', df_feat_ranked.shape[0]+1)\n",
    "df_feat_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py:357: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "selector = SelectFromModel(rfreg).fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.05611396e-03, 2.00018654e-03, 3.63372466e-04, 2.18193537e-03,\n",
       "       8.45401789e-04, 2.71075016e-03, 1.28380694e-03, 4.33286854e-03,\n",
       "       7.71382177e-04, 1.10071981e-03, 7.85956030e-04, 7.78656891e-03,\n",
       "       2.17257741e-03, 5.13098251e-04, 8.38780020e-04, 4.01362942e-04,\n",
       "       1.92692768e-03, 1.54378621e-03, 5.11322693e-04, 1.11348017e-03,\n",
       "       3.83924141e-03, 2.64394660e-03, 1.49132718e-03, 3.29639027e-04,\n",
       "       1.30680249e-03, 4.41098085e-04, 1.62855335e-03, 1.08350211e-03,\n",
       "       1.33460877e-03, 1.08610331e-03, 4.89163854e-03, 7.47883107e-04,\n",
       "       1.57075760e-03, 5.62090076e-04, 1.95954032e-03, 4.55657593e-03,\n",
       "       2.18855178e-03, 5.45478604e-03, 1.42133003e-03, 3.35011562e-03,\n",
       "       6.49961966e-04, 1.39711865e-03, 5.96103896e-04, 4.47267553e-03,\n",
       "       1.08139631e-03, 1.26957735e-03, 2.36931362e-03, 2.98854127e-04,\n",
       "       2.25045196e-03, 6.26976865e-04, 3.20889189e-04, 5.16785942e-04,\n",
       "       7.48724033e-04, 1.24798085e-03, 1.62614487e-03, 1.55613441e-03,\n",
       "       6.37479938e-04, 1.04340838e-03, 1.15793645e-03, 7.63996537e-04,\n",
       "       9.84978543e-04, 1.50493312e-03, 3.00385835e-04, 6.70206453e-04,\n",
       "       1.27396354e-03, 7.92185944e-04, 5.26033211e-04, 6.56133727e-04,\n",
       "       8.06405277e-04, 1.28384031e-03, 3.13784417e-04, 8.82747529e-04,\n",
       "       2.90968982e-04, 4.47024299e-03, 1.12553088e-03, 1.76696349e-03,\n",
       "       1.16146667e-03, 1.27351103e-03, 3.82806668e-03, 2.17969083e-03,\n",
       "       1.18646102e-03, 1.78012825e-03, 8.00384299e-04, 4.41071227e-04,\n",
       "       6.50461339e-04, 5.78633984e-04, 5.57726105e-04, 1.29684765e-03,\n",
       "       8.93910771e-04, 7.30716674e-04, 1.38000468e-03, 6.43179784e-04,\n",
       "       7.84380773e-04, 5.22564747e-03, 5.67262299e-04, 1.10489816e-03,\n",
       "       1.46586315e-03, 2.02901768e-03, 1.78657946e-03, 1.16616888e-03,\n",
       "       4.41980525e-04, 1.46200603e-03, 9.74235809e-04, 1.94611552e-03,\n",
       "       2.29271258e-03, 6.32230794e-04, 6.78901765e-04, 1.33697366e-03,\n",
       "       2.76976190e-04, 4.94690521e-04, 1.17444620e-03, 7.66082131e-04,\n",
       "       7.85068834e-04, 3.53383243e-04, 6.50115692e-04, 6.35526741e-04,\n",
       "       7.33562349e-04, 2.69263802e-03, 9.03344476e-04, 4.14003271e-03,\n",
       "       1.66814507e-03, 7.88494471e-03, 1.45475335e-03, 1.77999770e-03,\n",
       "       2.70085185e-03, 7.12528693e-04, 2.16358575e-03, 5.41798573e-04,\n",
       "       3.21832610e-03, 2.28574130e-03, 2.26989495e-03, 2.68198149e-03,\n",
       "       1.98023457e-04, 4.66702289e-04, 3.76201874e-04, 6.74473681e-04,\n",
       "       1.23287463e-03, 1.92776115e-03, 8.78375504e-04, 1.51541856e-03,\n",
       "       1.98193310e-03, 8.86475117e-03, 2.20149743e-03, 3.70668177e-04,\n",
       "       3.34776589e-03, 1.21927177e-02, 8.40957126e-04, 6.03232487e-04,\n",
       "       6.26653269e-04, 1.21806793e-02, 1.08156998e-03, 2.22095577e-03,\n",
       "       1.02107959e-03, 1.39970296e-03, 2.65795509e-03, 3.60969476e-04,\n",
       "       3.56954052e-04, 6.15350370e-04, 4.04219204e-03, 1.51505948e-03,\n",
       "       5.65244449e-04, 1.24339078e-03, 4.28024972e-03, 1.50508267e-03,\n",
       "       1.13758521e-03, 6.67045746e-04, 2.59598339e-03, 3.89607313e-03,\n",
       "       1.28537213e-02, 7.21149817e-04, 9.52359930e-04, 1.16508672e-03,\n",
       "       4.60185115e-04, 3.12461337e-03, 6.35022844e-04, 1.24890452e-03,\n",
       "       3.43478334e-04, 8.52333648e-04, 3.55886486e-03, 9.84066148e-04,\n",
       "       1.21856601e-03, 6.38378952e-04, 5.70387572e-04, 1.10901919e-03,\n",
       "       2.93508024e-03, 1.54793865e-03, 7.78019143e-04, 4.94419860e-04,\n",
       "       7.14885451e-04, 1.22976053e-03, 1.63917972e-03, 3.86970281e-04,\n",
       "       7.70989710e-04, 1.30558442e-03, 3.83930575e-04, 4.94384915e-04,\n",
       "       6.85907951e-04, 5.13529099e-04, 1.58419105e-03, 9.98366131e-03,\n",
       "       3.20967264e-03, 2.19418649e-04, 6.98065442e-04, 1.88077427e-03,\n",
       "       3.79535778e-04, 2.98790247e-03, 1.17784729e-03, 6.99143273e-03,\n",
       "       1.05236976e-03, 5.15164460e-03, 5.50685221e-03, 1.33229710e-03,\n",
       "       7.65426310e-04, 3.32015474e-04, 3.46982119e-04, 1.11911732e-03,\n",
       "       2.23557808e-03, 2.10081927e-03, 3.51950714e-04, 6.32629018e-04,\n",
       "       7.46206355e-04, 1.89331297e-03, 9.92723775e-04, 1.72928425e-03,\n",
       "       1.06039410e-03, 1.52386337e-03, 3.65923756e-04, 1.04637581e-03,\n",
       "       4.76280675e-04, 1.31972411e-04, 3.55992804e-03, 9.93228794e-04,\n",
       "       1.18819185e-03, 6.16708314e-04, 4.52196811e-04, 5.60674216e-04,\n",
       "       6.95686981e-04, 2.34327780e-03, 2.91978739e-04, 1.03537181e-02,\n",
       "       2.12000935e-03, 3.03083265e-04, 1.68234298e-04, 1.09872941e-03,\n",
       "       1.12528726e-03, 9.75792625e-03, 1.61484167e-03, 2.16475489e-03,\n",
       "       4.51413568e-04, 4.82797790e-04, 6.92662763e-05, 1.30153572e-03,\n",
       "       5.48271519e-04, 2.35925452e-02, 7.93864222e-04, 6.95702377e-04,\n",
       "       3.01581163e-04, 3.05498885e-03, 3.22936944e-03, 1.66155694e-03,\n",
       "       1.46375010e-03, 5.47331469e-04, 3.22202812e-04, 7.22861542e-04,\n",
       "       2.06605766e-03, 9.31809909e-04, 2.42192267e-03, 1.61762561e-03,\n",
       "       5.49752291e-04, 4.64938801e-04, 5.64052933e-04, 1.01085823e-03,\n",
       "       1.45957647e-03, 1.63526952e-03, 4.69987223e-04, 1.36075677e-03,\n",
       "       3.67303795e-04, 7.98601776e-04, 2.16799318e-03, 3.26455570e-04,\n",
       "       1.04116569e-03, 6.57631998e-03, 8.68638305e-04, 2.72502776e-03,\n",
       "       1.02823261e-03, 1.22789448e-02, 2.06393474e-03, 1.76617763e-03,\n",
       "       1.07369420e-03, 2.80969172e-03, 5.97671690e-04, 3.27708727e-03,\n",
       "       3.28898782e-04, 5.95391478e-04, 1.22913397e-03, 1.90528405e-03,\n",
       "       9.04787331e-04, 9.60599225e-04, 5.43551082e-04, 1.35287170e-03,\n",
       "       9.56168702e-04, 8.71654877e-04, 1.26289623e-04, 4.01900866e-03,\n",
       "       2.35216724e-03, 5.43794161e-04, 2.52420953e-04, 3.62297691e-04,\n",
       "       9.21546645e-04, 8.25000448e-03, 5.47714690e-04, 4.68550104e-04,\n",
       "       1.04525874e-02, 3.93941435e-04, 4.82528608e-03, 5.84099839e-04,\n",
       "       1.02545954e-03, 9.75921686e-04, 2.08091293e-03, 1.25999162e-03,\n",
       "       8.72384461e-04, 1.60977736e-03, 7.00307300e-04, 6.16920765e-04,\n",
       "       1.59016855e-03, 1.43581437e-03, 1.17537786e-03, 4.17601340e-04,\n",
       "       1.03385956e-03, 2.10891937e-02, 1.78398850e-04, 8.93866831e-04,\n",
       "       1.39580264e-03, 5.87644728e-04, 3.75625071e-04, 1.72447041e-03,\n",
       "       2.17505624e-03, 2.97664591e-03, 1.86173634e-03, 2.13164725e-03,\n",
       "       2.79464145e-03, 1.73535906e-03, 5.37841299e-03, 2.36699456e-03,\n",
       "       1.82305541e-03, 1.00939987e-02, 4.37738865e-04, 4.63982439e-04,\n",
       "       1.79682655e-03, 2.45939567e-04, 1.72111471e-03, 2.24421539e-04,\n",
       "       3.86293628e-04, 1.47549728e-03, 1.86535328e-03, 1.37909286e-03,\n",
       "       3.95986523e-04, 5.14293327e-03, 4.67668556e-04, 1.21057371e-03,\n",
       "       2.14976591e-03, 2.23454772e-03, 3.69405100e-03, 7.50986328e-04,\n",
       "       2.52577458e-04, 4.49852316e-04, 2.18321784e-03, 6.33993013e-03,\n",
       "       9.67371306e-04, 2.51761795e-03, 1.37350177e-03, 1.93022626e-03,\n",
       "       1.14070395e-03, 1.67301318e-03, 3.30147542e-03, 1.92510814e-03,\n",
       "       2.30629899e-03, 7.78661763e-04, 5.10550975e-04, 2.95694685e-04,\n",
       "       2.41051660e-03, 8.67981411e-04, 3.05128201e-03, 2.82390676e-04,\n",
       "       6.26240814e-04, 6.50084459e-04, 8.08831412e-04, 8.60044143e-04,\n",
       "       2.19019321e-02, 5.21658545e-03, 2.87938622e-02, 3.44678009e-05,\n",
       "       5.99954124e-04, 2.18439946e-03, 0.00000000e+00, 4.22578805e-03,\n",
       "       2.44714920e-05, 1.12399033e-03, 2.41677273e-03, 3.51989251e-03,\n",
       "       3.87757991e-03, 2.31618419e-07, 6.61160260e-03, 0.00000000e+00,\n",
       "       4.30352827e-06, 3.56304670e-03, 2.45681074e-03, 5.05266217e-03,\n",
       "       6.27067294e-04, 3.73559923e-03, 2.28640300e-05, 0.00000000e+00,\n",
       "       8.02627812e-04, 5.95728324e-03, 0.00000000e+00, 3.29839236e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.06458085e-03, 2.42544382e-03,\n",
       "       4.42304151e-03, 3.98619278e-04, 8.51798299e-04, 1.06418726e-04,\n",
       "       7.87858074e-03, 1.14903130e-04, 1.11858326e-02, 2.65584434e-03,\n",
       "       5.59856121e-03, 4.24062043e-04, 2.24517770e-04, 2.49355977e-03,\n",
       "       6.46140237e-04, 4.57577048e-02, 6.68666534e-03, 2.61773909e-03,\n",
       "       3.49349050e-04, 4.42756502e-03, 6.83111736e-03, 4.96310884e-04,\n",
       "       3.16015250e-04, 1.22132502e-02, 7.95790188e-04, 1.11397087e-03,\n",
       "       1.15678817e-04, 4.79750002e-03, 1.27029584e-03, 1.61407720e-02,\n",
       "       2.83121762e-04, 4.19511937e-04, 2.38468672e-04, 5.53138257e-03,\n",
       "       3.29109913e-04, 7.82907802e-05, 2.83738186e-03, 2.42318780e-03,\n",
       "       8.97148719e-04, 2.23261112e-03, 1.03531321e-03, 3.10336767e-04,\n",
       "       3.57112668e-03, 2.22304890e-04, 2.99703888e-04, 3.59149510e-04,\n",
       "       3.74536461e-04, 6.42077905e-04, 1.80043030e-02, 1.35525487e-04,\n",
       "       2.04177427e-03])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021321961620469083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False,  True, False,  True, False,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "        True,  True, False,  True, False, False, False,  True, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False,  True, False,  True, False, False,  True, False,\n",
       "        True, False,  True,  True,  True,  True, False, False, False,\n",
       "       False, False, False, False, False, False,  True,  True, False,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "       False,  True, False, False, False,  True, False, False, False,\n",
       "        True, False, False, False,  True,  True,  True, False, False,\n",
       "       False, False,  True, False, False, False, False,  True, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True, False, False, False, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False,  True, False,  True, False, False, False, False,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False,  True, False,  True, False,  True, False, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True,  True, False,\n",
       "       False, False, False,  True, False, False,  True, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False,  True,  True, False, False,  True, False,\n",
       "        True,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True,  True,  True, False, False, False,  True,  True, False,\n",
       "        True, False, False, False, False,  True, False,  True, False,\n",
       "       False, False,  True, False,  True, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False,  True, False,  True,\n",
       "       False, False,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True,  True, False,  True, False, False, False,  True,\n",
       "       False,  True, False, False,  True,  True,  True, False, False,\n",
       "       False,  True, False,  True,  True,  True, False, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True, False, False,\n",
       "        True, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False, False,  True,  True, False,  True, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_11</th>\n",
       "      <th>tfidf_12</th>\n",
       "      <th>tfidf_20</th>\n",
       "      <th>tfidf_21</th>\n",
       "      <th>tfidf_30</th>\n",
       "      <th>tfidf_35</th>\n",
       "      <th>tfidf_36</th>\n",
       "      <th>...</th>\n",
       "      <th>grapefruit</th>\n",
       "      <th>lemon</th>\n",
       "      <th>mint</th>\n",
       "      <th>orange</th>\n",
       "      <th>pine</th>\n",
       "      <th>pungent</th>\n",
       "      <th>rose</th>\n",
       "      <th>skunk</th>\n",
       "      <th>sweet</th>\n",
       "      <th>vanilla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tfidf_3   tfidf_5  tfidf_7  tfidf_11  tfidf_12  tfidf_20  tfidf_21  \\\n",
       "0          0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "1          0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "2          0.0  0.145484      0.0  0.000000       0.0       0.0       0.0   \n",
       "3          0.0  0.145484      0.0  0.000000       0.0       0.0       0.0   \n",
       "4          0.0  0.000000      0.0  0.165804       0.0       0.0       0.0   \n",
       "...        ...       ...      ...       ...       ...       ...       ...   \n",
       "74995      0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "74996      0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "74997      0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "74998      0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "74999      0.0  0.000000      0.0  0.000000       0.0       0.0       0.0   \n",
       "\n",
       "       tfidf_30  tfidf_35  tfidf_36  ...  grapefruit  lemon  mint  orange  \\\n",
       "0           0.0  0.000000       0.0  ...           0      0     0       0   \n",
       "1           0.0  0.000000       0.0  ...           0      0     0       0   \n",
       "2           0.0  0.213037       0.0  ...           0      0     0       0   \n",
       "3           0.0  0.213037       0.0  ...           0      0     0       0   \n",
       "4           0.0  0.000000       0.0  ...           0      0     0       0   \n",
       "...         ...       ...       ...  ...         ...    ...   ...     ...   \n",
       "74995       0.0  0.000000       0.0  ...           0      0     0       0   \n",
       "74996       0.0  0.000000       0.0  ...           0      0     0       0   \n",
       "74997       0.0  0.000000       0.0  ...           0      0     0       0   \n",
       "74998       0.0  0.000000       0.0  ...           1      1     1       1   \n",
       "74999       0.0  0.000000       0.0  ...           1      1     1       1   \n",
       "\n",
       "       pine  pungent  rose  skunk  sweet  vanilla  \n",
       "0         0        0     0      0      0        0  \n",
       "1         0        0     0      0      0        1  \n",
       "2         1        0     0      0      0        0  \n",
       "3         1        0     0      0      0        0  \n",
       "4         0        0     0      0      0        0  \n",
       "...     ...      ...   ...    ...    ...      ...  \n",
       "74995     0        0     0      0      0        0  \n",
       "74996     0        0     0      0      0        0  \n",
       "74997     0        0     0      0      0        0  \n",
       "74998     1        1     1      1      1        1  \n",
       "74999     1        1     1      1      1        1  \n",
       "\n",
       "[75000 rows x 126 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_X = df_rf[selected_features]\n",
    "selected_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split (after Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selected_X_rf_tfidf_cbc.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(selector, \"selector_rf_tfidf_cbc.pkl\")\n",
    "joblib.dump(selected_X, \"selected_X_rf_tfidf_cbc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selected_X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_8091/3758305.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    }
   ],
   "source": [
    "rfreg.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04519975309314101"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010132698576862715"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10066130625450236"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9660470986565023"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666857686953523"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [None, 10, 50, 100],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [100, 300, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(rfreg,  \n",
    "                     parameters,   \n",
    "                     cv=5, \n",
    "                     scoring='neg_mean_absolute_error',\n",
    "                     n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [None, 10, 50, 100],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 300, 500]},\n",
       "                   scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rscv_rf_tfidf_best_params_cbc.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rscv, \"rscv_rf_tfidf_cbc.pkl\")\n",
    "joblib.dump(rscv.best_params_, \"rscv_rf_tfidf_best_params_cbc.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF (after Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_8091/2637630324.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg_ht.fit(X_train1, y_train1)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "rfreg_ht = RandomForestRegressor(n_estimators = 100, min_samples_split = 2, min_samples_leaf = 1, max_features = 'auto', max_depth = None)\n",
    "rfreg_ht.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg_ht.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg_ht.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045304561706498"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010106798028300586"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10053257197694977"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659762218345742"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8670265379085818"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots after Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg_test = rfreg_ht.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_rfreg_tfidf_cbc.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_pred_rfreg_test, \"y_pred_rfreg_test_tfidf_cbc.pkl\")\n",
    "joblib.dump(y_test, \"y_test_rfreg_tfidf_cbc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04731499465364366"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010599334994077936"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10295307180496334"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8607216744358513"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assume y_pred is a numpy array and y_true is a pandas dataframe\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "column = \"X..CBC\"  # specify the target variable name\n",
    "ax.hist(y_pred_rfreg_test, alpha=0.5, label='y_pred', bins=20)\n",
    "ax.hist(y_test[column], alpha=0.5, label='y_true', bins=20)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(column)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('error_hist_knn_tfidf_cbc.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.934\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "corr_coef, p_value = pearsonr(y_pred_rfreg_test.flatten(), y_test.values.ravel())\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr_coef:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix = y_test.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
