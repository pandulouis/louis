{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = pd.read_csv(\"df_cbd_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'tfidf_0',\n",
       " 'tfidf_1',\n",
       " 'tfidf_2',\n",
       " 'tfidf_3',\n",
       " 'tfidf_4',\n",
       " 'tfidf_5',\n",
       " 'tfidf_6',\n",
       " 'tfidf_7',\n",
       " 'tfidf_8',\n",
       " 'tfidf_9',\n",
       " 'tfidf_10',\n",
       " 'tfidf_11',\n",
       " 'tfidf_12',\n",
       " 'tfidf_13',\n",
       " 'tfidf_14',\n",
       " 'tfidf_15',\n",
       " 'tfidf_16',\n",
       " 'tfidf_17',\n",
       " 'tfidf_18',\n",
       " 'tfidf_19',\n",
       " 'tfidf_20',\n",
       " 'tfidf_21',\n",
       " 'tfidf_22',\n",
       " 'tfidf_23',\n",
       " 'tfidf_24',\n",
       " 'tfidf_25',\n",
       " 'tfidf_26',\n",
       " 'tfidf_27',\n",
       " 'tfidf_28',\n",
       " 'tfidf_29',\n",
       " 'tfidf_30',\n",
       " 'tfidf_31',\n",
       " 'tfidf_32',\n",
       " 'tfidf_33',\n",
       " 'tfidf_34',\n",
       " 'tfidf_35',\n",
       " 'tfidf_36',\n",
       " 'tfidf_37',\n",
       " 'tfidf_38',\n",
       " 'tfidf_39',\n",
       " 'tfidf_40',\n",
       " 'tfidf_41',\n",
       " 'tfidf_42',\n",
       " 'tfidf_43',\n",
       " 'tfidf_44',\n",
       " 'tfidf_45',\n",
       " 'tfidf_46',\n",
       " 'tfidf_47',\n",
       " 'tfidf_48',\n",
       " 'tfidf_49',\n",
       " 'tfidf_50',\n",
       " 'tfidf_51',\n",
       " 'tfidf_52',\n",
       " 'tfidf_53',\n",
       " 'tfidf_54',\n",
       " 'tfidf_55',\n",
       " 'tfidf_56',\n",
       " 'tfidf_57',\n",
       " 'tfidf_58',\n",
       " 'tfidf_59',\n",
       " 'tfidf_60',\n",
       " 'tfidf_61',\n",
       " 'tfidf_62',\n",
       " 'tfidf_63',\n",
       " 'tfidf_64',\n",
       " 'tfidf_65',\n",
       " 'tfidf_66',\n",
       " 'tfidf_67',\n",
       " 'tfidf_68',\n",
       " 'tfidf_69',\n",
       " 'tfidf_70',\n",
       " 'tfidf_71',\n",
       " 'tfidf_72',\n",
       " 'tfidf_73',\n",
       " 'tfidf_74',\n",
       " 'tfidf_75',\n",
       " 'tfidf_76',\n",
       " 'tfidf_77',\n",
       " 'tfidf_78',\n",
       " 'tfidf_79',\n",
       " 'tfidf_80',\n",
       " 'tfidf_81',\n",
       " 'tfidf_82',\n",
       " 'tfidf_83',\n",
       " 'tfidf_84',\n",
       " 'tfidf_85',\n",
       " 'tfidf_86',\n",
       " 'tfidf_87',\n",
       " 'tfidf_88',\n",
       " 'tfidf_89',\n",
       " 'tfidf_90',\n",
       " 'tfidf_91',\n",
       " 'tfidf_92',\n",
       " 'tfidf_93',\n",
       " 'tfidf_94',\n",
       " 'tfidf_95',\n",
       " 'tfidf_96',\n",
       " 'tfidf_97',\n",
       " 'tfidf_98',\n",
       " 'tfidf_99',\n",
       " 'tfidf_100',\n",
       " 'tfidf_101',\n",
       " 'tfidf_102',\n",
       " 'tfidf_103',\n",
       " 'tfidf_104',\n",
       " 'tfidf_105',\n",
       " 'tfidf_106',\n",
       " 'tfidf_107',\n",
       " 'tfidf_108',\n",
       " 'tfidf_109',\n",
       " 'tfidf_110',\n",
       " 'tfidf_111',\n",
       " 'tfidf_112',\n",
       " 'tfidf_113',\n",
       " 'tfidf_114',\n",
       " 'tfidf_115',\n",
       " 'tfidf_116',\n",
       " 'tfidf_117',\n",
       " 'tfidf_118',\n",
       " 'tfidf_119',\n",
       " 'tfidf_120',\n",
       " 'tfidf_121',\n",
       " 'tfidf_122',\n",
       " 'tfidf_123',\n",
       " 'tfidf_124',\n",
       " 'tfidf_125',\n",
       " 'tfidf_126',\n",
       " 'tfidf_127',\n",
       " 'tfidf_128',\n",
       " 'tfidf_129',\n",
       " 'tfidf_130',\n",
       " 'tfidf_131',\n",
       " 'tfidf_132',\n",
       " 'tfidf_133',\n",
       " 'tfidf_134',\n",
       " 'tfidf_135',\n",
       " 'tfidf_136',\n",
       " 'tfidf_137',\n",
       " 'tfidf_138',\n",
       " 'tfidf_139',\n",
       " 'tfidf_140',\n",
       " 'tfidf_141',\n",
       " 'tfidf_142',\n",
       " 'tfidf_143',\n",
       " 'tfidf_144',\n",
       " 'tfidf_145',\n",
       " 'tfidf_146',\n",
       " 'tfidf_147',\n",
       " 'tfidf_148',\n",
       " 'tfidf_149',\n",
       " 'tfidf_150',\n",
       " 'tfidf_151',\n",
       " 'tfidf_152',\n",
       " 'tfidf_153',\n",
       " 'tfidf_154',\n",
       " 'tfidf_155',\n",
       " 'tfidf_156',\n",
       " 'tfidf_157',\n",
       " 'tfidf_158',\n",
       " 'tfidf_159',\n",
       " 'tfidf_160',\n",
       " 'tfidf_161',\n",
       " 'tfidf_162',\n",
       " 'tfidf_163',\n",
       " 'tfidf_164',\n",
       " 'tfidf_165',\n",
       " 'tfidf_166',\n",
       " 'tfidf_167',\n",
       " 'tfidf_168',\n",
       " 'tfidf_169',\n",
       " 'tfidf_170',\n",
       " 'tfidf_171',\n",
       " 'tfidf_172',\n",
       " 'tfidf_173',\n",
       " 'tfidf_174',\n",
       " 'tfidf_175',\n",
       " 'tfidf_176',\n",
       " 'tfidf_177',\n",
       " 'tfidf_178',\n",
       " 'tfidf_179',\n",
       " 'tfidf_180',\n",
       " 'tfidf_181',\n",
       " 'tfidf_182',\n",
       " 'tfidf_183',\n",
       " 'tfidf_184',\n",
       " 'tfidf_185',\n",
       " 'tfidf_186',\n",
       " 'tfidf_187',\n",
       " 'tfidf_188',\n",
       " 'tfidf_189',\n",
       " 'tfidf_190',\n",
       " 'tfidf_191',\n",
       " 'tfidf_192',\n",
       " 'tfidf_193',\n",
       " 'tfidf_194',\n",
       " 'tfidf_195',\n",
       " 'tfidf_196',\n",
       " 'tfidf_197',\n",
       " 'tfidf_198',\n",
       " 'tfidf_199',\n",
       " 'tfidf_200',\n",
       " 'tfidf_201',\n",
       " 'tfidf_202',\n",
       " 'tfidf_203',\n",
       " 'tfidf_204',\n",
       " 'tfidf_205',\n",
       " 'tfidf_206',\n",
       " 'tfidf_207',\n",
       " 'tfidf_208',\n",
       " 'tfidf_209',\n",
       " 'tfidf_210',\n",
       " 'tfidf_211',\n",
       " 'tfidf_212',\n",
       " 'tfidf_213',\n",
       " 'tfidf_214',\n",
       " 'tfidf_215',\n",
       " 'tfidf_216',\n",
       " 'tfidf_217',\n",
       " 'tfidf_218',\n",
       " 'tfidf_219',\n",
       " 'tfidf_220',\n",
       " 'tfidf_221',\n",
       " 'tfidf_222',\n",
       " 'tfidf_223',\n",
       " 'tfidf_224',\n",
       " 'tfidf_225',\n",
       " 'tfidf_226',\n",
       " 'tfidf_227',\n",
       " 'tfidf_228',\n",
       " 'tfidf_229',\n",
       " 'tfidf_230',\n",
       " 'tfidf_231',\n",
       " 'tfidf_232',\n",
       " 'tfidf_233',\n",
       " 'tfidf_234',\n",
       " 'tfidf_235',\n",
       " 'tfidf_236',\n",
       " 'tfidf_237',\n",
       " 'tfidf_238',\n",
       " 'tfidf_239',\n",
       " 'tfidf_240',\n",
       " 'tfidf_241',\n",
       " 'tfidf_242',\n",
       " 'tfidf_243',\n",
       " 'tfidf_244',\n",
       " 'tfidf_245',\n",
       " 'tfidf_246',\n",
       " 'tfidf_247',\n",
       " 'tfidf_248',\n",
       " 'tfidf_249',\n",
       " 'tfidf_250',\n",
       " 'tfidf_251',\n",
       " 'tfidf_252',\n",
       " 'tfidf_253',\n",
       " 'tfidf_254',\n",
       " 'tfidf_255',\n",
       " 'tfidf_256',\n",
       " 'tfidf_257',\n",
       " 'tfidf_258',\n",
       " 'tfidf_259',\n",
       " 'tfidf_260',\n",
       " 'tfidf_261',\n",
       " 'tfidf_262',\n",
       " 'tfidf_263',\n",
       " 'tfidf_264',\n",
       " 'tfidf_265',\n",
       " 'tfidf_266',\n",
       " 'tfidf_267',\n",
       " 'tfidf_268',\n",
       " 'tfidf_269',\n",
       " 'tfidf_270',\n",
       " 'tfidf_271',\n",
       " 'tfidf_272',\n",
       " 'tfidf_273',\n",
       " 'tfidf_274',\n",
       " 'tfidf_275',\n",
       " 'tfidf_276',\n",
       " 'tfidf_277',\n",
       " 'tfidf_278',\n",
       " 'tfidf_279',\n",
       " 'tfidf_280',\n",
       " 'tfidf_281',\n",
       " 'tfidf_282',\n",
       " 'tfidf_283',\n",
       " 'tfidf_284',\n",
       " 'tfidf_285',\n",
       " 'tfidf_286',\n",
       " 'tfidf_287',\n",
       " 'tfidf_288',\n",
       " 'tfidf_289',\n",
       " 'tfidf_290',\n",
       " 'tfidf_291',\n",
       " 'tfidf_292',\n",
       " 'tfidf_293',\n",
       " 'tfidf_294',\n",
       " 'tfidf_295',\n",
       " 'tfidf_296',\n",
       " 'tfidf_297',\n",
       " 'tfidf_298',\n",
       " 'tfidf_299',\n",
       " 'tfidf_300',\n",
       " 'tfidf_301',\n",
       " 'tfidf_302',\n",
       " 'tfidf_303',\n",
       " 'tfidf_304',\n",
       " 'tfidf_305',\n",
       " 'tfidf_306',\n",
       " 'tfidf_307',\n",
       " 'tfidf_308',\n",
       " 'tfidf_309',\n",
       " 'tfidf_310',\n",
       " 'tfidf_311',\n",
       " 'tfidf_312',\n",
       " 'tfidf_313',\n",
       " 'tfidf_314',\n",
       " 'tfidf_315',\n",
       " 'tfidf_316',\n",
       " 'tfidf_317',\n",
       " 'tfidf_318',\n",
       " 'tfidf_319',\n",
       " 'tfidf_320',\n",
       " 'tfidf_321',\n",
       " 'tfidf_322',\n",
       " 'tfidf_323',\n",
       " 'tfidf_324',\n",
       " 'tfidf_325',\n",
       " 'tfidf_326',\n",
       " 'tfidf_327',\n",
       " 'tfidf_328',\n",
       " 'tfidf_329',\n",
       " 'tfidf_330',\n",
       " 'tfidf_331',\n",
       " 'tfidf_332',\n",
       " 'tfidf_333',\n",
       " 'tfidf_334',\n",
       " 'tfidf_335',\n",
       " 'tfidf_336',\n",
       " 'tfidf_337',\n",
       " 'tfidf_338',\n",
       " 'tfidf_339',\n",
       " 'tfidf_340',\n",
       " 'tfidf_341',\n",
       " 'tfidf_342',\n",
       " 'tfidf_343',\n",
       " 'tfidf_344',\n",
       " 'tfidf_345',\n",
       " 'tfidf_346',\n",
       " 'tfidf_347',\n",
       " 'tfidf_348',\n",
       " 'tfidf_349',\n",
       " 'tfidf_350',\n",
       " 'tfidf_351',\n",
       " 'tfidf_352',\n",
       " 'tfidf_353',\n",
       " 'tfidf_354',\n",
       " 'tfidf_355',\n",
       " 'tfidf_356',\n",
       " 'tfidf_357',\n",
       " 'tfidf_358',\n",
       " 'tfidf_359',\n",
       " 'tfidf_360',\n",
       " 'tfidf_361',\n",
       " 'tfidf_362',\n",
       " 'tfidf_363',\n",
       " 'tfidf_364',\n",
       " 'tfidf_365',\n",
       " 'tfidf_366',\n",
       " 'tfidf_367',\n",
       " 'tfidf_368',\n",
       " 'tfidf_369',\n",
       " 'tfidf_370',\n",
       " 'tfidf_371',\n",
       " 'tfidf_372',\n",
       " 'tfidf_373',\n",
       " 'tfidf_374',\n",
       " 'tfidf_375',\n",
       " 'tfidf_376',\n",
       " 'tfidf_377',\n",
       " 'tfidf_378',\n",
       " 'tfidf_379',\n",
       " 'tfidf_380',\n",
       " 'tfidf_381',\n",
       " 'tfidf_382',\n",
       " 'tfidf_383',\n",
       " 'tfidf_384',\n",
       " 'tfidf_385',\n",
       " 'tfidf_386',\n",
       " 'tfidf_387',\n",
       " 'hybrid',\n",
       " 'indica',\n",
       " 'sativa',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'aroused',\n",
       " 'arthritis',\n",
       " 'creative',\n",
       " 'depression',\n",
       " 'dizzy',\n",
       " 'dry eyes',\n",
       " 'dry mouth',\n",
       " 'energetic',\n",
       " 'epilepsy',\n",
       " 'euphoric',\n",
       " 'eye pressure',\n",
       " 'fatigue',\n",
       " 'focused',\n",
       " 'giggly',\n",
       " 'happy',\n",
       " 'headache',\n",
       " 'hungry',\n",
       " 'migraines',\n",
       " 'pain',\n",
       " 'paranoid',\n",
       " 'relaxed',\n",
       " 'seizures',\n",
       " 'sleepy',\n",
       " 'spasticity',\n",
       " 'stress',\n",
       " 'talkative',\n",
       " 'tingly',\n",
       " 'uplifted',\n",
       " 'ammonia',\n",
       " 'apple',\n",
       " 'apricot',\n",
       " 'berry',\n",
       " 'blue cheese',\n",
       " 'blueberry',\n",
       " 'butter',\n",
       " 'cheese',\n",
       " 'chemical',\n",
       " 'chestnut',\n",
       " 'citrus',\n",
       " 'coffee',\n",
       " 'diesel',\n",
       " 'earthy',\n",
       " 'flowery',\n",
       " 'fruit',\n",
       " 'grape',\n",
       " 'grapefruit',\n",
       " 'honey',\n",
       " 'lavender',\n",
       " 'lemon',\n",
       " 'lime',\n",
       " 'mango',\n",
       " 'menthol',\n",
       " 'mint',\n",
       " 'nutty',\n",
       " 'orange',\n",
       " 'peach',\n",
       " 'pear',\n",
       " 'pepper',\n",
       " 'pine',\n",
       " 'pineapple',\n",
       " 'plum',\n",
       " 'pungent',\n",
       " 'rose',\n",
       " 'sage',\n",
       " 'skunk',\n",
       " 'spicy/herbal',\n",
       " 'strawberry',\n",
       " 'sweet',\n",
       " 'tar',\n",
       " 'tea',\n",
       " 'tobacco',\n",
       " 'tree',\n",
       " 'tropical',\n",
       " 'vanilla',\n",
       " 'violet',\n",
       " 'woody',\n",
       " 'X..CBD']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_rf.drop(['index', 'X..CBD'], axis = 1)\n",
    "y = df_rf[['X..CBD']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting histograms on target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00104646],\n",
       "       [0.00104646],\n",
       "       [0.00104646],\n",
       "       ...,\n",
       "       [0.00104646],\n",
       "       [0.00104646],\n",
       "       [0.00104646]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array = y.to_numpy()\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD4CAYAAADGmmByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZbklEQVR4nO3df5BV5Z3n8fcngEDUNvxoXZbutltDVMQxI60yiTulYRM72amAWzrbWSdg1myXyGQdTZzAJFm3aouUqVgJY2bVIuoCWVbCOE4ku0NGFo1uKvyYNhIRCUOPKPTISAccaeJKBL/7x31ar83t7kuf+4NLf15Vt+653/M89zwP6P1wftxzFRGYmZkN1weqPQAzM6ttDhIzM8vEQWJmZpk4SMzMLBMHiZmZZTK62gOotMmTJ0dzc3O1h2FmVlOeffbZX0dEfaF1Iy5Impub6ezsrPYwzMxqiqRXBlrnQ1tmZpaJg8TMzDJxkJiZWSYj7hyJWb63336b7u5u3nrrrWoP5aQybtw4GhoaGDNmTLWHYjXAQWIjWnd3N2eeeSbNzc1IqvZwTgoRwYEDB+ju7qalpaXaw7Ea4ENbNqK99dZbTJo0ySGSRxKTJk3yXpoVzUFiI55D5Hj+M7ET4SAxM7NMHCRmeRqbzkVSyR6NTecOur29e/fS0tLCwYMHAXj99ddpaWnhlVcG/O4XAPfccw8XXnghM2bM4NJLL2XlypUAXH311VxwwQV89KMf5aKLLmLZsmXv9mlubuaSSy7hkksuYfr06Xz961/nyJEjGf/EzHyy3YbQ2HQu3Xv3VGXbDY1N7N0z+AdqqXXv3cN3nthZsve741MXDLq+sbGRBQsWsGjRIpYtW8aiRYvo6Ojg3HMHDqAHHniA9evXs2XLFurq6njjjTf40Y9+9O76VatW0draysGDBzn//PO56aabOO200wB46qmnmDx5MocPH6ajo4OOjg5WrFhRkrlacU7F/6ccJDaoUn+wnoihPoRPFbfffjszZ85k6dKl/OxnP+N73/veoO2/+c1v8tRTT1FXVwfAWWedxfz5849rd/jwYU4//XRGjRp13LozzjiDBx54gMbGRg4ePMjEiRNLMxkb0qn4/5SDxKzKxowZw7e//W3a2tp44okn3t17KKS3t5fe3l7OP//8AdvceOONjB07ll27drF06dKCQQJQV1dHS0sLu3bt4sorr8w8Dxu5fI7kBJT6+Hkpj7VbbVu3bh1TpkzhhRdeGLRdRAx5RdWqVat4/vnn2bNnD/fcc8+g51siYljjNcvnPZITcCruklr1bd26lfXr17Np0yauuuoq2tvbmTJlSsG2dXV1nH766bz00kucd955g75vfX09l112GZs3by54zqW3t5eXX36Zj3zkIyWZh41c3iMxq6KIYMGCBSxdupSmpibuvPNOvvKVrwzaZ/HixSxcuJBDhw4BcOjQofddndXnzTff5Lnnnit4GOzw4cPceuutzJ07lwkTJpRmMjZieY/ELE9DY1NJ9/4aGpsGXf/973+fpqYmPvnJTwJw6623snz5cp5++mluu+02tm7dCsAXv/hFbrnlFlpbW1mwYAGHDx/m8ssvZ8yYMYwZM4Yvf/nL777njTfeyPjx4zly5Ag33XQTM2fOfHfdNddcQ0TwzjvvcN111/GNb3yjZHO1kUsj7Rhpa2trDPeHrSRV9dBWNf6uTvU579ixg4suuqis26hV/rMpj1r9f0rSsxHRWmidD22ZmVkmDhIzM8ukbEEi6WFJ+yUddz2jpK9ICkmT82qLJXVJ2inp2rz6TEnb0rp7la59lDRW0g9TfbOk5nLNxU5tI+3wbjH8Z2Inopx7JMuBtv5FSY3AJ4E9ebXpQDtwcepzn6S+b1HdD3QA09Kj7z1vBl6PiA8D3wW+VZZZ2Clt3LhxHDhwwB+cefp+j2TcuHHVHorViLJdtRURzwywl/Bd4E+Bx/Nqc4DVEXEE2C2pC7hC0stAXURsBJC0EpgLrEt9/kvq/yjwF5IU/kSwE9DQ0EB3dzc9PT3VHspJpe8XEs2KUdHLfyV9FvjHiPhlv2/nTgU25b3uTrW303L/el+fvQARcVTSG8Ak4NflGb2disaMGeNfATTLqGJBIumDwNeATxVaXaAWg9QH61No2x3kDo/R1DT4df1mZnZiKnnV1vlAC/DLdMiqAfiFpH9Bbk+jMa9tA/BqqjcUqJPfR9Jo4CzgYKENR8SyiGiNiNb6+vqSTcjMzCoYJBGxLSLOjojmiGgmFwSXRcQ/AWuB9nQlVgu5k+pbImIf0CtpVrpaax7vnVtZC/TdO/t64EmfHzEzq7xyXv77CLARuEBSt6SbB2obEduBNcCLwE+AhRFxLK1eADwIdAH/QO5EO8BDwKR0Yv4OYFFZJmJmZoMq51VbnxtifXO/10uAJQXadQIzCtTfAm7INkozM8vK32w3M7NMHCRmZpaJg8TsJOFf4LRa5d8jMTtJ+Bc4rVZ5j8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMyhYkkh6WtF/SC3m1b0v6laTnJf21pA/lrVssqUvSTknX5tVnStqW1t0rSak+VtIPU32zpOZyzcXMzAZWzj2S5UBbv9p6YEZE/A7w98BiAEnTgXbg4tTnPkmjUp/7gQ5gWnr0vefNwOsR8WHgu8C3yjYTMzMbUNmCJCKeAQ72qz0REUfTy01AQ1qeA6yOiCMRsRvoAq6QNAWoi4iNERHASmBuXp8VaflRYHbf3oqZmVVONc+R/AdgXVqeCuzNW9edalPTcv/6+/qkcHoDmFRoQ5I6JHVK6uzp6SnZBMzMrEpBIulrwFFgVV+pQLMYpD5Yn+OLEcsiojUiWuvr6090uGZmNoiKB4mk+cAfADemw1WQ29NozGvWALya6g0F6u/rI2k0cBb9DqWZmVn5VTRIJLUBXwU+GxFv5q1aC7SnK7FayJ1U3xIR+4BeSbPS+Y95wON5fean5euBJ/OCyczMKmR0ud5Y0iPA1cBkSd3AXeSu0hoLrE/nxTdFxC0RsV3SGuBFcoe8FkbEsfRWC8hdATae3DmVvvMqDwE/kNRFbk+kvVxzMTOzgZUtSCLicwXKDw3SfgmwpEC9E5hRoP4WcEOWMZqZWXb+ZruZmWXiIDEzs0wcJGZmlomDxMzMMnGQmJlZJg4SMzPLxEFiZmaZOEjMzCwTB4mZmWXiIDEzs0wcJGZmlomDxMzMMnGQmJlZJg4SMzPLxEFiZmaZOEjMzCwTB4mZmWXiIDEzs0wcJGZmlknZgkTSw5L2S3ohrzZR0npJu9LzhLx1iyV1Sdop6dq8+kxJ29K6eyUp1cdK+mGqb5bUXK65mJnZwMq5R7IcaOtXWwRsiIhpwIb0GknTgXbg4tTnPkmjUp/7gQ5gWnr0vefNwOsR8WHgu8C3yjYTMzMbUNmCJCKeAQ72K88BVqTlFcDcvPrqiDgSEbuBLuAKSVOAuojYGBEBrOzXp++9HgVm9+2tmJlZ5VT6HMk5EbEPID2fnepTgb157bpTbWpa7l9/X5+IOAq8AUwqtFFJHZI6JXX29PSUaCpmZgYnz8n2QnsSMUh9sD7HFyOWRURrRLTW19cPc4hmZlZIpYPktXS4ivS8P9W7gca8dg3Aq6neUKD+vj6SRgNncfyhNDMzK7NKB8laYH5ang88nldvT1ditZA7qb4lHf7qlTQrnf+Y169P33tdDzyZzqOYmVkFjS7XG0t6BLgamCypG7gLuBtYI+lmYA9wA0BEbJe0BngROAosjIhj6a0WkLsCbDywLj0AHgJ+IKmL3J5Ie7nmYmZmAytbkETE5wZYNXuA9kuAJQXqncCMAvW3SEFkZmbVc7KcbDczsxrlIDEzs0wcJGZmlomDxMzMMnGQmJlZJg4SMzPLxEFiZmaZOEjMzCwTB4mZmWXiIDEzs0wcJGZmlklRQSLp48XUzMxs5Cl2j+R7RdbMzGyEGfTuv5J+D/gYUC/pjrxVdcCocg7MzMxqw1C3kT8NOCO1OzOvfojcj0mZmdkIN2iQRMTTwNOSlkfEKxUak5mZ1ZBif9hqrKRlQHN+n4j4RDkGZWZmtaPYIPlL4AHgQeDYEG3NzGwEKfaqraMRcX9EbImIZ/sew92opNslbZf0gqRHJI2TNFHSekm70vOEvPaLJXVJ2inp2rz6TEnb0rp7JWm4YzIzs+EpNkh+LOlWSVPSB/5ESROHs0FJU4H/BLRGxAxyV3+1A4uADRExDdiQXiNpelp/MdAG3Cep74qx+4EOYFp6tA1nTGZmNnzFBsl84E7g58Cz6dGZYbujgfGSRgMfBF4F5gAr0voVwNy0PAdYHRFHImI30AVcIWkKUBcRGyMigJV5fczMrEKKOkcSES2l2mBE/KOke4A9wP8DnoiIJySdExH7Upt9ks5OXaYCm/LeojvV3k7L/evHkdRBbs+FpqamUk3FzMwoMkgkzStUj4iVJ7rBdO5jDtAC/DPwl5L+aLAuhTY9SP34YsQyYBlAa2trwTZmZjY8xV61dXne8jhgNvALcoeTTtS/BnZHRA+ApMfIfXv+NUlT0t7IFGB/at8NNOb1byB3KKw7Lfevm5lZBRV7aOtL+a8lnQX8YJjb3APMkvRBcoe2ZpM73/Ibcudi7k7Pj6f2a4H/Kek7wL8kd1J9S0Qck9QraRawGZiH7/9lZlZxxe6R9PcmuQ/0ExYRmyU9Sm6P5ijwHLnDTmcAayTdTC5sbkjtt0taA7yY2i+MiL7vsiwAlgPjgXXpYWZmFVTsOZIf8975h1HARcCa4W40Iu4C7upXPkJu76RQ+yXAkgL1TmDGcMdhZmbZFbtHck/e8lHglYjoHqixmZmNHEV9jyTdvPFX5O4APAH4bTkHZWZmtaPYX0j8Q2ALufMWfwhsluTbyJuZWdGHtr4GXB4R+wEk1QP/B3i0XAMzM7PaUOwtUj7QFyLJgRPoa2Zmp7Bi90h+IulvgUfS638H/E15hmRmZrVkqN9s/zBwTkTcKenfAleRuzXJRmBVBcZnZmYnuaEOTy0FegEi4rGIuCMibie3N7K0vEMzM7NaMFSQNEfE8/2L6YuAzWUZkZmZ1ZShgmTcIOvGl3IgZmZWm4YKkr+T9B/7F9P9sIb9U7tmZnbqGOqqrT8B/lrSjbwXHK3AacB1ZRyXmZnViEGDJCJeAz4m6Rreuzni/46IJ8s+MjMzqwnF/h7JU8BTZR6LmZnVIH873czMMnGQmJlZJg4SMzPLxEFiZmaZOEjMzCyTqgSJpA9JelTSryTtkPR7kiZKWi9pV3qekNd+saQuSTslXZtXnylpW1p3ryRVYz5mZiNZtfZI/hz4SURcCFwK7AAWARsiYhqwIb1G0nSgHbgYaAPukzQqvc/9QAcwLT3aKjkJMzOrQpBIqgN+H3gIICJ+GxH/DMwBVqRmK4C5aXkOsDoijkTEbqALuELSFKAuIjZGRAAr8/qYmVmFVGOP5DygB/jvkp6T9KCk08n97sk+gPR8dmo/Fdib17871aam5f7140jqkNQpqbOnp6e0szEzG+GqESSjgcuA+yPid4HfkA5jDaDQeY8YpH58MWJZRLRGRGt9ff2JjtfMzAZRjSDpBrojYnN6/Si5YHktHa4iPe/Pa9+Y178BeDXVGwrUzcysgioeJBHxT8BeSRek0mzgRWAtMD/V5gOPp+W1QLuksZJayJ1U35IOf/VKmpWu1pqX18fMzCqkqJs2lsGXgFWSTgNeAr5ALtTWpN862QPcABAR2yWtIRc2R4GFEXEsvc8CYDm5H9lalx5mZlZBVQmSiNhK7ndN+ps9QPslwJIC9U7eu729mZlVgb/ZbmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsk6oFiaRRkp6T9L/S64mS1kvalZ4n5LVdLKlL0k5J1+bVZ0raltbdK0nVmIuZ2UhWzT2S24Adea8XARsiYhqwIb1G0nSgHbgYaAPukzQq9bkf6ACmpUdbZYZuZmZ9qhIkkhqAfwM8mFeeA6xIyyuAuXn11RFxJCJ2A13AFZKmAHURsTEiAliZ18fMzCqkWnskS4E/Bd7Jq50TEfsA0vPZqT4V2JvXrjvVpqbl/nUzM6ugigeJpD8A9kfEs8V2KVCLQeqFttkhqVNSZ09PT5GbNTOzYlRjj+TjwGclvQysBj4h6X8Ar6XDVaTn/al9N9CY178BeDXVGwrUjxMRyyKiNSJa6+vrSzkXM7MRr+JBEhGLI6IhIprJnUR/MiL+CFgLzE/N5gOPp+W1QLuksZJayJ1U35IOf/VKmpWu1pqX18fMzCpkdLUHkOduYI2km4E9wA0AEbFd0hrgReAosDAijqU+C4DlwHhgXXqYmVkFVTVIIuKnwE/T8gFg9gDtlgBLCtQ7gRnlG6GZmQ3F32w3M7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDKpeJBIapT0lKQdkrZLui3VJ0paL2lXep6Q12expC5JOyVdm1efKWlbWnevJFV6PmZmI1019kiOAl+OiIuAWcBCSdOBRcCGiJgGbEivSevagYuBNuA+SaPSe90PdADT0qOtkhMxM7MqBElE7IuIX6TlXmAHMBWYA6xIzVYAc9PyHGB1RByJiN1AF3CFpClAXURsjIgAVub1MTOzCqnqORJJzcDvApuBcyJiH+TCBjg7NZsK7M3r1p1qU9Ny/3qh7XRI6pTU2dPTU9I5mJmNdFULEklnAH8F/ElEHBqsaYFaDFI/vhixLCJaI6K1vr7+xAdrZmYDqkqQSBpDLkRWRcRjqfxaOlxFet6f6t1AY173BuDVVG8oUDczswqqxlVbAh4CdkTEd/JWrQXmp+X5wON59XZJYyW1kDupviUd/uqVNCu957y8PmZmViGjq7DNjwOfB7ZJ2ppqfwbcDayRdDOwB7gBICK2S1oDvEjuiq+FEXEs9VsALAfGA+vSw8zMKqjiQRIRP6Pw+Q2A2QP0WQIsKVDvBGaUbnRmZnai/M12s34am85FUsUfZrWqGoe2zE5q3Xv38J0ndlZ8u3d86oKKb9OsFLxHYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDJxkJiZWSYOEjMzy8Q/bFUr9AH/ip6ZnZRqPkgktQF/DowCHoyIu6s8pPKId/yrfWZ2UqrpQ1uSRgH/Dfg0MB34nKTp1R2VmdnIUtNBAlwBdEXESxHxW2A1MKfKYzIzG1EUEdUew7BJuh5oi4gvptefB66MiD/u164D6EgvLwCGe4xoMvDrYfatVZ7zyOA5jwxZ5nxuRNQXWlHr50gKnX0+LhkjYhmwLPPGpM6IaM36PrXEcx4ZPOeRoVxzrvVDW91AY97rBuDVKo3FzGxEqvUg+TtgmqQWSacB7cDaKo/JzGxEqelDWxFxVNIfA39L7vLfhyNiexk3mfnwWA3ynEcGz3lkKMuca/pku5mZVV+tH9oyM7Mqc5CYmVkmDpICJLVJ2impS9KiAusl6d60/nlJl1VjnKVUxJxvTHN9XtLPJV1ajXGW0lBzzmt3uaRj6XtLNa2YOUu6WtJWSdslPV3pMZZSEf9dnyXpx5J+meb7hWqMs5QkPSxpv6QXBlhf+s+viPAj70HupP0/AOcBpwG/BKb3a/MZYB2577HMAjZXe9wVmPPHgAlp+dMjYc557Z4E/ga4vtrjrsDf84eAF4Gm9Prsao+7zPP9M+BbabkeOAicVu2xZ5z37wOXAS8MsL7kn1/eIzleMbddmQOsjJxNwIckTan0QEtoyDlHxM8j4vX0chO57+zUsmJvr/Ml4K+A/ZUcXJkUM+d/DzwWEXsAIqKW513MfAM4U7lba59BLkiOVnaYpRURz5Cbx0BK/vnlIDneVGBv3uvuVDvRNrXkROdzM7l/0dSyIecsaSpwHfBABcdVTsX8PX8EmCDpp5KelTSvYqMrvWLm+xfAReS+yLwNuC0i3qnM8Kqm5J9fNf09kjIp5rYrRd2apYYUPR9J15ALkqvKOqLyK2bOS4GvRsSxU+S3YIqZ82hgJjAbGA9slLQpIv6+3IMrg2Lmey2wFfgEcD6wXtL/jYhDZR5bNZX888tBcrxibrtyqt2apaj5SPod4EHg0xFxoEJjK5di5twKrE4hMhn4jKSjEfGjioyw9Ir9b/vXEfEb4DeSngEuBWoxSIqZ7xeAuyN38qBL0m7gQmBLZYZYFSX//PKhreMVc9uVtcC8dPXDLOCNiNhX6YGW0JBzltQEPAZ8vkb/ddrfkHOOiJaIaI6IZuBR4NYaDhEo7r/tx4F/JWm0pA8CVwI7KjzOUilmvnvI7X0h6Rxydwd/qaKjrLySf355j6SfGOC2K5JuSesfIHcFz2eALuBNcv+qqVlFzvk/A5OA+9K/0I9GDd85tcg5n1KKmXNE7JD0E+B54B1yvzpa8DLSk12Rf8f/FVguaRu5Qz5fjYiavrW8pEeAq4HJkrqBu4AxUL7PL98ixczMMvGhLTMzy8RBYmZmmThIzMwsEweJmZll4iAxM7NMHCRmZpaJg8TMzDL5/z9xBp8lVCTgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(y, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF modeling (before Feature selection and Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_7111/350139188.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026474957871207606"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008874740341889975"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09420584027484695"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986345150043155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9339403800218343"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots for each target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame({\n",
    "    \"features\": X_train1.columns,\n",
    "    \"score\": rfreg.feature_importances_\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.001055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.000713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.017223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.002370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     features     score\n",
       "0     tfidf_0  0.001945\n",
       "1     tfidf_1  0.001055\n",
       "2     tfidf_2  0.000713\n",
       "3     tfidf_3  0.000591\n",
       "4     tfidf_4  0.000226\n",
       "..        ...       ...\n",
       "464      tree  0.000382\n",
       "465  tropical  0.000247\n",
       "466   vanilla  0.017223\n",
       "467    violet  0.000036\n",
       "468     woody  0.002370\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_ranked = df_feat.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>tfidf_253</td>\n",
       "      <td>0.089850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>0.046647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tfidf_11</td>\n",
       "      <td>0.033615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>sativa</td>\n",
       "      <td>0.032965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.027060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>tfidf_329</td>\n",
       "      <td>0.020516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>sweet</td>\n",
       "      <td>0.019476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.017223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>tfidf_285</td>\n",
       "      <td>0.016976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>indica</td>\n",
       "      <td>0.016633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>pine</td>\n",
       "      <td>0.016177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>tfidf_345</td>\n",
       "      <td>0.013277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>euphoric</td>\n",
       "      <td>0.012126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>tfidf_149</td>\n",
       "      <td>0.010086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tfidf_48</td>\n",
       "      <td>0.009853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>tfidf_168</td>\n",
       "      <td>0.009176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>blueberry</td>\n",
       "      <td>0.009021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>tfidf_145</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>tfidf_258</td>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>tfidf_210</td>\n",
       "      <td>0.008528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>tfidf_73</td>\n",
       "      <td>0.007878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>tfidf_312</td>\n",
       "      <td>0.007865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>tfidf_209</td>\n",
       "      <td>0.007859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>tfidf_141</td>\n",
       "      <td>0.007795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>tfidf_239</td>\n",
       "      <td>0.007604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>tfidf_119</td>\n",
       "      <td>0.007580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>diesel</td>\n",
       "      <td>0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>tfidf_124</td>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tfidf_38</td>\n",
       "      <td>0.006353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>creative</td>\n",
       "      <td>0.006279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>tfidf_167</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>tfidf_207</td>\n",
       "      <td>0.005775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>tfidf_287</td>\n",
       "      <td>0.005770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>tfidf_121</td>\n",
       "      <td>0.005744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>tfidf_245</td>\n",
       "      <td>0.005710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>tfidf_189</td>\n",
       "      <td>0.005619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>tfidf_144</td>\n",
       "      <td>0.005482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>tfidf_130</td>\n",
       "      <td>0.005423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>earthy</td>\n",
       "      <td>0.005336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_43</td>\n",
       "      <td>0.005293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>tfidf_281</td>\n",
       "      <td>0.005085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>tfidf_309</td>\n",
       "      <td>0.004910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>tfidf_162</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_7</td>\n",
       "      <td>0.004893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>tfidf_196</td>\n",
       "      <td>0.004816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>tfidf_90</td>\n",
       "      <td>0.004620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>tfidf_142</td>\n",
       "      <td>0.004410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>tfidf_93</td>\n",
       "      <td>0.004407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>tfidf_283</td>\n",
       "      <td>0.004261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>tfidf_199</td>\n",
       "      <td>0.004173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>tfidf_382</td>\n",
       "      <td>0.004099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>tfidf_343</td>\n",
       "      <td>0.004054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tfidf_46</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>tfidf_230</td>\n",
       "      <td>0.004043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>relaxed</td>\n",
       "      <td>0.003964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tfidf_105</td>\n",
       "      <td>0.003948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tfidf_98</td>\n",
       "      <td>0.003946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>tfidf_362</td>\n",
       "      <td>0.003940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tfidf_30</td>\n",
       "      <td>0.003938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>tfidf_289</td>\n",
       "      <td>0.003894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfidf_6</td>\n",
       "      <td>0.003667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>tfidf_353</td>\n",
       "      <td>0.003666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>tfidf_303</td>\n",
       "      <td>0.003652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>tfidf_357</td>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>tfidf_360</td>\n",
       "      <td>0.003308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>hungry</td>\n",
       "      <td>0.003306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>tfidf_314</td>\n",
       "      <td>0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>tfidf_354</td>\n",
       "      <td>0.003206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tfidf_175</td>\n",
       "      <td>0.003117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>tfidf_337</td>\n",
       "      <td>0.003043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tfidf_37</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>tfidf_81</td>\n",
       "      <td>0.002962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>tfidf_225</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>tfidf_374</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>tfidf_158</td>\n",
       "      <td>0.002918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>tfidf_320</td>\n",
       "      <td>0.002876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>rose</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>tfidf_372</td>\n",
       "      <td>0.002732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>talkative</td>\n",
       "      <td>0.002658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>tfidf_178</td>\n",
       "      <td>0.002626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>orange</td>\n",
       "      <td>0.002597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>dry eyes</td>\n",
       "      <td>0.002590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>tfidf_267</td>\n",
       "      <td>0.002573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>tfidf_64</td>\n",
       "      <td>0.002564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>tfidf_336</td>\n",
       "      <td>0.002526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>citrus</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>dry mouth</td>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.002370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>tfidf_203</td>\n",
       "      <td>0.002293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>tfidf_211</td>\n",
       "      <td>0.002287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>tfidf_131</td>\n",
       "      <td>0.002257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>tfidf_154</td>\n",
       "      <td>0.002237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>lemon</td>\n",
       "      <td>0.002236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>energetic</td>\n",
       "      <td>0.002235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>tfidf_78</td>\n",
       "      <td>0.002218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>tfidf_166</td>\n",
       "      <td>0.002203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>tfidf_264</td>\n",
       "      <td>0.002199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>tfidf_200</td>\n",
       "      <td>0.002199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>cheese</td>\n",
       "      <td>0.002199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>sleepy</td>\n",
       "      <td>0.002178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>tfidf_317</td>\n",
       "      <td>0.002173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>tfidf_342</td>\n",
       "      <td>0.002158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tfidf_39</td>\n",
       "      <td>0.002150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>tfidf_205</td>\n",
       "      <td>0.002098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.002095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>tfidf_104</td>\n",
       "      <td>0.002092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>tfidf_164</td>\n",
       "      <td>0.002063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>pungent</td>\n",
       "      <td>0.002056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>tfidf_75</td>\n",
       "      <td>0.002031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>tfidf_181</td>\n",
       "      <td>0.001878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tfidf_24</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tfidf_55</td>\n",
       "      <td>0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>tfidf_215</td>\n",
       "      <td>0.001766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>tfidf_321</td>\n",
       "      <td>0.001761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>tfidf_190</td>\n",
       "      <td>0.001722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>tfidf_319</td>\n",
       "      <td>0.001718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>tfidf_380</td>\n",
       "      <td>0.001717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>tfidf_156</td>\n",
       "      <td>0.001717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>tfidf_366</td>\n",
       "      <td>0.001699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>tfidf_355</td>\n",
       "      <td>0.001694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>tfidf_370</td>\n",
       "      <td>0.001682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>tfidf_122</td>\n",
       "      <td>0.001679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>tingly</td>\n",
       "      <td>0.001660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>tfidf_117</td>\n",
       "      <td>0.001653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>tfidf_340</td>\n",
       "      <td>0.001647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>tfidf_151</td>\n",
       "      <td>0.001624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tfidf_16</td>\n",
       "      <td>0.001610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>tfidf_110</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tfidf_67</td>\n",
       "      <td>0.001594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>tfidf_173</td>\n",
       "      <td>0.001594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>tfidf_263</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>tfidf_126</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>tfidf_129</td>\n",
       "      <td>0.001585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>tfidf_363</td>\n",
       "      <td>0.001567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>tfidf_280</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>giggly</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tfidf_58</td>\n",
       "      <td>0.001544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>tfidf_223</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>tfidf_96</td>\n",
       "      <td>0.001536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>uplifted</td>\n",
       "      <td>0.001533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>tfidf_177</td>\n",
       "      <td>0.001526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>tfidf_123</td>\n",
       "      <td>0.001516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tfidf_45</td>\n",
       "      <td>0.001509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>tfidf_217</td>\n",
       "      <td>0.001506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>tfidf_367</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>tfidf_259</td>\n",
       "      <td>0.001493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>tfidf_284</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tfidf_21</td>\n",
       "      <td>0.001448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>tfidf_82</td>\n",
       "      <td>0.001413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tfidf_25</td>\n",
       "      <td>0.001398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>tfidf_297</td>\n",
       "      <td>0.001384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>tfidf_273</td>\n",
       "      <td>0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>tfidf_338</td>\n",
       "      <td>0.001361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>tfidf_325</td>\n",
       "      <td>0.001358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tfidf_69</td>\n",
       "      <td>0.001357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>tfidf_261</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tfidf_26</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_5</td>\n",
       "      <td>0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tfidf_208</td>\n",
       "      <td>0.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>tfidf_304</td>\n",
       "      <td>0.001293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>tfidf_101</td>\n",
       "      <td>0.001289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>tfidf_231</td>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>tfidf_277</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>tfidf_315</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>tfidf_368</td>\n",
       "      <td>0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>focused</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>tfidf_251</td>\n",
       "      <td>0.001231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tfidf_19</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>tfidf_240</td>\n",
       "      <td>0.001228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>tfidf_324</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>tfidf_361</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tfidf_20</td>\n",
       "      <td>0.001218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tfidf_54</td>\n",
       "      <td>0.001218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tfidf_34</td>\n",
       "      <td>0.001210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>tfidf_111</td>\n",
       "      <td>0.001207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>skunk</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>tfidf_237</td>\n",
       "      <td>0.001176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>tfidf_376</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>tfidf_286</td>\n",
       "      <td>0.001172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>tfidf_270</td>\n",
       "      <td>0.001164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>chemical</td>\n",
       "      <td>0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>tfidf_318</td>\n",
       "      <td>0.001148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>tfidf_128</td>\n",
       "      <td>0.001129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>tfidf_265</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>tfidf_159</td>\n",
       "      <td>0.001103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tfidf_236</td>\n",
       "      <td>0.001094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>tfidf_256</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>tfidf_232</td>\n",
       "      <td>0.001089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>tfidf_107</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>tfidf_371</td>\n",
       "      <td>0.001076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>tfidf_138</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>tfidf_323</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>tfidf_103</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.001055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>tfidf_257</td>\n",
       "      <td>0.001047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>tfidf_184</td>\n",
       "      <td>0.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>tfidf_313</td>\n",
       "      <td>0.001041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>tfidf_193</td>\n",
       "      <td>0.001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>tfidf_97</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>tfidf_227</td>\n",
       "      <td>0.001027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>tfidf_234</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>tfidf_153</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>tfidf_221</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>dizzy</td>\n",
       "      <td>0.001016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>tfidf_127</td>\n",
       "      <td>0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>tfidf_248</td>\n",
       "      <td>0.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>tfidf_188</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>tfidf_222</td>\n",
       "      <td>0.000985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>mint</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>tfidf_295</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>tfidf_179</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tfidf_32</td>\n",
       "      <td>0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>aroused</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>tfidf_115</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>tfidf_339</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>tfidf_157</td>\n",
       "      <td>0.000893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>tfidf_244</td>\n",
       "      <td>0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>tfidf_252</td>\n",
       "      <td>0.000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tfidf_49</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>tfidf_61</td>\n",
       "      <td>0.000882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>tfidf_350</td>\n",
       "      <td>0.000882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>tfidf_373</td>\n",
       "      <td>0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>tfidf_118</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>tfidf_375</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>tfidf_341</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tfidf_28</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>tfidf_379</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tfidf_9</td>\n",
       "      <td>0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tfidf_66</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>tfidf_369</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>tfidf_218</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>tfidf_344</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>tfidf_92</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tfidf_31</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>tfidf_140</td>\n",
       "      <td>0.000740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>grape</td>\n",
       "      <td>0.000725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tfidf_22</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>lime</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>tfidf_198</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.000713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>tfidf_359</td>\n",
       "      <td>0.000707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>tfidf_148</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>tfidf_326</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>tfidf_291</td>\n",
       "      <td>0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>tfidf_216</td>\n",
       "      <td>0.000697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>tfidf_288</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>tfidf_74</td>\n",
       "      <td>0.000690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>tfidf_238</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>tfidf_108</td>\n",
       "      <td>0.000653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tfidf_299</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>tfidf_172</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>tfidf_102</td>\n",
       "      <td>0.000636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>tfidf_202</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>tfidf_308</td>\n",
       "      <td>0.000634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>tfidf_163</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>tfidf_170</td>\n",
       "      <td>0.000622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>tfidf_161</td>\n",
       "      <td>0.000621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>tfidf_185</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>tfidf_194</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tfidf_27</td>\n",
       "      <td>0.000593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>tfidf_269</td>\n",
       "      <td>0.000593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>tfidf_206</td>\n",
       "      <td>0.000588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>tfidf_381</td>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>tfidf_219</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>tfidf_80</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>tfidf_333</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tfidf_33</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>tfidf_278</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>tfidf_112</td>\n",
       "      <td>0.000545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>tfidf_307</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>flowery</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>tfidf_385</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>tfidf_88</td>\n",
       "      <td>0.000531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>tfidf_91</td>\n",
       "      <td>0.000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tfidf_13</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>tfidf_298</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tfidf_8</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>tfidf_305</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>tar</td>\n",
       "      <td>0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>tfidf_220</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tfidf_86</td>\n",
       "      <td>0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>tfidf_254</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>tfidf_322</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>fruit</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>tfidf_60</td>\n",
       "      <td>0.000496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tfidf_42</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>tfidf_331</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>tfidf_246</td>\n",
       "      <td>0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>tfidf_306</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>nutty</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tfidf_99</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tfidf_14</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>tfidf_135</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>tfidf_387</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>spicy/herbal</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>plum</td>\n",
       "      <td>0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>tfidf_83</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>tfidf_235</td>\n",
       "      <td>0.000434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>tfidf_120</td>\n",
       "      <td>0.000433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>tfidf_76</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>tfidf_136</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>tfidf_274</td>\n",
       "      <td>0.000406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>tfidf_213</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>tfidf_348</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>tfidf_197</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>honey</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>tfidf_300</td>\n",
       "      <td>0.000394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>tfidf_62</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tfidf_17</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tfidf_53</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>tfidf_276</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>tfidf_187</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>tfidf_356</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>tfidf_247</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tfidf_106</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tfidf_51</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>tfidf_386</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tfidf_15</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tfidf_12</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tfidf_79</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>tfidf_171</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>tfidf_116</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>tfidf_335</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>tfidf_59</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>paranoid</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tfidf_44</td>\n",
       "      <td>0.000345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>tfidf_212</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>tfidf_316</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tfidf_41</td>\n",
       "      <td>0.000325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>tfidf_63</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>tfidf_183</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>tfidf_146</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>tfidf_224</td>\n",
       "      <td>0.000315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>tfidf_282</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>tfidf_160</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>tfidf_85</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>anxious</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>mango</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tfidf_10</td>\n",
       "      <td>0.000307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>tfidf_182</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tfidf_50</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>tfidf_292</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tfidf_18</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>tfidf_378</td>\n",
       "      <td>0.000296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tfidf_56</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tfidf_94</td>\n",
       "      <td>0.000289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>tfidf_147</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>tfidf_346</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tfidf_52</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>tfidf_290</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>tfidf_214</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>tfidf_272</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>tfidf_271</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>tfidf_87</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>tfidf_113</td>\n",
       "      <td>0.000274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>tfidf_310</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>tfidf_327</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>tfidf_352</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>tfidf_195</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>tfidf_260</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>tfidf_169</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>tfidf_152</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tfidf_23</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>pear</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>tfidf_243</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>tfidf_358</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>tfidf_71</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tfidf_332</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>butter</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>tfidf_201</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>tfidf_349</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>tfidf_294</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>tfidf_133</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>tea</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>tfidf_125</td>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>tfidf_228</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>tfidf_95</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>tfidf_311</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tfidf_65</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>tfidf_293</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>tfidf_89</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>headache</td>\n",
       "      <td>0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>tfidf_279</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>lavender</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>tfidf_84</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>tfidf_72</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>tfidf_226</td>\n",
       "      <td>0.000171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>pepper</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>tfidf_109</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>tfidf_70</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>tfidf_233</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>tfidf_132</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>tfidf_241</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>tfidf_77</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>tfidf_229</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>tfidf_150</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>tfidf_143</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>tfidf_364</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>tfidf_100</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>tfidf_301</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tfidf_29</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>tfidf_377</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>tfidf_334</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>tfidf_176</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>0.000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>tfidf_275</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>chestnut</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>tfidf_384</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>tfidf_262</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>tfidf_351</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>tfidf_174</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>menthol</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>tfidf_242</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>tfidf_383</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>tfidf_266</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>tfidf_137</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>tfidf_186</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>tfidf_180</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tfidf_35</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>tfidf_255</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>tfidf_114</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tfidf_36</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>tfidf_204</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>tfidf_347</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tfidf_191</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>tfidf_139</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>tfidf_249</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>tfidf_134</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>tfidf_328</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>tfidf_155</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>tfidf_192</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>sage</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>tfidf_330</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tfidf_47</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>pineapple</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>tfidf_365</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>peach</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>tfidf_57</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>tfidf_165</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tfidf_302</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>ammonia</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>tfidf_268</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>apple</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tfidf_40</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tfidf_68</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tfidf_296</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>apricot</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>blue cheese</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>depression</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>tfidf_250</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>migraines</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>eye pressure</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>epilepsy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>arthritis</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>pain</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>seizures</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>spasticity</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>stress</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>fatigue</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         features     score\n",
       "253     tfidf_253  0.089850\n",
       "388        hybrid  0.046647\n",
       "11       tfidf_11  0.033615\n",
       "390        sativa  0.032965\n",
       "407         happy  0.027060\n",
       "329     tfidf_329  0.020516\n",
       "460         sweet  0.019476\n",
       "466       vanilla  0.017223\n",
       "285     tfidf_285  0.016976\n",
       "389        indica  0.016633\n",
       "451          pine  0.016177\n",
       "345     tfidf_345  0.013277\n",
       "402      euphoric  0.012126\n",
       "149     tfidf_149  0.010086\n",
       "48       tfidf_48  0.009853\n",
       "168     tfidf_168  0.009176\n",
       "426     blueberry  0.009021\n",
       "145     tfidf_145  0.008825\n",
       "258     tfidf_258  0.008582\n",
       "210     tfidf_210  0.008528\n",
       "73       tfidf_73  0.007878\n",
       "312     tfidf_312  0.007865\n",
       "209     tfidf_209  0.007859\n",
       "141     tfidf_141  0.007795\n",
       "239     tfidf_239  0.007604\n",
       "119     tfidf_119  0.007580\n",
       "433        diesel  0.007394\n",
       "124     tfidf_124  0.007225\n",
       "38       tfidf_38  0.006353\n",
       "395      creative  0.006279\n",
       "167     tfidf_167  0.006200\n",
       "207     tfidf_207  0.005775\n",
       "287     tfidf_287  0.005770\n",
       "121     tfidf_121  0.005744\n",
       "245     tfidf_245  0.005710\n",
       "189     tfidf_189  0.005619\n",
       "144     tfidf_144  0.005482\n",
       "130     tfidf_130  0.005423\n",
       "434        earthy  0.005336\n",
       "43       tfidf_43  0.005293\n",
       "281     tfidf_281  0.005085\n",
       "309     tfidf_309  0.004910\n",
       "162     tfidf_162  0.004900\n",
       "7         tfidf_7  0.004893\n",
       "196     tfidf_196  0.004816\n",
       "90       tfidf_90  0.004620\n",
       "142     tfidf_142  0.004410\n",
       "93       tfidf_93  0.004407\n",
       "283     tfidf_283  0.004261\n",
       "199     tfidf_199  0.004173\n",
       "382     tfidf_382  0.004099\n",
       "343     tfidf_343  0.004054\n",
       "46       tfidf_46  0.004047\n",
       "230     tfidf_230  0.004043\n",
       "413       relaxed  0.003964\n",
       "105     tfidf_105  0.003948\n",
       "98       tfidf_98  0.003946\n",
       "362     tfidf_362  0.003940\n",
       "30       tfidf_30  0.003938\n",
       "289     tfidf_289  0.003894\n",
       "6         tfidf_6  0.003667\n",
       "353     tfidf_353  0.003666\n",
       "303     tfidf_303  0.003652\n",
       "357     tfidf_357  0.003582\n",
       "360     tfidf_360  0.003308\n",
       "409        hungry  0.003306\n",
       "314     tfidf_314  0.003295\n",
       "354     tfidf_354  0.003206\n",
       "175     tfidf_175  0.003117\n",
       "337     tfidf_337  0.003043\n",
       "37       tfidf_37  0.002968\n",
       "81       tfidf_81  0.002962\n",
       "225     tfidf_225  0.002959\n",
       "374     tfidf_374  0.002929\n",
       "158     tfidf_158  0.002918\n",
       "320     tfidf_320  0.002876\n",
       "455          rose  0.002810\n",
       "372     tfidf_372  0.002732\n",
       "418     talkative  0.002658\n",
       "178     tfidf_178  0.002626\n",
       "447        orange  0.002597\n",
       "398      dry eyes  0.002590\n",
       "267     tfidf_267  0.002573\n",
       "64       tfidf_64  0.002564\n",
       "336     tfidf_336  0.002526\n",
       "431        citrus  0.002413\n",
       "399     dry mouth  0.002392\n",
       "468         woody  0.002370\n",
       "203     tfidf_203  0.002293\n",
       "211     tfidf_211  0.002287\n",
       "131     tfidf_131  0.002257\n",
       "154     tfidf_154  0.002237\n",
       "441         lemon  0.002236\n",
       "400     energetic  0.002235\n",
       "78       tfidf_78  0.002218\n",
       "166     tfidf_166  0.002203\n",
       "264     tfidf_264  0.002199\n",
       "200     tfidf_200  0.002199\n",
       "428        cheese  0.002199\n",
       "415        sleepy  0.002178\n",
       "317     tfidf_317  0.002173\n",
       "342     tfidf_342  0.002158\n",
       "39       tfidf_39  0.002150\n",
       "205     tfidf_205  0.002098\n",
       "424         berry  0.002095\n",
       "104     tfidf_104  0.002092\n",
       "164     tfidf_164  0.002063\n",
       "454       pungent  0.002056\n",
       "75       tfidf_75  0.002031\n",
       "0         tfidf_0  0.001945\n",
       "181     tfidf_181  0.001878\n",
       "24       tfidf_24  0.001791\n",
       "55       tfidf_55  0.001788\n",
       "215     tfidf_215  0.001766\n",
       "321     tfidf_321  0.001761\n",
       "190     tfidf_190  0.001722\n",
       "319     tfidf_319  0.001718\n",
       "380     tfidf_380  0.001717\n",
       "156     tfidf_156  0.001717\n",
       "366     tfidf_366  0.001699\n",
       "355     tfidf_355  0.001694\n",
       "370     tfidf_370  0.001682\n",
       "122     tfidf_122  0.001679\n",
       "419        tingly  0.001660\n",
       "117     tfidf_117  0.001653\n",
       "340     tfidf_340  0.001647\n",
       "151     tfidf_151  0.001624\n",
       "16       tfidf_16  0.001610\n",
       "110     tfidf_110  0.001609\n",
       "67       tfidf_67  0.001594\n",
       "173     tfidf_173  0.001594\n",
       "263     tfidf_263  0.001588\n",
       "126     tfidf_126  0.001588\n",
       "129     tfidf_129  0.001585\n",
       "363     tfidf_363  0.001567\n",
       "280     tfidf_280  0.001550\n",
       "406        giggly  0.001548\n",
       "58       tfidf_58  0.001544\n",
       "223     tfidf_223  0.001543\n",
       "96       tfidf_96  0.001536\n",
       "420      uplifted  0.001533\n",
       "177     tfidf_177  0.001526\n",
       "123     tfidf_123  0.001516\n",
       "45       tfidf_45  0.001509\n",
       "217     tfidf_217  0.001506\n",
       "367     tfidf_367  0.001495\n",
       "259     tfidf_259  0.001493\n",
       "284     tfidf_284  0.001488\n",
       "21       tfidf_21  0.001448\n",
       "82       tfidf_82  0.001413\n",
       "25       tfidf_25  0.001398\n",
       "297     tfidf_297  0.001384\n",
       "273     tfidf_273  0.001377\n",
       "338     tfidf_338  0.001361\n",
       "325     tfidf_325  0.001358\n",
       "69       tfidf_69  0.001357\n",
       "261     tfidf_261  0.001343\n",
       "26       tfidf_26  0.001339\n",
       "5         tfidf_5  0.001337\n",
       "208     tfidf_208  0.001317\n",
       "304     tfidf_304  0.001293\n",
       "101     tfidf_101  0.001289\n",
       "231     tfidf_231  0.001258\n",
       "277     tfidf_277  0.001247\n",
       "315     tfidf_315  0.001240\n",
       "368     tfidf_368  0.001239\n",
       "405       focused  0.001237\n",
       "251     tfidf_251  0.001231\n",
       "19       tfidf_19  0.001229\n",
       "240     tfidf_240  0.001228\n",
       "324     tfidf_324  0.001224\n",
       "361     tfidf_361  0.001220\n",
       "20       tfidf_20  0.001218\n",
       "54       tfidf_54  0.001218\n",
       "34       tfidf_34  0.001210\n",
       "111     tfidf_111  0.001207\n",
       "457         skunk  0.001202\n",
       "237     tfidf_237  0.001176\n",
       "376     tfidf_376  0.001174\n",
       "286     tfidf_286  0.001172\n",
       "270     tfidf_270  0.001164\n",
       "429      chemical  0.001161\n",
       "318     tfidf_318  0.001148\n",
       "128     tfidf_128  0.001129\n",
       "265     tfidf_265  0.001111\n",
       "159     tfidf_159  0.001103\n",
       "236     tfidf_236  0.001094\n",
       "256     tfidf_256  0.001091\n",
       "232     tfidf_232  0.001089\n",
       "107     tfidf_107  0.001079\n",
       "371     tfidf_371  0.001076\n",
       "138     tfidf_138  0.001064\n",
       "323     tfidf_323  0.001062\n",
       "103     tfidf_103  0.001057\n",
       "1         tfidf_1  0.001055\n",
       "257     tfidf_257  0.001047\n",
       "184     tfidf_184  0.001042\n",
       "313     tfidf_313  0.001041\n",
       "193     tfidf_193  0.001029\n",
       "97       tfidf_97  0.001028\n",
       "227     tfidf_227  0.001027\n",
       "234     tfidf_234  0.001026\n",
       "153     tfidf_153  0.001026\n",
       "221     tfidf_221  0.001018\n",
       "397         dizzy  0.001016\n",
       "127     tfidf_127  0.001007\n",
       "248     tfidf_248  0.001004\n",
       "188     tfidf_188  0.000990\n",
       "222     tfidf_222  0.000985\n",
       "445          mint  0.000967\n",
       "295     tfidf_295  0.000963\n",
       "179     tfidf_179  0.000945\n",
       "32       tfidf_32  0.000932\n",
       "393       aroused  0.000930\n",
       "115     tfidf_115  0.000913\n",
       "339     tfidf_339  0.000894\n",
       "157     tfidf_157  0.000893\n",
       "244     tfidf_244  0.000890\n",
       "252     tfidf_252  0.000888\n",
       "49       tfidf_49  0.000885\n",
       "61       tfidf_61  0.000882\n",
       "350     tfidf_350  0.000882\n",
       "373     tfidf_373  0.000873\n",
       "118     tfidf_118  0.000846\n",
       "375     tfidf_375  0.000826\n",
       "341     tfidf_341  0.000795\n",
       "28       tfidf_28  0.000786\n",
       "379     tfidf_379  0.000785\n",
       "9         tfidf_9  0.000782\n",
       "66       tfidf_66  0.000779\n",
       "369     tfidf_369  0.000772\n",
       "218     tfidf_218  0.000766\n",
       "344     tfidf_344  0.000761\n",
       "92       tfidf_92  0.000749\n",
       "31       tfidf_31  0.000741\n",
       "140     tfidf_140  0.000740\n",
       "437         grape  0.000725\n",
       "22       tfidf_22  0.000724\n",
       "442          lime  0.000717\n",
       "198     tfidf_198  0.000715\n",
       "2         tfidf_2  0.000713\n",
       "359     tfidf_359  0.000707\n",
       "148     tfidf_148  0.000705\n",
       "326     tfidf_326  0.000705\n",
       "291     tfidf_291  0.000701\n",
       "216     tfidf_216  0.000697\n",
       "288     tfidf_288  0.000692\n",
       "74       tfidf_74  0.000690\n",
       "238     tfidf_238  0.000673\n",
       "108     tfidf_108  0.000653\n",
       "299     tfidf_299  0.000645\n",
       "172     tfidf_172  0.000645\n",
       "102     tfidf_102  0.000636\n",
       "202     tfidf_202  0.000635\n",
       "308     tfidf_308  0.000634\n",
       "163     tfidf_163  0.000633\n",
       "170     tfidf_170  0.000622\n",
       "161     tfidf_161  0.000621\n",
       "185     tfidf_185  0.000616\n",
       "194     tfidf_194  0.000606\n",
       "27       tfidf_27  0.000593\n",
       "269     tfidf_269  0.000593\n",
       "3         tfidf_3  0.000591\n",
       "206     tfidf_206  0.000588\n",
       "381     tfidf_381  0.000575\n",
       "219     tfidf_219  0.000572\n",
       "80       tfidf_80  0.000556\n",
       "333     tfidf_333  0.000551\n",
       "33       tfidf_33  0.000551\n",
       "278     tfidf_278  0.000546\n",
       "112     tfidf_112  0.000545\n",
       "307     tfidf_307  0.000541\n",
       "435       flowery  0.000541\n",
       "385     tfidf_385  0.000535\n",
       "88       tfidf_88  0.000531\n",
       "91       tfidf_91  0.000529\n",
       "13       tfidf_13  0.000528\n",
       "298     tfidf_298  0.000516\n",
       "8         tfidf_8  0.000515\n",
       "305     tfidf_305  0.000515\n",
       "461           tar  0.000509\n",
       "220     tfidf_220  0.000507\n",
       "86       tfidf_86  0.000503\n",
       "254     tfidf_254  0.000500\n",
       "322     tfidf_322  0.000498\n",
       "436         fruit  0.000497\n",
       "60       tfidf_60  0.000496\n",
       "42       tfidf_42  0.000493\n",
       "331     tfidf_331  0.000490\n",
       "246     tfidf_246  0.000482\n",
       "306     tfidf_306  0.000480\n",
       "446         nutty  0.000473\n",
       "99       tfidf_99  0.000469\n",
       "14       tfidf_14  0.000461\n",
       "135     tfidf_135  0.000460\n",
       "387     tfidf_387  0.000458\n",
       "458  spicy/herbal  0.000452\n",
       "453          plum  0.000448\n",
       "83       tfidf_83  0.000438\n",
       "235     tfidf_235  0.000434\n",
       "120     tfidf_120  0.000433\n",
       "76       tfidf_76  0.000428\n",
       "136     tfidf_136  0.000427\n",
       "274     tfidf_274  0.000406\n",
       "213     tfidf_213  0.000405\n",
       "348     tfidf_348  0.000403\n",
       "197     tfidf_197  0.000401\n",
       "439         honey  0.000399\n",
       "300     tfidf_300  0.000394\n",
       "62       tfidf_62  0.000393\n",
       "17       tfidf_17  0.000385\n",
       "53       tfidf_53  0.000384\n",
       "464          tree  0.000382\n",
       "276     tfidf_276  0.000382\n",
       "187     tfidf_187  0.000379\n",
       "356     tfidf_356  0.000377\n",
       "247     tfidf_247  0.000374\n",
       "106     tfidf_106  0.000374\n",
       "51       tfidf_51  0.000371\n",
       "386     tfidf_386  0.000371\n",
       "15       tfidf_15  0.000366\n",
       "12       tfidf_12  0.000358\n",
       "79       tfidf_79  0.000357\n",
       "171     tfidf_171  0.000356\n",
       "116     tfidf_116  0.000351\n",
       "335     tfidf_335  0.000351\n",
       "59       tfidf_59  0.000348\n",
       "412      paranoid  0.000348\n",
       "44       tfidf_44  0.000345\n",
       "212     tfidf_212  0.000343\n",
       "316     tfidf_316  0.000335\n",
       "41       tfidf_41  0.000325\n",
       "63       tfidf_63  0.000321\n",
       "183     tfidf_183  0.000320\n",
       "146     tfidf_146  0.000320\n",
       "224     tfidf_224  0.000315\n",
       "282     tfidf_282  0.000312\n",
       "160     tfidf_160  0.000310\n",
       "85       tfidf_85  0.000309\n",
       "392       anxious  0.000309\n",
       "443         mango  0.000309\n",
       "10       tfidf_10  0.000307\n",
       "182     tfidf_182  0.000302\n",
       "50       tfidf_50  0.000302\n",
       "292     tfidf_292  0.000297\n",
       "18       tfidf_18  0.000297\n",
       "378     tfidf_378  0.000296\n",
       "56       tfidf_56  0.000292\n",
       "94       tfidf_94  0.000289\n",
       "147     tfidf_147  0.000288\n",
       "346     tfidf_346  0.000286\n",
       "52       tfidf_52  0.000286\n",
       "290     tfidf_290  0.000282\n",
       "214     tfidf_214  0.000280\n",
       "272     tfidf_272  0.000277\n",
       "271     tfidf_271  0.000276\n",
       "87       tfidf_87  0.000276\n",
       "113     tfidf_113  0.000274\n",
       "310     tfidf_310  0.000273\n",
       "327     tfidf_327  0.000272\n",
       "352     tfidf_352  0.000263\n",
       "438    grapefruit  0.000261\n",
       "195     tfidf_195  0.000260\n",
       "260     tfidf_260  0.000253\n",
       "169     tfidf_169  0.000248\n",
       "465      tropical  0.000247\n",
       "152     tfidf_152  0.000245\n",
       "23       tfidf_23  0.000244\n",
       "449          pear  0.000240\n",
       "243     tfidf_243  0.000237\n",
       "358     tfidf_358  0.000235\n",
       "71       tfidf_71  0.000233\n",
       "332     tfidf_332  0.000232\n",
       "427        butter  0.000232\n",
       "201     tfidf_201  0.000229\n",
       "4         tfidf_4  0.000226\n",
       "349     tfidf_349  0.000226\n",
       "294     tfidf_294  0.000223\n",
       "133     tfidf_133  0.000221\n",
       "462           tea  0.000219\n",
       "125     tfidf_125  0.000218\n",
       "228     tfidf_228  0.000215\n",
       "95       tfidf_95  0.000214\n",
       "311     tfidf_311  0.000206\n",
       "65       tfidf_65  0.000197\n",
       "293     tfidf_293  0.000193\n",
       "89       tfidf_89  0.000193\n",
       "463       tobacco  0.000191\n",
       "408      headache  0.000190\n",
       "279     tfidf_279  0.000189\n",
       "440      lavender  0.000187\n",
       "84       tfidf_84  0.000175\n",
       "72       tfidf_72  0.000173\n",
       "226     tfidf_226  0.000171\n",
       "450        pepper  0.000167\n",
       "109     tfidf_109  0.000164\n",
       "70       tfidf_70  0.000159\n",
       "233     tfidf_233  0.000156\n",
       "132     tfidf_132  0.000156\n",
       "241     tfidf_241  0.000155\n",
       "77       tfidf_77  0.000149\n",
       "229     tfidf_229  0.000143\n",
       "150     tfidf_150  0.000143\n",
       "143     tfidf_143  0.000141\n",
       "364     tfidf_364  0.000141\n",
       "100     tfidf_100  0.000139\n",
       "301     tfidf_301  0.000137\n",
       "29       tfidf_29  0.000136\n",
       "377     tfidf_377  0.000135\n",
       "334     tfidf_334  0.000133\n",
       "176     tfidf_176  0.000133\n",
       "459    strawberry  0.000128\n",
       "275     tfidf_275  0.000127\n",
       "430      chestnut  0.000125\n",
       "384     tfidf_384  0.000122\n",
       "262     tfidf_262  0.000122\n",
       "351     tfidf_351  0.000121\n",
       "174     tfidf_174  0.000120\n",
       "444       menthol  0.000114\n",
       "242     tfidf_242  0.000110\n",
       "383     tfidf_383  0.000109\n",
       "266     tfidf_266  0.000109\n",
       "137     tfidf_137  0.000104\n",
       "186     tfidf_186  0.000103\n",
       "180     tfidf_180  0.000103\n",
       "35       tfidf_35  0.000102\n",
       "255     tfidf_255  0.000101\n",
       "114     tfidf_114  0.000098\n",
       "36       tfidf_36  0.000096\n",
       "204     tfidf_204  0.000094\n",
       "432        coffee  0.000094\n",
       "347     tfidf_347  0.000093\n",
       "191     tfidf_191  0.000093\n",
       "139     tfidf_139  0.000090\n",
       "249     tfidf_249  0.000089\n",
       "134     tfidf_134  0.000084\n",
       "328     tfidf_328  0.000084\n",
       "155     tfidf_155  0.000078\n",
       "192     tfidf_192  0.000075\n",
       "456          sage  0.000070\n",
       "330     tfidf_330  0.000068\n",
       "47       tfidf_47  0.000066\n",
       "452     pineapple  0.000058\n",
       "365     tfidf_365  0.000057\n",
       "448         peach  0.000053\n",
       "57       tfidf_57  0.000048\n",
       "165     tfidf_165  0.000048\n",
       "302     tfidf_302  0.000043\n",
       "421       ammonia  0.000038\n",
       "268     tfidf_268  0.000038\n",
       "467        violet  0.000036\n",
       "422         apple  0.000034\n",
       "40       tfidf_40  0.000033\n",
       "68       tfidf_68  0.000027\n",
       "296     tfidf_296  0.000022\n",
       "423       apricot  0.000022\n",
       "425   blue cheese  0.000018\n",
       "391       anxiety  0.000006\n",
       "396    depression  0.000005\n",
       "250     tfidf_250  0.000002\n",
       "410     migraines  0.000001\n",
       "403  eye pressure  0.000000\n",
       "401      epilepsy  0.000000\n",
       "394     arthritis  0.000000\n",
       "411          pain  0.000000\n",
       "414      seizures  0.000000\n",
       "416    spasticity  0.000000\n",
       "417        stress  0.000000\n",
       "404       fatigue  0.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', df_feat_ranked.shape[0]+1)\n",
    "df_feat_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py:357: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "selector = SelectFromModel(rfreg).fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35646314e-03, 1.00479382e-03, 6.20233991e-04, 6.45466645e-04,\n",
       "       3.37966802e-04, 1.47533857e-03, 3.40894295e-03, 5.08472959e-03,\n",
       "       4.56895765e-04, 7.46890769e-04, 4.33974071e-04, 3.37134455e-02,\n",
       "       6.17922103e-04, 4.96530521e-04, 3.85593681e-04, 3.21122945e-04,\n",
       "       1.45438121e-03, 2.91309143e-04, 3.03332201e-04, 1.47097223e-03,\n",
       "       1.06779811e-03, 1.15496608e-03, 4.97989101e-04, 2.35645392e-04,\n",
       "       1.81962500e-03, 1.32309269e-03, 1.33641039e-03, 5.76298559e-04,\n",
       "       8.87416273e-04, 1.06560827e-04, 3.69085287e-03, 6.59459357e-04,\n",
       "       1.34644745e-03, 6.00035418e-04, 1.22773619e-03, 9.82795943e-05,\n",
       "       1.08652462e-04, 3.09957883e-03, 6.51442271e-03, 2.55972050e-03,\n",
       "       7.63311759e-05, 2.30641914e-04, 6.23676132e-04, 5.12240204e-03,\n",
       "       3.56905710e-04, 1.47306084e-03, 3.99649099e-03, 5.02452002e-05,\n",
       "       9.26279598e-03, 7.79431540e-04, 2.89897632e-04, 3.82099020e-04,\n",
       "       2.50819985e-04, 3.45502848e-04, 1.11094711e-03, 2.04571713e-03,\n",
       "       2.33500394e-04, 3.51002913e-05, 1.38353975e-03, 3.80982528e-04,\n",
       "       2.71242599e-04, 7.50786306e-04, 3.27805578e-04, 4.62800744e-04,\n",
       "       2.49281529e-03, 2.70470790e-04, 6.72712613e-04, 1.83147368e-03,\n",
       "       6.72913636e-05, 1.25537651e-03, 1.61897943e-04, 1.70756408e-04,\n",
       "       2.03355262e-04, 9.06746195e-03, 8.58871016e-04, 1.82717310e-03,\n",
       "       4.74584832e-04, 1.57765751e-04, 2.20940582e-03, 3.34906308e-04,\n",
       "       4.22190225e-04, 2.53331088e-03, 1.52267708e-03, 6.21084190e-04,\n",
       "       1.90739990e-04, 1.92103218e-04, 4.00823340e-04, 3.34868356e-04,\n",
       "       5.77086972e-04, 1.67626723e-04, 4.53238655e-03, 5.69076031e-04,\n",
       "       5.99455783e-04, 4.17799342e-03, 1.53414043e-04, 2.15186165e-04,\n",
       "       1.45936128e-03, 1.25874044e-03, 4.05245922e-03, 3.62922594e-04,\n",
       "       1.46934719e-04, 1.33722940e-03, 5.62458458e-04, 1.22639034e-03,\n",
       "       2.52698411e-03, 3.62353826e-03, 3.83948283e-04, 1.04822570e-03,\n",
       "       5.44398712e-04, 1.82368550e-04, 1.86321615e-03, 1.10790443e-03,\n",
       "       5.93324847e-04, 3.21302704e-04, 9.81486240e-05, 1.06458772e-03,\n",
       "       3.25857306e-04, 1.41026096e-03, 6.23298978e-04, 7.03034264e-03,\n",
       "       5.48106227e-04, 5.85994039e-03, 1.61973156e-03, 1.62850987e-03,\n",
       "       7.18239539e-03, 2.64736035e-04, 1.83224237e-03, 1.03973648e-03,\n",
       "       1.17262915e-03, 1.46185491e-03, 5.48251685e-03, 2.36738084e-03,\n",
       "       1.96187758e-04, 1.08813533e-04, 9.78461261e-05, 4.53904604e-04,\n",
       "       4.80062650e-04, 1.19207608e-04, 1.12200809e-03, 1.18005488e-04,\n",
       "       7.66376251e-04, 8.06333647e-03, 4.67018264e-03, 2.29253983e-04,\n",
       "       4.80680496e-03, 8.45507189e-03, 3.45522407e-04, 2.07607774e-04,\n",
       "       2.02399291e-04, 1.00843317e-02, 3.14368811e-04, 1.44393683e-03,\n",
       "       3.47096476e-04, 1.00841537e-03, 2.73106645e-03, 6.37923061e-05,\n",
       "       1.78793724e-03, 6.79190024e-04, 2.93185223e-03, 1.31637391e-03,\n",
       "       3.15220768e-04, 6.52050749e-04, 4.81406950e-03, 7.84745465e-04,\n",
       "       2.23234291e-03, 3.18627208e-05, 1.92854824e-03, 5.88933981e-03,\n",
       "       8.79381966e-03, 1.92701511e-04, 3.98269251e-04, 4.39187077e-04,\n",
       "       5.24814393e-04, 1.73074116e-03, 1.34107156e-04, 3.23860190e-03,\n",
       "       1.16667568e-04, 1.62668507e-03, 2.27981252e-03, 8.77884519e-04,\n",
       "       1.18395904e-04, 2.14214882e-03, 3.64100871e-04, 2.67100913e-04,\n",
       "       7.92184288e-04, 6.51528298e-04, 1.07651851e-04, 3.32552195e-04,\n",
       "       1.23760503e-03, 5.71292525e-03, 2.15995885e-03, 8.53041368e-05,\n",
       "       7.84676254e-05, 1.31675802e-03, 6.10604270e-04, 3.38380895e-04,\n",
       "       5.57921203e-03, 4.47451538e-04, 7.41655459e-04, 3.58804760e-03,\n",
       "       2.03076668e-03, 2.58022770e-04, 6.69669478e-04, 2.20047874e-03,\n",
       "       9.25822118e-05, 2.13710909e-03, 7.57903106e-04, 5.56835483e-03,\n",
       "       1.34735957e-03, 8.05963417e-03, 8.71941996e-03, 2.25214088e-03,\n",
       "       3.47459180e-04, 3.07990397e-04, 3.65086366e-04, 2.10700223e-03,\n",
       "       8.95678322e-04, 1.55791013e-03, 5.91982395e-04, 6.09323232e-04,\n",
       "       4.68648972e-04, 1.10730257e-03, 8.13451603e-04, 1.79066559e-03,\n",
       "       3.76817903e-04, 3.04758384e-03, 1.97694622e-04, 1.39664576e-03,\n",
       "       1.19111366e-04, 1.35708474e-04, 4.20727141e-03, 1.20943071e-03,\n",
       "       7.37372525e-04, 1.13263978e-04, 8.52040942e-04, 4.93023423e-04,\n",
       "       9.78702098e-04, 1.07785207e-03, 4.67193473e-04, 7.55376551e-03,\n",
       "       1.41299492e-03, 1.51608495e-04, 1.94272828e-04, 3.68780935e-04,\n",
       "       8.95215943e-04, 5.27355200e-03, 6.65000756e-04, 3.78695482e-04,\n",
       "       1.01804585e-03, 6.93747423e-05, 4.07007748e-06, 1.21454442e-03,\n",
       "       7.21442891e-04, 9.01013501e-02, 5.31697970e-04, 6.25307017e-05,\n",
       "       8.35561178e-04, 1.02406985e-03, 8.63336913e-03, 2.25005437e-03,\n",
       "       1.66148967e-04, 1.14622179e-03, 1.31636917e-04, 1.51710961e-03,\n",
       "       2.04619010e-03, 1.10242412e-03, 2.40119222e-04, 2.21167081e-03,\n",
       "       5.06743243e-05, 6.74802049e-04, 1.38524527e-03, 3.10914655e-04,\n",
       "       4.08397935e-04, 1.25552474e-03, 4.44588380e-04, 1.23745391e-04,\n",
       "       3.62059527e-04, 1.30315998e-03, 6.81591088e-04, 1.35652581e-04,\n",
       "       1.64062019e-03, 5.02213681e-03, 3.86081426e-04, 3.87477482e-03,\n",
       "       1.23401686e-03, 1.77611086e-02, 1.30238926e-03, 5.94385618e-03,\n",
       "       8.61084325e-04, 3.08247804e-03, 3.13321483e-04, 9.36008175e-04,\n",
       "       2.95097714e-04, 3.92114580e-04, 3.01126446e-04, 9.85144433e-04,\n",
       "       1.08499084e-04, 1.49653449e-03, 6.15551150e-04, 9.48222254e-04,\n",
       "       4.22623284e-04, 1.25435614e-04, 1.29311675e-05, 3.55340201e-03,\n",
       "       1.15418732e-03, 6.27002782e-04, 4.43330001e-04, 4.51890355e-04,\n",
       "       6.66369150e-04, 4.89772946e-03, 2.32618791e-04, 1.91889575e-04,\n",
       "       7.49656307e-03, 1.12986729e-03, 3.68724968e-03, 1.26792909e-03,\n",
       "       3.54657387e-04, 2.43473028e-03, 1.05981945e-03, 1.48169092e-03,\n",
       "       2.83026381e-03, 2.21114318e-03, 5.02208032e-04, 8.24869111e-04,\n",
       "       1.32515280e-03, 1.55164134e-03, 7.03683565e-04, 2.74919303e-04,\n",
       "       7.69837722e-05, 1.95622482e-02, 7.10680119e-05, 4.94006387e-04,\n",
       "       2.67037462e-04, 5.91107955e-04, 1.86600139e-04, 4.54105615e-04,\n",
       "       2.48871029e-03, 3.20502984e-03, 1.41730906e-03, 8.36941181e-04,\n",
       "       1.47210780e-03, 9.35723688e-04, 1.95289325e-03, 3.50361130e-03,\n",
       "       8.45268685e-04, 1.22699405e-02, 2.65467278e-04, 8.67792923e-05,\n",
       "       4.70659213e-04, 3.36476013e-04, 8.89852259e-04, 1.99419876e-04,\n",
       "       2.59182283e-04, 3.23590749e-03, 3.23447524e-03, 2.01421801e-03,\n",
       "       4.00986224e-04, 3.69658782e-03, 2.26002601e-04, 7.49425899e-04,\n",
       "       3.23528544e-03, 1.33415678e-03, 4.63091067e-03, 1.66682108e-03,\n",
       "       1.11144030e-04, 6.61288597e-05, 1.85874715e-03, 1.40616651e-03,\n",
       "       1.38875035e-03, 8.95232193e-04, 1.89974276e-03, 1.24302710e-03,\n",
       "       3.06878358e-03, 7.05486040e-04, 2.82858861e-03, 7.29746591e-04,\n",
       "       1.22919025e-03, 1.20682908e-04, 2.41449712e-04, 7.56792155e-04,\n",
       "       1.62785920e-03, 4.95922385e-04, 4.39750570e-03, 1.14642657e-04,\n",
       "       1.53686792e-04, 7.37227452e-04, 3.12362449e-04, 3.91643313e-04,\n",
       "       4.64829940e-02, 1.73122431e-02, 3.31641467e-02, 4.13904686e-06,\n",
       "       2.60990620e-04, 8.57406808e-04, 0.00000000e+00, 6.86491079e-03,\n",
       "       1.00466183e-08, 8.19335861e-04, 2.60908815e-03, 2.38826378e-03,\n",
       "       2.38273253e-03, 0.00000000e+00, 1.27827286e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.18834488e-03, 1.70259614e-03, 2.52104417e-02,\n",
       "       3.38915162e-04, 3.37442713e-03, 5.71503808e-07, 0.00000000e+00,\n",
       "       2.50102003e-04, 3.62208654e-03, 0.00000000e+00, 1.94340568e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.31750842e-03, 1.51380903e-03,\n",
       "       1.54238316e-03, 2.88605415e-05, 1.26690189e-05, 1.32660750e-05,\n",
       "       2.02891876e-03, 1.10462499e-05, 9.21684911e-03, 2.25558849e-04,\n",
       "       2.12753509e-03, 9.85972204e-04, 2.74026431e-04, 2.47048624e-03,\n",
       "       8.70769747e-05, 7.21016058e-03, 5.12595080e-03, 4.26186969e-04,\n",
       "       5.49560618e-04, 7.11601186e-04, 2.24929338e-04, 3.78722399e-04,\n",
       "       3.42058447e-04, 2.70697020e-03, 5.63269187e-04, 2.90250921e-04,\n",
       "       9.01397401e-05, 7.80491471e-04, 5.46730119e-04, 2.31099756e-03,\n",
       "       9.35080591e-05, 1.66933670e-04, 1.61611052e-04, 1.62059767e-02,\n",
       "       1.67754503e-04, 4.13411129e-04, 2.42331599e-03, 2.75659038e-03,\n",
       "       7.25901742e-05, 1.29972993e-03, 5.14587894e-04, 1.80114314e-04,\n",
       "       1.96232912e-02, 4.73168869e-04, 1.90398274e-04, 2.46451714e-04,\n",
       "       3.89166466e-04, 3.64492629e-04, 1.64326350e-02, 2.03103999e-05,\n",
       "       2.36198346e-03])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021321961620469083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False,  True, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "        True, False, False,  True, False, False, False, False,  True,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False, False,  True, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False, False, False, False, False,  True,  True, False,\n",
       "        True,  True, False, False, False,  True, False, False, False,\n",
       "       False,  True, False, False, False,  True, False, False, False,\n",
       "        True, False,  True, False, False,  True,  True, False, False,\n",
       "       False, False, False, False,  True, False, False,  True, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False,  True, False,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False,  True, False, False,  True, False,  True,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False,  True,  True, False, False,  True, False, False,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False, False,  True,\n",
       "       False, False,  True,  True,  True, False,  True, False, False,\n",
       "       False, False,  True, False,  True, False, False, False,  True,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False,  True, False, False,\n",
       "       False,  True, False, False,  True,  True, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "        True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_11</th>\n",
       "      <th>tfidf_30</th>\n",
       "      <th>tfidf_37</th>\n",
       "      <th>tfidf_38</th>\n",
       "      <th>tfidf_39</th>\n",
       "      <th>tfidf_43</th>\n",
       "      <th>tfidf_46</th>\n",
       "      <th>tfidf_48</th>\n",
       "      <th>...</th>\n",
       "      <th>diesel</th>\n",
       "      <th>earthy</th>\n",
       "      <th>lemon</th>\n",
       "      <th>orange</th>\n",
       "      <th>pine</th>\n",
       "      <th>pungent</th>\n",
       "      <th>rose</th>\n",
       "      <th>sweet</th>\n",
       "      <th>vanilla</th>\n",
       "      <th>woody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tfidf_6  tfidf_7  tfidf_11  tfidf_30  tfidf_37  tfidf_38  tfidf_39  \\\n",
       "0          0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "1          0.0      0.0   0.11584       0.0       0.0       0.0       0.0   \n",
       "2          0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "3          0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "4          0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "59995      0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "59996      0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "59997      0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "59998      0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "59999      0.0      0.0   0.00000       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       tfidf_43  tfidf_46  tfidf_48  ...  diesel  earthy  lemon  orange  pine  \\\n",
       "0           0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "1           0.0       0.0       0.0  ...       0       1      0       0     0   \n",
       "2           0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "3           0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "4           0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "...         ...       ...       ...  ...     ...     ...    ...     ...   ...   \n",
       "59995       0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "59996       0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "59997       0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "59998       0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "59999       0.0       0.0       0.0  ...       0       0      0       0     0   \n",
       "\n",
       "       pungent  rose  sweet  vanilla  woody  \n",
       "0            0     0      0        0      0  \n",
       "1            0     0      0        1      0  \n",
       "2            0     1      0        1      1  \n",
       "3            0     0      0        0      0  \n",
       "4            0     0      0        0      0  \n",
       "...        ...   ...    ...      ...    ...  \n",
       "59995        0     0      0        0      0  \n",
       "59996        0     0      0        0      0  \n",
       "59997        0     0      0        0      0  \n",
       "59998        0     0      0        0      0  \n",
       "59999        0     0      0        0      0  \n",
       "\n",
       "[60000 rows x 105 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_X = df_rf[selected_features]\n",
    "selected_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split (after Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selected_X_rf_tfidf_cbd.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(selector, \"selector_rf_tfidf_cbd.pkl\")\n",
    "joblib.dump(selected_X, \"selected_X_rf_tfidf_cbd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selected_X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_7111/3758305.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    }
   ],
   "source": [
    "rfreg.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02540909082354229"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009372722217296141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09681282052133458"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9845444134717833"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9302221143830612"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [None, 10, 50, 100],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [100, 300, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(rfreg,  \n",
    "                     parameters,   \n",
    "                     cv=5, \n",
    "                     scoring='neg_mean_absolute_error',\n",
    "                     n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [None, 10, 50, 100],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 300, 500]},\n",
       "                   scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 100}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rscv_rf_tfidf_best_params_cbd.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rscv, \"rscv_rf_tfidf_cbd.pkl\")\n",
    "joblib.dump(rscv.best_params_, \"rscv_rf_tfidf_best_params_cbd.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF (after Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_7111/1949321143.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg_ht.fit(X_train1, y_train1)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "rfreg_ht = RandomForestRegressor(n_estimators = 100, min_samples_split = 2, min_samples_leaf = 1, max_features = 'auto', max_depth = 100)\n",
    "rfreg_ht.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg_ht.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg_ht.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025643888425807674"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009479257793309118"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09736148002834139"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9842467738225484"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9294289801084265"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots after Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg_test = rfreg_ht.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_rfreg_tfidf_cbd.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_pred_rfreg_test, \"y_pred_rfreg_test_tfidf_cbd.pkl\")\n",
    "joblib.dump(y_test, \"y_test_rfreg_tfidf_cbd.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023742919661553837"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008016439835754294"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08953457341024357"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9396403680747688"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assume y_pred is a numpy array and y_true is a pandas dataframe\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "column = \"X..CBD\"  # specify the target variable name\n",
    "ax.hist(y_pred_rfreg_test, alpha=0.5, label='y_pred', bins=20)\n",
    "ax.hist(y_test[column], alpha=0.5, label='y_true', bins=20)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(column)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('error_hist_knn_tfidf_cbd.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.971\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "corr_coef, p_value = pearsonr(y_pred_rfreg_test.flatten(), y_test.values.ravel())\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr_coef:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix = y_test.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
