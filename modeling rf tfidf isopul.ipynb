{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = pd.read_csv(\"df_isopul_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'tfidf_0',\n",
       " 'tfidf_1',\n",
       " 'tfidf_2',\n",
       " 'tfidf_3',\n",
       " 'tfidf_4',\n",
       " 'tfidf_5',\n",
       " 'tfidf_6',\n",
       " 'tfidf_7',\n",
       " 'tfidf_8',\n",
       " 'tfidf_9',\n",
       " 'tfidf_10',\n",
       " 'tfidf_11',\n",
       " 'tfidf_12',\n",
       " 'tfidf_13',\n",
       " 'tfidf_14',\n",
       " 'tfidf_15',\n",
       " 'tfidf_16',\n",
       " 'tfidf_17',\n",
       " 'tfidf_18',\n",
       " 'tfidf_19',\n",
       " 'tfidf_20',\n",
       " 'tfidf_21',\n",
       " 'tfidf_22',\n",
       " 'tfidf_23',\n",
       " 'tfidf_24',\n",
       " 'tfidf_25',\n",
       " 'tfidf_26',\n",
       " 'tfidf_27',\n",
       " 'tfidf_28',\n",
       " 'tfidf_29',\n",
       " 'tfidf_30',\n",
       " 'tfidf_31',\n",
       " 'tfidf_32',\n",
       " 'tfidf_33',\n",
       " 'tfidf_34',\n",
       " 'tfidf_35',\n",
       " 'tfidf_36',\n",
       " 'tfidf_37',\n",
       " 'tfidf_38',\n",
       " 'tfidf_39',\n",
       " 'tfidf_40',\n",
       " 'tfidf_41',\n",
       " 'tfidf_42',\n",
       " 'tfidf_43',\n",
       " 'tfidf_44',\n",
       " 'tfidf_45',\n",
       " 'tfidf_46',\n",
       " 'tfidf_47',\n",
       " 'tfidf_48',\n",
       " 'tfidf_49',\n",
       " 'tfidf_50',\n",
       " 'tfidf_51',\n",
       " 'tfidf_52',\n",
       " 'tfidf_53',\n",
       " 'tfidf_54',\n",
       " 'tfidf_55',\n",
       " 'tfidf_56',\n",
       " 'tfidf_57',\n",
       " 'tfidf_58',\n",
       " 'tfidf_59',\n",
       " 'tfidf_60',\n",
       " 'tfidf_61',\n",
       " 'tfidf_62',\n",
       " 'tfidf_63',\n",
       " 'tfidf_64',\n",
       " 'tfidf_65',\n",
       " 'tfidf_66',\n",
       " 'tfidf_67',\n",
       " 'tfidf_68',\n",
       " 'tfidf_69',\n",
       " 'tfidf_70',\n",
       " 'tfidf_71',\n",
       " 'tfidf_72',\n",
       " 'tfidf_73',\n",
       " 'tfidf_74',\n",
       " 'tfidf_75',\n",
       " 'tfidf_76',\n",
       " 'tfidf_77',\n",
       " 'tfidf_78',\n",
       " 'tfidf_79',\n",
       " 'tfidf_80',\n",
       " 'tfidf_81',\n",
       " 'tfidf_82',\n",
       " 'tfidf_83',\n",
       " 'tfidf_84',\n",
       " 'tfidf_85',\n",
       " 'tfidf_86',\n",
       " 'tfidf_87',\n",
       " 'tfidf_88',\n",
       " 'tfidf_89',\n",
       " 'tfidf_90',\n",
       " 'tfidf_91',\n",
       " 'tfidf_92',\n",
       " 'tfidf_93',\n",
       " 'tfidf_94',\n",
       " 'tfidf_95',\n",
       " 'tfidf_96',\n",
       " 'tfidf_97',\n",
       " 'tfidf_98',\n",
       " 'tfidf_99',\n",
       " 'tfidf_100',\n",
       " 'tfidf_101',\n",
       " 'tfidf_102',\n",
       " 'tfidf_103',\n",
       " 'tfidf_104',\n",
       " 'tfidf_105',\n",
       " 'tfidf_106',\n",
       " 'tfidf_107',\n",
       " 'tfidf_108',\n",
       " 'tfidf_109',\n",
       " 'tfidf_110',\n",
       " 'tfidf_111',\n",
       " 'tfidf_112',\n",
       " 'tfidf_113',\n",
       " 'tfidf_114',\n",
       " 'tfidf_115',\n",
       " 'tfidf_116',\n",
       " 'tfidf_117',\n",
       " 'tfidf_118',\n",
       " 'tfidf_119',\n",
       " 'tfidf_120',\n",
       " 'tfidf_121',\n",
       " 'tfidf_122',\n",
       " 'tfidf_123',\n",
       " 'tfidf_124',\n",
       " 'tfidf_125',\n",
       " 'tfidf_126',\n",
       " 'tfidf_127',\n",
       " 'tfidf_128',\n",
       " 'tfidf_129',\n",
       " 'tfidf_130',\n",
       " 'tfidf_131',\n",
       " 'tfidf_132',\n",
       " 'tfidf_133',\n",
       " 'tfidf_134',\n",
       " 'tfidf_135',\n",
       " 'tfidf_136',\n",
       " 'tfidf_137',\n",
       " 'tfidf_138',\n",
       " 'tfidf_139',\n",
       " 'tfidf_140',\n",
       " 'tfidf_141',\n",
       " 'tfidf_142',\n",
       " 'tfidf_143',\n",
       " 'tfidf_144',\n",
       " 'tfidf_145',\n",
       " 'tfidf_146',\n",
       " 'tfidf_147',\n",
       " 'tfidf_148',\n",
       " 'tfidf_149',\n",
       " 'tfidf_150',\n",
       " 'tfidf_151',\n",
       " 'tfidf_152',\n",
       " 'tfidf_153',\n",
       " 'tfidf_154',\n",
       " 'tfidf_155',\n",
       " 'tfidf_156',\n",
       " 'tfidf_157',\n",
       " 'tfidf_158',\n",
       " 'tfidf_159',\n",
       " 'tfidf_160',\n",
       " 'tfidf_161',\n",
       " 'tfidf_162',\n",
       " 'tfidf_163',\n",
       " 'tfidf_164',\n",
       " 'tfidf_165',\n",
       " 'tfidf_166',\n",
       " 'tfidf_167',\n",
       " 'tfidf_168',\n",
       " 'tfidf_169',\n",
       " 'tfidf_170',\n",
       " 'tfidf_171',\n",
       " 'tfidf_172',\n",
       " 'tfidf_173',\n",
       " 'tfidf_174',\n",
       " 'tfidf_175',\n",
       " 'tfidf_176',\n",
       " 'tfidf_177',\n",
       " 'tfidf_178',\n",
       " 'tfidf_179',\n",
       " 'tfidf_180',\n",
       " 'tfidf_181',\n",
       " 'tfidf_182',\n",
       " 'tfidf_183',\n",
       " 'tfidf_184',\n",
       " 'tfidf_185',\n",
       " 'tfidf_186',\n",
       " 'tfidf_187',\n",
       " 'tfidf_188',\n",
       " 'tfidf_189',\n",
       " 'tfidf_190',\n",
       " 'tfidf_191',\n",
       " 'tfidf_192',\n",
       " 'tfidf_193',\n",
       " 'tfidf_194',\n",
       " 'tfidf_195',\n",
       " 'tfidf_196',\n",
       " 'tfidf_197',\n",
       " 'tfidf_198',\n",
       " 'tfidf_199',\n",
       " 'tfidf_200',\n",
       " 'tfidf_201',\n",
       " 'tfidf_202',\n",
       " 'tfidf_203',\n",
       " 'tfidf_204',\n",
       " 'tfidf_205',\n",
       " 'tfidf_206',\n",
       " 'tfidf_207',\n",
       " 'tfidf_208',\n",
       " 'tfidf_209',\n",
       " 'tfidf_210',\n",
       " 'tfidf_211',\n",
       " 'tfidf_212',\n",
       " 'tfidf_213',\n",
       " 'tfidf_214',\n",
       " 'tfidf_215',\n",
       " 'tfidf_216',\n",
       " 'tfidf_217',\n",
       " 'tfidf_218',\n",
       " 'tfidf_219',\n",
       " 'tfidf_220',\n",
       " 'tfidf_221',\n",
       " 'tfidf_222',\n",
       " 'tfidf_223',\n",
       " 'tfidf_224',\n",
       " 'tfidf_225',\n",
       " 'tfidf_226',\n",
       " 'tfidf_227',\n",
       " 'tfidf_228',\n",
       " 'tfidf_229',\n",
       " 'tfidf_230',\n",
       " 'tfidf_231',\n",
       " 'tfidf_232',\n",
       " 'tfidf_233',\n",
       " 'tfidf_234',\n",
       " 'tfidf_235',\n",
       " 'tfidf_236',\n",
       " 'tfidf_237',\n",
       " 'tfidf_238',\n",
       " 'tfidf_239',\n",
       " 'tfidf_240',\n",
       " 'tfidf_241',\n",
       " 'tfidf_242',\n",
       " 'tfidf_243',\n",
       " 'tfidf_244',\n",
       " 'tfidf_245',\n",
       " 'tfidf_246',\n",
       " 'tfidf_247',\n",
       " 'tfidf_248',\n",
       " 'tfidf_249',\n",
       " 'tfidf_250',\n",
       " 'tfidf_251',\n",
       " 'tfidf_252',\n",
       " 'tfidf_253',\n",
       " 'tfidf_254',\n",
       " 'tfidf_255',\n",
       " 'tfidf_256',\n",
       " 'tfidf_257',\n",
       " 'tfidf_258',\n",
       " 'tfidf_259',\n",
       " 'tfidf_260',\n",
       " 'tfidf_261',\n",
       " 'tfidf_262',\n",
       " 'tfidf_263',\n",
       " 'tfidf_264',\n",
       " 'tfidf_265',\n",
       " 'tfidf_266',\n",
       " 'tfidf_267',\n",
       " 'tfidf_268',\n",
       " 'tfidf_269',\n",
       " 'tfidf_270',\n",
       " 'tfidf_271',\n",
       " 'tfidf_272',\n",
       " 'tfidf_273',\n",
       " 'tfidf_274',\n",
       " 'tfidf_275',\n",
       " 'tfidf_276',\n",
       " 'tfidf_277',\n",
       " 'tfidf_278',\n",
       " 'tfidf_279',\n",
       " 'tfidf_280',\n",
       " 'tfidf_281',\n",
       " 'tfidf_282',\n",
       " 'tfidf_283',\n",
       " 'tfidf_284',\n",
       " 'tfidf_285',\n",
       " 'tfidf_286',\n",
       " 'tfidf_287',\n",
       " 'tfidf_288',\n",
       " 'tfidf_289',\n",
       " 'tfidf_290',\n",
       " 'tfidf_291',\n",
       " 'tfidf_292',\n",
       " 'tfidf_293',\n",
       " 'tfidf_294',\n",
       " 'tfidf_295',\n",
       " 'tfidf_296',\n",
       " 'tfidf_297',\n",
       " 'tfidf_298',\n",
       " 'tfidf_299',\n",
       " 'tfidf_300',\n",
       " 'tfidf_301',\n",
       " 'tfidf_302',\n",
       " 'tfidf_303',\n",
       " 'tfidf_304',\n",
       " 'tfidf_305',\n",
       " 'tfidf_306',\n",
       " 'tfidf_307',\n",
       " 'tfidf_308',\n",
       " 'tfidf_309',\n",
       " 'tfidf_310',\n",
       " 'tfidf_311',\n",
       " 'tfidf_312',\n",
       " 'tfidf_313',\n",
       " 'tfidf_314',\n",
       " 'tfidf_315',\n",
       " 'tfidf_316',\n",
       " 'tfidf_317',\n",
       " 'tfidf_318',\n",
       " 'tfidf_319',\n",
       " 'tfidf_320',\n",
       " 'tfidf_321',\n",
       " 'tfidf_322',\n",
       " 'tfidf_323',\n",
       " 'tfidf_324',\n",
       " 'tfidf_325',\n",
       " 'tfidf_326',\n",
       " 'tfidf_327',\n",
       " 'tfidf_328',\n",
       " 'tfidf_329',\n",
       " 'tfidf_330',\n",
       " 'tfidf_331',\n",
       " 'tfidf_332',\n",
       " 'tfidf_333',\n",
       " 'tfidf_334',\n",
       " 'tfidf_335',\n",
       " 'tfidf_336',\n",
       " 'tfidf_337',\n",
       " 'tfidf_338',\n",
       " 'tfidf_339',\n",
       " 'tfidf_340',\n",
       " 'tfidf_341',\n",
       " 'tfidf_342',\n",
       " 'tfidf_343',\n",
       " 'tfidf_344',\n",
       " 'tfidf_345',\n",
       " 'tfidf_346',\n",
       " 'tfidf_347',\n",
       " 'tfidf_348',\n",
       " 'tfidf_349',\n",
       " 'tfidf_350',\n",
       " 'tfidf_351',\n",
       " 'tfidf_352',\n",
       " 'tfidf_353',\n",
       " 'tfidf_354',\n",
       " 'tfidf_355',\n",
       " 'tfidf_356',\n",
       " 'tfidf_357',\n",
       " 'tfidf_358',\n",
       " 'tfidf_359',\n",
       " 'tfidf_360',\n",
       " 'tfidf_361',\n",
       " 'tfidf_362',\n",
       " 'tfidf_363',\n",
       " 'tfidf_364',\n",
       " 'tfidf_365',\n",
       " 'tfidf_366',\n",
       " 'tfidf_367',\n",
       " 'tfidf_368',\n",
       " 'tfidf_369',\n",
       " 'tfidf_370',\n",
       " 'tfidf_371',\n",
       " 'tfidf_372',\n",
       " 'tfidf_373',\n",
       " 'tfidf_374',\n",
       " 'tfidf_375',\n",
       " 'tfidf_376',\n",
       " 'tfidf_377',\n",
       " 'tfidf_378',\n",
       " 'tfidf_379',\n",
       " 'tfidf_380',\n",
       " 'tfidf_381',\n",
       " 'tfidf_382',\n",
       " 'tfidf_383',\n",
       " 'tfidf_384',\n",
       " 'tfidf_385',\n",
       " 'tfidf_386',\n",
       " 'tfidf_387',\n",
       " 'hybrid',\n",
       " 'indica',\n",
       " 'sativa',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'aroused',\n",
       " 'arthritis',\n",
       " 'creative',\n",
       " 'depression',\n",
       " 'dizzy',\n",
       " 'dry eyes',\n",
       " 'dry mouth',\n",
       " 'energetic',\n",
       " 'epilepsy',\n",
       " 'euphoric',\n",
       " 'eye pressure',\n",
       " 'fatigue',\n",
       " 'focused',\n",
       " 'giggly',\n",
       " 'happy',\n",
       " 'headache',\n",
       " 'hungry',\n",
       " 'migraines',\n",
       " 'pain',\n",
       " 'paranoid',\n",
       " 'relaxed',\n",
       " 'seizures',\n",
       " 'sleepy',\n",
       " 'spasticity',\n",
       " 'stress',\n",
       " 'talkative',\n",
       " 'tingly',\n",
       " 'uplifted',\n",
       " 'ammonia',\n",
       " 'apple',\n",
       " 'apricot',\n",
       " 'berry',\n",
       " 'blue cheese',\n",
       " 'blueberry',\n",
       " 'butter',\n",
       " 'cheese',\n",
       " 'chemical',\n",
       " 'chestnut',\n",
       " 'citrus',\n",
       " 'coffee',\n",
       " 'diesel',\n",
       " 'earthy',\n",
       " 'flowery',\n",
       " 'fruit',\n",
       " 'grape',\n",
       " 'grapefruit',\n",
       " 'honey',\n",
       " 'lavender',\n",
       " 'lemon',\n",
       " 'lime',\n",
       " 'mango',\n",
       " 'menthol',\n",
       " 'mint',\n",
       " 'nutty',\n",
       " 'orange',\n",
       " 'peach',\n",
       " 'pear',\n",
       " 'pepper',\n",
       " 'pine',\n",
       " 'pineapple',\n",
       " 'plum',\n",
       " 'pungent',\n",
       " 'rose',\n",
       " 'sage',\n",
       " 'skunk',\n",
       " 'spicy/herbal',\n",
       " 'strawberry',\n",
       " 'sweet',\n",
       " 'tar',\n",
       " 'tea',\n",
       " 'tobacco',\n",
       " 'tree',\n",
       " 'tropical',\n",
       " 'vanilla',\n",
       " 'violet',\n",
       " 'woody',\n",
       " 'X..Isopulegol']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_rf.drop(['index', 'X..Isopulegol'], axis = 1)\n",
    "y = df_rf[['X..Isopulegol']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting histograms on target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array = y.to_numpy()\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(y, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF modeling (before Feature selection and Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_11510/350139188.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02149466175683392"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007075075735529268"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08411346940609017"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990659156901988"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716991978317326"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots for each target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame({\n",
    "    \"features\": X_train1.columns,\n",
    "    \"score\": rfreg.feature_importances_\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     features     score\n",
       "0     tfidf_0  0.000329\n",
       "1     tfidf_1  0.000818\n",
       "2     tfidf_2  0.000272\n",
       "3     tfidf_3  0.002347\n",
       "4     tfidf_4  0.001928\n",
       "..        ...       ...\n",
       "464      tree  0.000159\n",
       "465  tropical  0.000390\n",
       "466   vanilla  0.000124\n",
       "467    violet  0.000115\n",
       "468     woody  0.000437\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_ranked = df_feat.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>0.296244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>diesel</td>\n",
       "      <td>0.019598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>cheese</td>\n",
       "      <td>0.012849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>tfidf_306</td>\n",
       "      <td>0.012636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>orange</td>\n",
       "      <td>0.012181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>tfidf_342</td>\n",
       "      <td>0.010267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>flowery</td>\n",
       "      <td>0.010259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>tfidf_329</td>\n",
       "      <td>0.009510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>tfidf_149</td>\n",
       "      <td>0.007952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tfidf_37</td>\n",
       "      <td>0.007860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>sweet</td>\n",
       "      <td>0.007802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>tfidf_253</td>\n",
       "      <td>0.007562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>tfidf_286</td>\n",
       "      <td>0.007482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>tfidf_154</td>\n",
       "      <td>0.006638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>lemon</td>\n",
       "      <td>0.006539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>citrus</td>\n",
       "      <td>0.006472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>tfidf_312</td>\n",
       "      <td>0.006305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>tfidf_145</td>\n",
       "      <td>0.006257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>tfidf_309</td>\n",
       "      <td>0.006107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>tfidf_93</td>\n",
       "      <td>0.005827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tfidf_175</td>\n",
       "      <td>0.005711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>tfidf_239</td>\n",
       "      <td>0.005627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>aroused</td>\n",
       "      <td>0.005071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>tfidf_168</td>\n",
       "      <td>0.004896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>tfidf_263</td>\n",
       "      <td>0.004537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>tfidf_303</td>\n",
       "      <td>0.004415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>tfidf_338</td>\n",
       "      <td>0.004402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>tfidf_345</td>\n",
       "      <td>0.004358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>mango</td>\n",
       "      <td>0.004282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>giggly</td>\n",
       "      <td>0.004277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>tfidf_128</td>\n",
       "      <td>0.004246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>tfidf_199</td>\n",
       "      <td>0.004140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>tfidf_186</td>\n",
       "      <td>0.004092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>tfidf_367</td>\n",
       "      <td>0.004086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>tfidf_251</td>\n",
       "      <td>0.004008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>tfidf_118</td>\n",
       "      <td>0.003975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>tfidf_90</td>\n",
       "      <td>0.003903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>tfidf_377</td>\n",
       "      <td>0.003857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>tfidf_152</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tfidf_29</td>\n",
       "      <td>0.003728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_7</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tfidf_20</td>\n",
       "      <td>0.003615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>tfidf_207</td>\n",
       "      <td>0.003591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>grape</td>\n",
       "      <td>0.003584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>blueberry</td>\n",
       "      <td>0.003567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>spicy/herbal</td>\n",
       "      <td>0.003470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>tfidf_380</td>\n",
       "      <td>0.003461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>tfidf_188</td>\n",
       "      <td>0.003430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tfidf_53</td>\n",
       "      <td>0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>relaxed</td>\n",
       "      <td>0.003325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>tfidf_102</td>\n",
       "      <td>0.003222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>tfidf_361</td>\n",
       "      <td>0.003085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>mint</td>\n",
       "      <td>0.003021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>tfidf_111</td>\n",
       "      <td>0.002898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>tfidf_148</td>\n",
       "      <td>0.002864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>tfidf_119</td>\n",
       "      <td>0.002833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>tfidf_200</td>\n",
       "      <td>0.002830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>tfidf_163</td>\n",
       "      <td>0.002772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>creative</td>\n",
       "      <td>0.002759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>tfidf_203</td>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>tfidf_281</td>\n",
       "      <td>0.002736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tfidf_69</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>tfidf_245</td>\n",
       "      <td>0.002671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>tfidf_278</td>\n",
       "      <td>0.002652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>tfidf_366</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>tfidf_297</td>\n",
       "      <td>0.002608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>euphoric</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>tfidf_180</td>\n",
       "      <td>0.002594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>tfidf_319</td>\n",
       "      <td>0.002592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>tfidf_337</td>\n",
       "      <td>0.002582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>tfidf_181</td>\n",
       "      <td>0.002578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tfidf_22</td>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tfidf_23</td>\n",
       "      <td>0.002510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>tfidf_196</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>tfidf_381</td>\n",
       "      <td>0.002483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>tfidf_311</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>tfidf_331</td>\n",
       "      <td>0.002462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>tfidf_285</td>\n",
       "      <td>0.002459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>tfidf_162</td>\n",
       "      <td>0.002452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tfidf_34</td>\n",
       "      <td>0.002399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tfidf_33</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>tfidf_294</td>\n",
       "      <td>0.002393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>tfidf_362</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tfidf_41</td>\n",
       "      <td>0.002352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.002347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>tfidf_121</td>\n",
       "      <td>0.002343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_43</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>tfidf_357</td>\n",
       "      <td>0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>chemical</td>\n",
       "      <td>0.002307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tfidf_56</td>\n",
       "      <td>0.002297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>tfidf_273</td>\n",
       "      <td>0.002284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>tfidf_326</td>\n",
       "      <td>0.002281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>tfidf_144</td>\n",
       "      <td>0.002278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>tfidf_204</td>\n",
       "      <td>0.002257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>tfidf_165</td>\n",
       "      <td>0.002253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.002197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tfidf_58</td>\n",
       "      <td>0.002192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>tfidf_258</td>\n",
       "      <td>0.002189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>tfidf_209</td>\n",
       "      <td>0.002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tfidf_105</td>\n",
       "      <td>0.002165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>tfidf_315</td>\n",
       "      <td>0.002149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>tfidf_368</td>\n",
       "      <td>0.002090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>dry mouth</td>\n",
       "      <td>0.002072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tfidf_30</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>tfidf_354</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>tfidf_261</td>\n",
       "      <td>0.001979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>tfidf_314</td>\n",
       "      <td>0.001970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>tfidf_146</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>tfidf_283</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>tfidf_363</td>\n",
       "      <td>0.001941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>tfidf_330</td>\n",
       "      <td>0.001941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>tfidf_116</td>\n",
       "      <td>0.001880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>tfidf_205</td>\n",
       "      <td>0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>skunk</td>\n",
       "      <td>0.001811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>tfidf_153</td>\n",
       "      <td>0.001808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>tfidf_78</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>tfidf_210</td>\n",
       "      <td>0.001794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>tfidf_178</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tfidf_48</td>\n",
       "      <td>0.001786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>tfidf_298</td>\n",
       "      <td>0.001781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>tfidf_237</td>\n",
       "      <td>0.001776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>tfidf_206</td>\n",
       "      <td>0.001764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>tfidf_230</td>\n",
       "      <td>0.001745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>tfidf_240</td>\n",
       "      <td>0.001725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>tfidf_101</td>\n",
       "      <td>0.001724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>tfidf_147</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>tfidf_343</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>tfidf_260</td>\n",
       "      <td>0.001691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tfidf_54</td>\n",
       "      <td>0.001689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>tfidf_267</td>\n",
       "      <td>0.001685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>tfidf_71</td>\n",
       "      <td>0.001678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tfidf_45</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>tfidf_184</td>\n",
       "      <td>0.001630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>tfidf_193</td>\n",
       "      <td>0.001627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>tfidf_190</td>\n",
       "      <td>0.001615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tfidf_17</td>\n",
       "      <td>0.001602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>tfidf_211</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tfidf_299</td>\n",
       "      <td>0.001597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tfidf_79</td>\n",
       "      <td>0.001590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>tfidf_257</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>tfidf_179</td>\n",
       "      <td>0.001583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>tfidf_139</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>tfidf_85</td>\n",
       "      <td>0.001563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>tfidf_158</td>\n",
       "      <td>0.001562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>tfidf_141</td>\n",
       "      <td>0.001555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>tfidf_151</td>\n",
       "      <td>0.001553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>tfidf_123</td>\n",
       "      <td>0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>tfidf_198</td>\n",
       "      <td>0.001545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>tfidf_353</td>\n",
       "      <td>0.001532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>tfidf_246</td>\n",
       "      <td>0.001524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>tfidf_217</td>\n",
       "      <td>0.001519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>lavender</td>\n",
       "      <td>0.001519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tfidf_13</td>\n",
       "      <td>0.001503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>tfidf_137</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>talkative</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>tfidf_231</td>\n",
       "      <td>0.001482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>tfidf_167</td>\n",
       "      <td>0.001480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>tfidf_232</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>tfidf_340</td>\n",
       "      <td>0.001457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>tfidf_295</td>\n",
       "      <td>0.001451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>uplifted</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tfidf_31</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>tfidf_104</td>\n",
       "      <td>0.001431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>tfidf_350</td>\n",
       "      <td>0.001431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tfidf_332</td>\n",
       "      <td>0.001424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>tfidf_194</td>\n",
       "      <td>0.001401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tfidf_19</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>tfidf_166</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>tfidf_115</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>tfidf_321</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>energetic</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>tfidf_346</td>\n",
       "      <td>0.001291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tfidf_24</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>tfidf_73</td>\n",
       "      <td>0.001287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>tfidf_279</td>\n",
       "      <td>0.001284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>tfidf_117</td>\n",
       "      <td>0.001280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>hungry</td>\n",
       "      <td>0.001270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tfidf_86</td>\n",
       "      <td>0.001264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>focused</td>\n",
       "      <td>0.001254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>tingly</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>tfidf_189</td>\n",
       "      <td>0.001218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>tfidf_88</td>\n",
       "      <td>0.001217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tfidf_11</td>\n",
       "      <td>0.001198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>tfidf_212</td>\n",
       "      <td>0.001197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>tfidf_222</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tfidf_99</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>tfidf_249</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>tfidf_265</td>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>tfidf_355</td>\n",
       "      <td>0.001140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>tfidf_107</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>tfidf_150</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>tfidf_197</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>tfidf_276</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>tfidf_229</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tfidf_9</td>\n",
       "      <td>0.001109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>tfidf_169</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>tfidf_160</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>tfidf_173</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>tfidf_288</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>tfidf_182</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tfidf_50</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tfidf_10</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>sleepy</td>\n",
       "      <td>0.001050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>tfidf_185</td>\n",
       "      <td>0.001038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>tfidf_352</td>\n",
       "      <td>0.001031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>tfidf_289</td>\n",
       "      <td>0.001030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>tfidf_334</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>tfidf_171</td>\n",
       "      <td>0.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tfidf_14</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>tfidf_247</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tfidf_21</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>tfidf_318</td>\n",
       "      <td>0.000992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>tfidf_256</td>\n",
       "      <td>0.000992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>tfidf_97</td>\n",
       "      <td>0.000989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>tfidf_275</td>\n",
       "      <td>0.000987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>tfidf_375</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>tfidf_60</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>pine</td>\n",
       "      <td>0.000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>tfidf_142</td>\n",
       "      <td>0.000971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>tfidf_157</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>tfidf_243</td>\n",
       "      <td>0.000962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tfidf_25</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tfidf_46</td>\n",
       "      <td>0.000951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>tfidf_215</td>\n",
       "      <td>0.000950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>tfidf_322</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>tfidf_130</td>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tfidf_98</td>\n",
       "      <td>0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>tfidf_170</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>tfidf_385</td>\n",
       "      <td>0.000934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>tfidf_290</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>tfidf_320</td>\n",
       "      <td>0.000913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>tfidf_228</td>\n",
       "      <td>0.000909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>tfidf_371</td>\n",
       "      <td>0.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tfidf_35</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>pungent</td>\n",
       "      <td>0.000903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>tfidf_159</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>tfidf_124</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>tfidf_277</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tfidf_65</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>tfidf_136</td>\n",
       "      <td>0.000869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>earthy</td>\n",
       "      <td>0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>tfidf_341</td>\n",
       "      <td>0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>tfidf_120</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_5</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>tfidf_112</td>\n",
       "      <td>0.000853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>tfidf_378</td>\n",
       "      <td>0.000849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>tfidf_70</td>\n",
       "      <td>0.000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>tfidf_125</td>\n",
       "      <td>0.000838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>tfidf_233</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tfidf_68</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>tfidf_122</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>tfidf_172</td>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>tfidf_293</td>\n",
       "      <td>0.000808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>tfidf_347</td>\n",
       "      <td>0.000806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>tfidf_214</td>\n",
       "      <td>0.000806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tfidf_39</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>tfidf_344</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>dry eyes</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>tfidf_336</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>0.000780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>tfidf_92</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>tfidf_360</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>tfidf_255</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>tfidf_376</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>tfidf_284</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>tfidf_225</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tfidf_49</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>tfidf_221</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>tfidf_374</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tfidf_32</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>tfidf_220</td>\n",
       "      <td>0.000732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>tfidf_356</td>\n",
       "      <td>0.000731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>tfidf_87</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>tfidf_313</td>\n",
       "      <td>0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tfidf_16</td>\n",
       "      <td>0.000728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>tfidf_291</td>\n",
       "      <td>0.000714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tfidf_26</td>\n",
       "      <td>0.000708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>tfidf_126</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>tfidf_370</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>tfidf_241</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>anxious</td>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tfidf_236</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>tfidf_234</td>\n",
       "      <td>0.000691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>pineapple</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>tfidf_307</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>tfidf_156</td>\n",
       "      <td>0.000682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>tfidf_373</td>\n",
       "      <td>0.000677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>tfidf_103</td>\n",
       "      <td>0.000671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>tfidf_384</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tfidf_27</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>tfidf_364</td>\n",
       "      <td>0.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>tfidf_259</td>\n",
       "      <td>0.000657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>tfidf_379</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>tfidf_109</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>tfidf_89</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tfidf_8</td>\n",
       "      <td>0.000638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>tfidf_164</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>tfidf_382</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>tfidf_272</td>\n",
       "      <td>0.000613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tfidf_67</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>tfidf_358</td>\n",
       "      <td>0.000610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>apricot</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>tfidf_216</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>tfidf_325</td>\n",
       "      <td>0.000593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tfidf_40</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tfidf_12</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tfidf_28</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>tfidf_317</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.000587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>tfidf_287</td>\n",
       "      <td>0.000582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>tfidf_81</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>tfidf_59</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>tfidf_63</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>tfidf_383</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>tfidf_134</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>tfidf_64</td>\n",
       "      <td>0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>tfidf_135</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tfidf_42</td>\n",
       "      <td>0.000550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>tfidf_129</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>tfidf_110</td>\n",
       "      <td>0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>tfidf_264</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>tfidf_224</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>tfidf_187</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>tfidf_140</td>\n",
       "      <td>0.000519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>tfidf_262</td>\n",
       "      <td>0.000519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>tfidf_114</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>tfidf_127</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>tfidf_244</td>\n",
       "      <td>0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>tfidf_304</td>\n",
       "      <td>0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>tfidf_268</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tfidf_47</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>tfidf_280</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>tfidf_108</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>tfidf_238</td>\n",
       "      <td>0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>tfidf_248</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>tfidf_155</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>tfidf_328</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tfidf_36</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>tfidf_75</td>\n",
       "      <td>0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>tfidf_270</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>dizzy</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>paranoid</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tfidf_52</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>tfidf_195</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>tea</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>tfidf_83</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>tfidf_201</td>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>tfidf_61</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>tfidf_235</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>tfidf_132</td>\n",
       "      <td>0.000394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>plum</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>tfidf_227</td>\n",
       "      <td>0.000389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>tfidf_310</td>\n",
       "      <td>0.000376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>tfidf_202</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>tfidf_177</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>tfidf_386</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfidf_6</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>tfidf_95</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>tfidf_213</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>tfidf_76</td>\n",
       "      <td>0.000359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>tfidf_192</td>\n",
       "      <td>0.000352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>tfidf_91</td>\n",
       "      <td>0.000345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>tfidf_131</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tfidf_66</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>tfidf_305</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>tfidf_84</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>tfidf_96</td>\n",
       "      <td>0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tfidf_106</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>tfidf_292</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>tfidf_274</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>tfidf_176</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>tfidf_80</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>tfidf_226</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>tfidf_282</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tfidf_15</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>tfidf_348</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>tfidf_387</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tfidf_51</td>\n",
       "      <td>0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>tfidf_252</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>tfidf_271</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>honey</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>tfidf_316</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>tfidf_372</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>tfidf_365</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>tfidf_269</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>fruit</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>tfidf_77</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tfidf_94</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>tfidf_242</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>tfidf_324</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>headache</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>tfidf_369</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tfidf_191</td>\n",
       "      <td>0.000228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tfidf_18</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>tfidf_74</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tfidf_302</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>nutty</td>\n",
       "      <td>0.000205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tfidf_44</td>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>tfidf_138</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>tfidf_183</td>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>apple</td>\n",
       "      <td>0.000198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>tfidf_323</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>tfidf_143</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>tfidf_100</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>tfidf_113</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>tfidf_308</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>tfidf_351</td>\n",
       "      <td>0.000151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tfidf_38</td>\n",
       "      <td>0.000151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>tfidf_349</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>tfidf_254</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>tfidf_266</td>\n",
       "      <td>0.000146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>tfidf_223</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>tfidf_161</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>tfidf_82</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>sage</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>tfidf_62</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>chestnut</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tfidf_296</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>pepper</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tfidf_55</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>tfidf_250</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>tfidf_339</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>menthol</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>ammonia</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>tfidf_218</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>lime</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>tfidf_301</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>tfidf_174</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tfidf_208</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>tfidf_219</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>tfidf_300</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>tfidf_333</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>tfidf_133</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>tfidf_359</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>tfidf_57</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>tfidf_335</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>tfidf_72</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>tfidf_327</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>pear</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>peach</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>blue cheese</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>tar</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>rose</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>depression</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>butter</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>stress</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>migraines</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>sativa</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>indica</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>arthritis</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>eye pressure</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>fatigue</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>pain</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>seizures</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>spasticity</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>epilepsy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         features     score\n",
       "388        hybrid  0.296244\n",
       "433        diesel  0.019598\n",
       "428        cheese  0.012849\n",
       "306     tfidf_306  0.012636\n",
       "447        orange  0.012181\n",
       "342     tfidf_342  0.010267\n",
       "435       flowery  0.010259\n",
       "329     tfidf_329  0.009510\n",
       "149     tfidf_149  0.007952\n",
       "37       tfidf_37  0.007860\n",
       "460         sweet  0.007802\n",
       "253     tfidf_253  0.007562\n",
       "286     tfidf_286  0.007482\n",
       "154     tfidf_154  0.006638\n",
       "441         lemon  0.006539\n",
       "431        citrus  0.006472\n",
       "312     tfidf_312  0.006305\n",
       "145     tfidf_145  0.006257\n",
       "309     tfidf_309  0.006107\n",
       "93       tfidf_93  0.005827\n",
       "175     tfidf_175  0.005711\n",
       "239     tfidf_239  0.005627\n",
       "393       aroused  0.005071\n",
       "168     tfidf_168  0.004896\n",
       "263     tfidf_263  0.004537\n",
       "303     tfidf_303  0.004415\n",
       "338     tfidf_338  0.004402\n",
       "345     tfidf_345  0.004358\n",
       "443         mango  0.004282\n",
       "406        giggly  0.004277\n",
       "128     tfidf_128  0.004246\n",
       "199     tfidf_199  0.004140\n",
       "186     tfidf_186  0.004092\n",
       "367     tfidf_367  0.004086\n",
       "251     tfidf_251  0.004008\n",
       "118     tfidf_118  0.003975\n",
       "90       tfidf_90  0.003903\n",
       "377     tfidf_377  0.003857\n",
       "152     tfidf_152  0.003827\n",
       "29       tfidf_29  0.003728\n",
       "7         tfidf_7  0.003639\n",
       "20       tfidf_20  0.003615\n",
       "207     tfidf_207  0.003591\n",
       "437         grape  0.003584\n",
       "426     blueberry  0.003567\n",
       "458  spicy/herbal  0.003470\n",
       "380     tfidf_380  0.003461\n",
       "188     tfidf_188  0.003430\n",
       "53       tfidf_53  0.003345\n",
       "413       relaxed  0.003325\n",
       "102     tfidf_102  0.003222\n",
       "361     tfidf_361  0.003085\n",
       "445          mint  0.003021\n",
       "111     tfidf_111  0.002898\n",
       "148     tfidf_148  0.002864\n",
       "119     tfidf_119  0.002833\n",
       "200     tfidf_200  0.002830\n",
       "163     tfidf_163  0.002772\n",
       "395      creative  0.002759\n",
       "203     tfidf_203  0.002740\n",
       "281     tfidf_281  0.002736\n",
       "69       tfidf_69  0.002710\n",
       "245     tfidf_245  0.002671\n",
       "278     tfidf_278  0.002652\n",
       "366     tfidf_366  0.002612\n",
       "297     tfidf_297  0.002608\n",
       "402      euphoric  0.002600\n",
       "180     tfidf_180  0.002594\n",
       "319     tfidf_319  0.002592\n",
       "337     tfidf_337  0.002582\n",
       "181     tfidf_181  0.002578\n",
       "22       tfidf_22  0.002533\n",
       "23       tfidf_23  0.002510\n",
       "196     tfidf_196  0.002500\n",
       "381     tfidf_381  0.002483\n",
       "311     tfidf_311  0.002477\n",
       "331     tfidf_331  0.002462\n",
       "285     tfidf_285  0.002459\n",
       "162     tfidf_162  0.002452\n",
       "34       tfidf_34  0.002399\n",
       "33       tfidf_33  0.002394\n",
       "294     tfidf_294  0.002393\n",
       "362     tfidf_362  0.002354\n",
       "41       tfidf_41  0.002352\n",
       "3         tfidf_3  0.002347\n",
       "121     tfidf_121  0.002343\n",
       "43       tfidf_43  0.002322\n",
       "357     tfidf_357  0.002321\n",
       "429      chemical  0.002307\n",
       "56       tfidf_56  0.002297\n",
       "273     tfidf_273  0.002284\n",
       "326     tfidf_326  0.002281\n",
       "144     tfidf_144  0.002278\n",
       "204     tfidf_204  0.002257\n",
       "165     tfidf_165  0.002253\n",
       "407         happy  0.002197\n",
       "58       tfidf_58  0.002192\n",
       "258     tfidf_258  0.002189\n",
       "209     tfidf_209  0.002182\n",
       "105     tfidf_105  0.002165\n",
       "315     tfidf_315  0.002149\n",
       "368     tfidf_368  0.002090\n",
       "399     dry mouth  0.002072\n",
       "30       tfidf_30  0.002036\n",
       "354     tfidf_354  0.001993\n",
       "261     tfidf_261  0.001979\n",
       "314     tfidf_314  0.001970\n",
       "146     tfidf_146  0.001947\n",
       "283     tfidf_283  0.001947\n",
       "363     tfidf_363  0.001941\n",
       "330     tfidf_330  0.001941\n",
       "4         tfidf_4  0.001928\n",
       "116     tfidf_116  0.001880\n",
       "205     tfidf_205  0.001854\n",
       "457         skunk  0.001811\n",
       "153     tfidf_153  0.001808\n",
       "78       tfidf_78  0.001804\n",
       "210     tfidf_210  0.001794\n",
       "178     tfidf_178  0.001791\n",
       "48       tfidf_48  0.001786\n",
       "298     tfidf_298  0.001781\n",
       "237     tfidf_237  0.001776\n",
       "206     tfidf_206  0.001764\n",
       "230     tfidf_230  0.001745\n",
       "240     tfidf_240  0.001725\n",
       "101     tfidf_101  0.001724\n",
       "147     tfidf_147  0.001711\n",
       "343     tfidf_343  0.001710\n",
       "260     tfidf_260  0.001691\n",
       "54       tfidf_54  0.001689\n",
       "267     tfidf_267  0.001685\n",
       "71       tfidf_71  0.001678\n",
       "45       tfidf_45  0.001652\n",
       "184     tfidf_184  0.001630\n",
       "193     tfidf_193  0.001627\n",
       "190     tfidf_190  0.001615\n",
       "17       tfidf_17  0.001602\n",
       "211     tfidf_211  0.001600\n",
       "299     tfidf_299  0.001597\n",
       "79       tfidf_79  0.001590\n",
       "257     tfidf_257  0.001584\n",
       "179     tfidf_179  0.001583\n",
       "139     tfidf_139  0.001574\n",
       "85       tfidf_85  0.001563\n",
       "158     tfidf_158  0.001562\n",
       "141     tfidf_141  0.001555\n",
       "151     tfidf_151  0.001553\n",
       "123     tfidf_123  0.001549\n",
       "198     tfidf_198  0.001545\n",
       "353     tfidf_353  0.001532\n",
       "246     tfidf_246  0.001524\n",
       "217     tfidf_217  0.001519\n",
       "440      lavender  0.001519\n",
       "13       tfidf_13  0.001503\n",
       "137     tfidf_137  0.001495\n",
       "418     talkative  0.001495\n",
       "231     tfidf_231  0.001482\n",
       "167     tfidf_167  0.001480\n",
       "232     tfidf_232  0.001462\n",
       "340     tfidf_340  0.001457\n",
       "295     tfidf_295  0.001451\n",
       "420      uplifted  0.001449\n",
       "31       tfidf_31  0.001449\n",
       "104     tfidf_104  0.001431\n",
       "350     tfidf_350  0.001431\n",
       "332     tfidf_332  0.001424\n",
       "194     tfidf_194  0.001401\n",
       "19       tfidf_19  0.001395\n",
       "166     tfidf_166  0.001374\n",
       "115     tfidf_115  0.001367\n",
       "321     tfidf_321  0.001346\n",
       "400     energetic  0.001318\n",
       "346     tfidf_346  0.001291\n",
       "24       tfidf_24  0.001290\n",
       "73       tfidf_73  0.001287\n",
       "279     tfidf_279  0.001284\n",
       "117     tfidf_117  0.001280\n",
       "409        hungry  0.001270\n",
       "86       tfidf_86  0.001264\n",
       "405       focused  0.001254\n",
       "419        tingly  0.001244\n",
       "189     tfidf_189  0.001218\n",
       "88       tfidf_88  0.001217\n",
       "11       tfidf_11  0.001198\n",
       "212     tfidf_212  0.001197\n",
       "222     tfidf_222  0.001193\n",
       "99       tfidf_99  0.001185\n",
       "249     tfidf_249  0.001156\n",
       "265     tfidf_265  0.001146\n",
       "355     tfidf_355  0.001140\n",
       "107     tfidf_107  0.001132\n",
       "150     tfidf_150  0.001130\n",
       "197     tfidf_197  0.001124\n",
       "276     tfidf_276  0.001123\n",
       "229     tfidf_229  0.001119\n",
       "9         tfidf_9  0.001109\n",
       "169     tfidf_169  0.001108\n",
       "160     tfidf_160  0.001091\n",
       "173     tfidf_173  0.001065\n",
       "288     tfidf_288  0.001062\n",
       "182     tfidf_182  0.001054\n",
       "50       tfidf_50  0.001053\n",
       "10       tfidf_10  0.001053\n",
       "415        sleepy  0.001050\n",
       "185     tfidf_185  0.001038\n",
       "352     tfidf_352  0.001031\n",
       "289     tfidf_289  0.001030\n",
       "334     tfidf_334  0.001018\n",
       "171     tfidf_171  0.001004\n",
       "14       tfidf_14  0.000997\n",
       "247     tfidf_247  0.000996\n",
       "21       tfidf_21  0.000995\n",
       "318     tfidf_318  0.000992\n",
       "256     tfidf_256  0.000992\n",
       "97       tfidf_97  0.000989\n",
       "275     tfidf_275  0.000987\n",
       "375     tfidf_375  0.000986\n",
       "60       tfidf_60  0.000976\n",
       "451          pine  0.000973\n",
       "142     tfidf_142  0.000971\n",
       "157     tfidf_157  0.000967\n",
       "243     tfidf_243  0.000962\n",
       "25       tfidf_25  0.000956\n",
       "46       tfidf_46  0.000951\n",
       "215     tfidf_215  0.000950\n",
       "322     tfidf_322  0.000945\n",
       "130     tfidf_130  0.000943\n",
       "98       tfidf_98  0.000940\n",
       "170     tfidf_170  0.000936\n",
       "385     tfidf_385  0.000934\n",
       "290     tfidf_290  0.000926\n",
       "320     tfidf_320  0.000913\n",
       "228     tfidf_228  0.000909\n",
       "371     tfidf_371  0.000906\n",
       "35       tfidf_35  0.000904\n",
       "454       pungent  0.000903\n",
       "159     tfidf_159  0.000901\n",
       "124     tfidf_124  0.000885\n",
       "277     tfidf_277  0.000883\n",
       "65       tfidf_65  0.000875\n",
       "136     tfidf_136  0.000869\n",
       "434        earthy  0.000868\n",
       "341     tfidf_341  0.000866\n",
       "120     tfidf_120  0.000865\n",
       "5         tfidf_5  0.000859\n",
       "112     tfidf_112  0.000853\n",
       "378     tfidf_378  0.000849\n",
       "70       tfidf_70  0.000845\n",
       "125     tfidf_125  0.000838\n",
       "233     tfidf_233  0.000835\n",
       "68       tfidf_68  0.000823\n",
       "1         tfidf_1  0.000818\n",
       "122     tfidf_122  0.000818\n",
       "172     tfidf_172  0.000816\n",
       "293     tfidf_293  0.000808\n",
       "347     tfidf_347  0.000806\n",
       "214     tfidf_214  0.000806\n",
       "39       tfidf_39  0.000804\n",
       "344     tfidf_344  0.000800\n",
       "398      dry eyes  0.000786\n",
       "336     tfidf_336  0.000786\n",
       "459    strawberry  0.000780\n",
       "92       tfidf_92  0.000778\n",
       "360     tfidf_360  0.000778\n",
       "255     tfidf_255  0.000775\n",
       "376     tfidf_376  0.000772\n",
       "284     tfidf_284  0.000772\n",
       "225     tfidf_225  0.000772\n",
       "49       tfidf_49  0.000770\n",
       "221     tfidf_221  0.000768\n",
       "374     tfidf_374  0.000747\n",
       "32       tfidf_32  0.000741\n",
       "220     tfidf_220  0.000732\n",
       "356     tfidf_356  0.000731\n",
       "87       tfidf_87  0.000730\n",
       "313     tfidf_313  0.000729\n",
       "438    grapefruit  0.000729\n",
       "16       tfidf_16  0.000728\n",
       "291     tfidf_291  0.000714\n",
       "26       tfidf_26  0.000708\n",
       "126     tfidf_126  0.000702\n",
       "370     tfidf_370  0.000696\n",
       "241     tfidf_241  0.000696\n",
       "392       anxious  0.000693\n",
       "236     tfidf_236  0.000692\n",
       "234     tfidf_234  0.000691\n",
       "452     pineapple  0.000689\n",
       "307     tfidf_307  0.000685\n",
       "156     tfidf_156  0.000682\n",
       "373     tfidf_373  0.000677\n",
       "103     tfidf_103  0.000671\n",
       "384     tfidf_384  0.000667\n",
       "27       tfidf_27  0.000662\n",
       "364     tfidf_364  0.000661\n",
       "259     tfidf_259  0.000657\n",
       "379     tfidf_379  0.000641\n",
       "109     tfidf_109  0.000640\n",
       "89       tfidf_89  0.000639\n",
       "8         tfidf_8  0.000638\n",
       "164     tfidf_164  0.000635\n",
       "382     tfidf_382  0.000617\n",
       "272     tfidf_272  0.000613\n",
       "67       tfidf_67  0.000612\n",
       "358     tfidf_358  0.000610\n",
       "423       apricot  0.000604\n",
       "216     tfidf_216  0.000600\n",
       "325     tfidf_325  0.000593\n",
       "40       tfidf_40  0.000592\n",
       "12       tfidf_12  0.000591\n",
       "28       tfidf_28  0.000590\n",
       "317     tfidf_317  0.000589\n",
       "424         berry  0.000587\n",
       "287     tfidf_287  0.000582\n",
       "81       tfidf_81  0.000569\n",
       "59       tfidf_59  0.000566\n",
       "63       tfidf_63  0.000563\n",
       "383     tfidf_383  0.000560\n",
       "134     tfidf_134  0.000556\n",
       "64       tfidf_64  0.000553\n",
       "135     tfidf_135  0.000551\n",
       "42       tfidf_42  0.000550\n",
       "129     tfidf_129  0.000549\n",
       "110     tfidf_110  0.000538\n",
       "264     tfidf_264  0.000537\n",
       "224     tfidf_224  0.000537\n",
       "187     tfidf_187  0.000528\n",
       "140     tfidf_140  0.000519\n",
       "262     tfidf_262  0.000519\n",
       "114     tfidf_114  0.000510\n",
       "127     tfidf_127  0.000505\n",
       "244     tfidf_244  0.000503\n",
       "304     tfidf_304  0.000499\n",
       "268     tfidf_268  0.000497\n",
       "47       tfidf_47  0.000491\n",
       "280     tfidf_280  0.000480\n",
       "108     tfidf_108  0.000478\n",
       "238     tfidf_238  0.000471\n",
       "248     tfidf_248  0.000470\n",
       "155     tfidf_155  0.000462\n",
       "328     tfidf_328  0.000458\n",
       "36       tfidf_36  0.000456\n",
       "75       tfidf_75  0.000455\n",
       "270     tfidf_270  0.000454\n",
       "397         dizzy  0.000438\n",
       "412      paranoid  0.000437\n",
       "468         woody  0.000437\n",
       "52       tfidf_52  0.000436\n",
       "195     tfidf_195  0.000428\n",
       "462           tea  0.000420\n",
       "83       tfidf_83  0.000417\n",
       "201     tfidf_201  0.000410\n",
       "61       tfidf_61  0.000408\n",
       "235     tfidf_235  0.000397\n",
       "132     tfidf_132  0.000394\n",
       "453          plum  0.000390\n",
       "465      tropical  0.000390\n",
       "227     tfidf_227  0.000389\n",
       "310     tfidf_310  0.000376\n",
       "202     tfidf_202  0.000374\n",
       "177     tfidf_177  0.000372\n",
       "386     tfidf_386  0.000370\n",
       "6         tfidf_6  0.000370\n",
       "95       tfidf_95  0.000369\n",
       "213     tfidf_213  0.000364\n",
       "76       tfidf_76  0.000359\n",
       "192     tfidf_192  0.000352\n",
       "91       tfidf_91  0.000345\n",
       "131     tfidf_131  0.000340\n",
       "66       tfidf_66  0.000339\n",
       "305     tfidf_305  0.000339\n",
       "84       tfidf_84  0.000339\n",
       "96       tfidf_96  0.000339\n",
       "106     tfidf_106  0.000334\n",
       "292     tfidf_292  0.000330\n",
       "0         tfidf_0  0.000329\n",
       "274     tfidf_274  0.000326\n",
       "176     tfidf_176  0.000308\n",
       "80       tfidf_80  0.000303\n",
       "226     tfidf_226  0.000298\n",
       "282     tfidf_282  0.000294\n",
       "15       tfidf_15  0.000293\n",
       "348     tfidf_348  0.000288\n",
       "387     tfidf_387  0.000285\n",
       "51       tfidf_51  0.000283\n",
       "252     tfidf_252  0.000275\n",
       "2         tfidf_2  0.000272\n",
       "271     tfidf_271  0.000263\n",
       "439         honey  0.000263\n",
       "316     tfidf_316  0.000260\n",
       "372     tfidf_372  0.000257\n",
       "365     tfidf_365  0.000255\n",
       "269     tfidf_269  0.000246\n",
       "436         fruit  0.000239\n",
       "77       tfidf_77  0.000239\n",
       "94       tfidf_94  0.000236\n",
       "242     tfidf_242  0.000235\n",
       "324     tfidf_324  0.000235\n",
       "408      headache  0.000232\n",
       "369     tfidf_369  0.000231\n",
       "191     tfidf_191  0.000228\n",
       "18       tfidf_18  0.000223\n",
       "74       tfidf_74  0.000221\n",
       "302     tfidf_302  0.000206\n",
       "446         nutty  0.000205\n",
       "44       tfidf_44  0.000204\n",
       "138     tfidf_138  0.000203\n",
       "183     tfidf_183  0.000199\n",
       "422         apple  0.000198\n",
       "323     tfidf_323  0.000196\n",
       "143     tfidf_143  0.000196\n",
       "100     tfidf_100  0.000194\n",
       "113     tfidf_113  0.000188\n",
       "308     tfidf_308  0.000173\n",
       "464          tree  0.000159\n",
       "351     tfidf_351  0.000151\n",
       "38       tfidf_38  0.000151\n",
       "349     tfidf_349  0.000147\n",
       "254     tfidf_254  0.000147\n",
       "266     tfidf_266  0.000146\n",
       "223     tfidf_223  0.000144\n",
       "161     tfidf_161  0.000144\n",
       "463       tobacco  0.000144\n",
       "82       tfidf_82  0.000138\n",
       "456          sage  0.000138\n",
       "62       tfidf_62  0.000135\n",
       "466       vanilla  0.000124\n",
       "430      chestnut  0.000121\n",
       "296     tfidf_296  0.000121\n",
       "450        pepper  0.000121\n",
       "55       tfidf_55  0.000120\n",
       "250     tfidf_250  0.000120\n",
       "339     tfidf_339  0.000118\n",
       "467        violet  0.000115\n",
       "444       menthol  0.000115\n",
       "421       ammonia  0.000113\n",
       "218     tfidf_218  0.000112\n",
       "442          lime  0.000101\n",
       "301     tfidf_301  0.000101\n",
       "174     tfidf_174  0.000098\n",
       "208     tfidf_208  0.000094\n",
       "219     tfidf_219  0.000092\n",
       "300     tfidf_300  0.000088\n",
       "333     tfidf_333  0.000081\n",
       "133     tfidf_133  0.000080\n",
       "359     tfidf_359  0.000073\n",
       "57       tfidf_57  0.000066\n",
       "335     tfidf_335  0.000049\n",
       "72       tfidf_72  0.000048\n",
       "327     tfidf_327  0.000046\n",
       "449          pear  0.000045\n",
       "448         peach  0.000042\n",
       "425   blue cheese  0.000026\n",
       "461           tar  0.000024\n",
       "455          rose  0.000024\n",
       "432        coffee  0.000022\n",
       "396    depression  0.000010\n",
       "427        butter  0.000008\n",
       "417        stress  0.000007\n",
       "391       anxiety  0.000006\n",
       "410     migraines  0.000006\n",
       "390        sativa  0.000000\n",
       "389        indica  0.000000\n",
       "394     arthritis  0.000000\n",
       "403  eye pressure  0.000000\n",
       "404       fatigue  0.000000\n",
       "411          pain  0.000000\n",
       "414      seizures  0.000000\n",
       "416    spasticity  0.000000\n",
       "401      epilepsy  0.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', df_feat_ranked.shape[0]+1)\n",
    "df_feat_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py:357: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "selector = SelectFromModel(rfreg).fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.08018386e-04, 8.79697992e-04, 2.32344385e-04, 2.59218747e-03,\n",
       "       1.60572450e-03, 9.01625791e-04, 3.50236328e-04, 2.99486742e-03,\n",
       "       6.14599230e-04, 1.20652616e-03, 1.11298580e-03, 1.24782910e-03,\n",
       "       4.69875507e-04, 1.32987688e-03, 1.09352229e-03, 3.53478586e-04,\n",
       "       6.72945926e-04, 1.63439633e-03, 1.84143995e-04, 1.31774687e-03,\n",
       "       3.58053505e-03, 1.26557254e-03, 2.56014013e-03, 2.46723285e-03,\n",
       "       1.23179739e-03, 8.65412283e-04, 5.87914158e-04, 6.58872306e-04,\n",
       "       6.06657740e-04, 3.80941867e-03, 2.31982516e-03, 1.42844139e-03,\n",
       "       9.00365720e-04, 2.16386557e-03, 2.44239288e-03, 8.56763508e-04,\n",
       "       3.52312810e-04, 7.98300905e-03, 1.37674057e-04, 8.29117457e-04,\n",
       "       3.80942083e-04, 2.35086807e-03, 4.49842751e-04, 2.76893485e-03,\n",
       "       2.46459200e-04, 1.85925308e-03, 1.03312454e-03, 5.59741935e-04,\n",
       "       1.77156811e-03, 7.29480732e-04, 1.10647139e-03, 3.01403600e-04,\n",
       "       3.15190609e-04, 3.52865159e-03, 1.88566869e-03, 1.10250623e-04,\n",
       "       2.53924377e-03, 6.85704465e-05, 2.20409950e-03, 6.85966820e-04,\n",
       "       7.53316916e-04, 3.73502097e-04, 2.09828755e-04, 6.72756723e-04,\n",
       "       4.14702438e-04, 9.46136925e-04, 2.74599927e-04, 5.06744153e-04,\n",
       "       7.78207198e-04, 2.93277241e-03, 7.40934145e-04, 1.80880803e-03,\n",
       "       8.56276666e-05, 1.44623305e-03, 2.55580604e-04, 3.88919754e-04,\n",
       "       3.12575281e-04, 3.00062793e-04, 2.01973125e-03, 1.47237322e-03,\n",
       "       2.72283167e-04, 7.88025023e-04, 2.74588740e-04, 3.70968642e-04,\n",
       "       3.81997087e-04, 1.37604641e-03, 1.40926270e-03, 7.42456522e-04,\n",
       "       1.35010956e-03, 4.45599288e-04, 3.69928605e-03, 3.82349030e-04,\n",
       "       7.03970824e-04, 6.11975226e-03, 2.48452943e-04, 4.05694478e-04,\n",
       "       3.36658995e-04, 1.20838130e-03, 8.05791461e-04, 9.35760320e-04,\n",
       "       1.83363712e-04, 1.46676479e-03, 3.46874702e-03, 6.27453379e-04,\n",
       "       1.30912140e-03, 2.37622975e-03, 3.38798644e-04, 1.23539944e-03,\n",
       "       5.29143713e-04, 7.83267124e-04, 4.52340650e-04, 2.79805025e-03,\n",
       "       4.88138377e-04, 1.84132628e-04, 3.36826600e-04, 9.44314487e-04,\n",
       "       1.86412737e-03, 1.32727496e-03, 3.65802279e-03, 2.80820795e-03,\n",
       "       8.06898025e-04, 2.65644429e-03, 9.53629330e-04, 1.57862351e-03,\n",
       "       1.17534719e-03, 8.92851194e-04, 7.77678771e-04, 5.50122686e-04,\n",
       "       4.63471615e-03, 8.11949546e-04, 9.16452111e-04, 4.11903813e-04,\n",
       "       3.63943236e-04, 4.36919721e-05, 4.47225066e-04, 4.70763944e-04,\n",
       "       1.02726253e-03, 1.37397548e-03, 3.51112188e-04, 1.50858894e-03,\n",
       "       5.83402045e-04, 1.78893522e-03, 9.47572344e-04, 2.12318940e-04,\n",
       "       2.56789794e-03, 6.03081025e-03, 2.07300019e-03, 1.72246747e-03,\n",
       "       2.98750662e-03, 7.32240090e-03, 1.00375590e-03, 1.46101663e-03,\n",
       "       3.80844969e-03, 2.06100554e-03, 6.93879086e-03, 4.33290706e-04,\n",
       "       5.57433771e-04, 7.73590728e-04, 1.50473354e-03, 9.01009278e-04,\n",
       "       1.10805942e-03, 9.09951012e-05, 2.28497731e-03, 2.67582285e-03,\n",
       "       6.79444746e-04, 2.40989496e-03, 1.28827424e-03, 1.67696315e-03,\n",
       "       5.60001648e-03, 1.03163272e-03, 1.07141990e-03, 9.19144305e-04,\n",
       "       6.00434792e-04, 1.05011101e-03, 5.27896510e-05, 5.40925590e-03,\n",
       "       3.74493619e-04, 2.95698725e-04, 1.38382761e-03, 1.57630766e-03,\n",
       "       2.44734137e-03, 2.35466229e-03, 1.20052073e-03, 1.07672315e-04,\n",
       "       1.50515004e-03, 8.99700146e-04, 3.95555425e-03, 5.61071428e-04,\n",
       "       3.14111446e-03, 1.11079072e-03, 1.73212980e-03, 1.45739975e-04,\n",
       "       2.22374849e-04, 1.65521807e-03, 1.54519345e-03, 3.76854557e-04,\n",
       "       2.31643461e-03, 8.56852474e-04, 1.68937233e-03, 4.40713805e-03,\n",
       "       3.01754469e-03, 3.59290112e-04, 3.33798934e-04, 2.52130651e-03,\n",
       "       2.31846263e-03, 1.98690097e-03, 1.99199005e-03, 3.23019838e-03,\n",
       "       1.16367040e-04, 1.95746433e-03, 1.76581019e-03, 1.52610316e-03,\n",
       "       9.30420676e-04, 3.64604858e-04, 6.43626381e-04, 9.92330066e-04,\n",
       "       7.53049667e-04, 1.37882990e-03, 6.97956150e-05, 3.48052229e-05,\n",
       "       7.20638209e-04, 5.87707868e-04, 1.26564676e-03, 1.27477409e-04,\n",
       "       4.38957725e-04, 6.80373861e-04, 2.41229926e-04, 3.05913380e-04,\n",
       "       7.97387614e-04, 1.12291276e-03, 1.54206627e-03, 1.38229076e-03,\n",
       "       1.53173954e-03, 7.95884502e-04, 6.97422717e-04, 3.46477205e-04,\n",
       "       7.08418181e-04, 1.97544058e-03, 3.86595370e-04, 5.67026629e-03,\n",
       "       1.66463973e-03, 7.00601247e-04, 1.28526883e-04, 6.58081779e-04,\n",
       "       3.45251979e-04, 2.40255818e-03, 1.16655605e-03, 1.10911081e-03,\n",
       "       4.46521348e-04, 1.20405914e-03, 7.66680715e-05, 3.85030489e-03,\n",
       "       3.09615320e-04, 7.27409276e-03, 1.44183515e-04, 8.69407513e-04,\n",
       "       8.41235603e-04, 1.58704515e-03, 2.65955823e-03, 6.52793941e-04,\n",
       "       1.40529818e-03, 1.86864264e-03, 3.95703153e-04, 4.43842497e-03,\n",
       "       5.69021363e-04, 1.28007934e-03, 1.04180517e-04, 2.04261192e-03,\n",
       "       5.67581708e-04, 2.06167044e-04, 4.03809945e-04, 2.70409742e-04,\n",
       "       6.02930404e-04, 2.36184575e-03, 3.37634858e-04, 1.16212120e-03,\n",
       "       8.58705864e-04, 9.38291595e-04, 2.93242540e-03, 1.06896010e-03,\n",
       "       4.42396050e-04, 2.60806473e-03, 3.57864374e-04, 2.13417464e-03,\n",
       "       6.56252739e-04, 2.71612224e-03, 7.43186977e-03, 4.66527786e-04,\n",
       "       9.93989856e-04, 1.01005973e-03, 8.67393310e-04, 7.83852128e-04,\n",
       "       3.81524108e-04, 7.50467304e-04, 2.38281096e-03, 1.16331339e-03,\n",
       "       1.40135242e-04, 2.31312110e-03, 1.81754295e-03, 1.51939280e-03,\n",
       "       1.57523834e-04, 1.19736945e-04, 2.18589969e-04, 4.51940259e-03,\n",
       "       6.26687700e-04, 3.54105301e-04, 1.33183226e-02, 4.85611952e-04,\n",
       "       1.95969172e-04, 6.36468076e-03, 3.36878375e-04, 2.22153323e-03,\n",
       "       6.56356473e-03, 7.49229653e-04, 1.86750670e-03, 2.10966120e-03,\n",
       "       3.05088482e-04, 5.66652836e-04, 9.63480014e-04, 2.41239201e-03,\n",
       "       1.15846591e-03, 1.92236930e-03, 7.41632798e-04, 2.31389423e-04,\n",
       "       3.02129110e-04, 6.33677255e-04, 2.36699143e-03, 4.43647996e-05,\n",
       "       4.07465223e-04, 9.97403519e-03, 2.04895078e-03, 2.33563529e-03,\n",
       "       1.68083284e-03, 2.32294934e-04, 8.81230737e-04, 5.02279216e-05,\n",
       "       8.30057211e-04, 2.55361089e-03, 4.45405950e-03, 1.78272433e-04,\n",
       "       1.80853187e-03, 9.40995531e-04, 1.06715523e-02, 2.47272685e-03,\n",
       "       9.10889832e-04, 4.78092428e-03, 1.15003385e-03, 8.60277664e-04,\n",
       "       2.96952283e-04, 1.09590895e-04, 1.18293882e-03, 2.00870527e-04,\n",
       "       1.02915457e-03, 1.34850895e-03, 2.20829775e-03, 1.26887268e-03,\n",
       "       6.84294278e-04, 2.51553511e-03, 6.47017347e-04, 6.89954047e-05,\n",
       "       6.79843939e-04, 3.18421805e-03, 2.51037476e-03, 1.68810488e-03,\n",
       "       7.71809149e-04, 2.51613650e-04, 2.54924058e-03, 3.87513178e-03,\n",
       "       2.03582356e-03, 1.54950825e-04, 5.44971603e-04, 7.70136605e-04,\n",
       "       2.56827798e-04, 6.79867473e-04, 8.96808325e-04, 1.14090898e-03,\n",
       "       6.79390374e-04, 3.85644814e-03, 6.63980714e-04, 5.64365748e-04,\n",
       "       3.59424895e-03, 2.23737587e-03, 6.55795771e-04, 7.63643511e-04,\n",
       "       6.54825021e-04, 6.64236562e-04, 3.05119852e-04, 2.18865719e-04,\n",
       "       2.96416465e-01, 0.00000000e+00, 0.00000000e+00, 1.43962526e-05,\n",
       "       7.63793917e-04, 5.19567160e-03, 0.00000000e+00, 3.45283116e-03,\n",
       "       2.51098792e-05, 4.38210719e-04, 8.20366516e-04, 2.10383508e-03,\n",
       "       1.41098148e-03, 0.00000000e+00, 2.23014905e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.33665730e-03, 4.05915904e-03, 1.90505334e-03,\n",
       "       2.36706909e-04, 1.45053326e-03, 1.41666490e-05, 0.00000000e+00,\n",
       "       3.73518677e-04, 3.11651274e-03, 0.00000000e+00, 1.08928278e-03,\n",
       "       0.00000000e+00, 1.25920633e-06, 1.75375253e-03, 1.57967739e-03,\n",
       "       1.50528119e-03, 1.56451891e-04, 1.42237443e-04, 7.45103102e-04,\n",
       "       6.76849528e-04, 1.32036387e-06, 3.58622351e-03, 5.70327996e-05,\n",
       "       1.25494745e-02, 2.43023730e-03, 1.66733868e-04, 6.17601649e-03,\n",
       "       8.07779915e-06, 1.96849748e-02, 7.65251768e-04, 1.00822955e-02,\n",
       "       1.41329235e-04, 3.48725990e-03, 6.25603756e-04, 2.93016941e-04,\n",
       "       1.42569382e-03, 5.86629161e-03, 1.35544777e-04, 4.80509006e-03,\n",
       "       2.25349231e-04, 2.94941904e-03, 8.18367010e-05, 1.21032327e-02,\n",
       "       1.82708432e-05, 5.02473046e-05, 7.67486765e-05, 9.43672457e-04,\n",
       "       6.08007946e-04, 3.55576092e-04, 6.21598684e-04, 3.30153571e-05,\n",
       "       1.32591673e-04, 2.00652750e-03, 3.64108754e-03, 6.81300567e-04,\n",
       "       7.74186478e-03, 2.77968015e-05, 5.93183870e-04, 1.49221449e-04,\n",
       "       2.81817013e-04, 3.79937580e-04, 5.88763695e-05, 1.09490978e-04,\n",
       "       3.58293489e-04])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021321961620469083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True,  True, False, False, False,\n",
       "       False, False,  True,  True, False, False,  True,  True, False,\n",
       "       False,  True, False, False, False,  True, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False,  True, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False,  True,  True, False,  True, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "        True,  True, False,  True, False, False,  True, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False,  True,  True, False, False,  True,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "       False, False,  True, False,  True, False,  True,  True, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False,  True, False, False,\n",
       "        True, False, False,  True, False,  True,  True, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False,  True, False, False,  True, False,  True, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "        True,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "       False,  True,  True, False, False, False,  True,  True, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False,  True, False,  True,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True,  True, False,  True,\n",
       "       False,  True, False,  True, False,  True, False, False, False,\n",
       "        True, False,  True, False,  True, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_20</th>\n",
       "      <th>tfidf_22</th>\n",
       "      <th>tfidf_23</th>\n",
       "      <th>tfidf_29</th>\n",
       "      <th>tfidf_30</th>\n",
       "      <th>tfidf_33</th>\n",
       "      <th>tfidf_34</th>\n",
       "      <th>tfidf_37</th>\n",
       "      <th>...</th>\n",
       "      <th>citrus</th>\n",
       "      <th>diesel</th>\n",
       "      <th>flowery</th>\n",
       "      <th>grape</th>\n",
       "      <th>lemon</th>\n",
       "      <th>mango</th>\n",
       "      <th>mint</th>\n",
       "      <th>orange</th>\n",
       "      <th>spicy/herbal</th>\n",
       "      <th>sweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194586</td>\n",
       "      <td>0.15591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194586</td>\n",
       "      <td>0.15591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194586</td>\n",
       "      <td>0.15591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.098314</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tfidf_3   tfidf_7  tfidf_20  tfidf_22  tfidf_23  tfidf_29  tfidf_30  \\\n",
       "0          0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "1          0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "2          0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "3          0.0  0.000000  0.280306       0.0       0.0       0.0  0.000000   \n",
       "4          0.0  0.000000  0.138272       0.0       0.0       0.0  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "49995      0.0  0.261458  0.000000       0.0       0.0       0.0  0.256018   \n",
       "49996      0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "49997      0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "49998      0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "49999      0.0  0.000000  0.000000       0.0       0.0       0.0  0.000000   \n",
       "\n",
       "       tfidf_33  tfidf_34  tfidf_37  ...  citrus  diesel  flowery  grape  \\\n",
       "0      0.194586   0.15591  0.000000  ...       0       0        0      1   \n",
       "1      0.194586   0.15591  0.000000  ...       0       0        0      1   \n",
       "2      0.194586   0.15591  0.000000  ...       0       0        0      1   \n",
       "3      0.000000   0.00000  0.000000  ...       0       0        0      0   \n",
       "4      0.000000   0.00000  0.098314  ...       0       0        0      0   \n",
       "...         ...       ...       ...  ...     ...     ...      ...    ...   \n",
       "49995  0.000000   0.00000  0.000000  ...       0       0        0      0   \n",
       "49996  0.000000   0.00000  0.000000  ...       0       0        0      0   \n",
       "49997  0.000000   0.00000  0.000000  ...       0       0        0      0   \n",
       "49998  0.000000   0.00000  0.000000  ...       0       0        0      0   \n",
       "49999  0.000000   0.00000  0.000000  ...       0       0        0      0   \n",
       "\n",
       "       lemon  mango  mint  orange  spicy/herbal  sweet  \n",
       "0          0      0     0       0             0      0  \n",
       "1          0      0     0       0             0      0  \n",
       "2          0      0     0       0             0      0  \n",
       "3          0      0     0       0             0      0  \n",
       "4          0      0     0       0             1      0  \n",
       "...      ...    ...   ...     ...           ...    ...  \n",
       "49995      0      0     0       0             0      0  \n",
       "49996      0      0     0       0             0      0  \n",
       "49997      0      0     0       0             0      0  \n",
       "49998      0      0     0       0             0      0  \n",
       "49999      0      0     0       0             0      0  \n",
       "\n",
       "[50000 rows x 102 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_X = df_rf[selected_features]\n",
    "selected_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split (after Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selected_X_rf_tfidf_isopul.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(selector, \"selector_rf_tfidf_isopul.pkl\")\n",
    "joblib.dump(selected_X, \"selected_X_rf_tfidf_isopul.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "selected_X = joblib.load(\"selected_X_rf_tfidf_isopul.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selected_X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_11510/3758305.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    }
   ],
   "source": [
    "rfreg.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023440166954937645"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009238230783862683"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09611571559252255"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981948167088853"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630467974059547"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [None, 10, 50, 100],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [100, 300, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(rfreg,  \n",
    "                     parameters,   \n",
    "                     cv=5, \n",
    "                     scoring='neg_mean_absolute_error',\n",
    "                     n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [None, 10, 50, 100],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 300, 500]},\n",
       "                   scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 300,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rscv_rf_tfidf_best_params_isopul.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rscv, \"rscv_rf_tfidf_isopul.pkl\")\n",
    "joblib.dump(rscv.best_params_, \"rscv_rf_tfidf_best_params_isopul.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF (after Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_11510/1652105768.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg_ht.fit(X_train1, y_train1)\n"
     ]
    }
   ],
   "source": [
    "rfreg_ht = RandomForestRegressor(n_estimators = 300, min_samples_split = 5, min_samples_leaf = 1, max_features = 'sqrt', max_depth = None)\n",
    "rfreg_ht.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg_ht.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg_ht.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022077654174706456"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006037630358403213"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07770219017764694"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835891956038394"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758492959266876"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots after Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg_test = rfreg_ht.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_rfreg_tfidf_isopul.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_pred_rfreg_test, \"y_pred_rfreg_test_tfidf_isopul.pkl\")\n",
    "joblib.dump(y_test, \"y_test_rfreg_tfidf_isopul.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022870750044777767"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006552783017944336"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08094926199752742"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9737835816496265"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assume y_pred is a numpy array and y_true is a pandas dataframe\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "column = \"X..Isopulegol\"  # specify the target variable name\n",
    "ax.hist(y_pred_rfreg_test, alpha=0.5, label='y_pred', bins=20)\n",
    "ax.hist(y_test[column], alpha=0.5, label='y_true', bins=20)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(column)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('error_hist_knn_tfidf_isopul.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.988\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "corr_coef, p_value = pearsonr(y_pred_rfreg_test.flatten(), y_test.values.ravel())\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr_coef:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix = y_test.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
