{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = pd.read_csv(\"df_euca_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'tfidf_0',\n",
       " 'tfidf_1',\n",
       " 'tfidf_2',\n",
       " 'tfidf_3',\n",
       " 'tfidf_4',\n",
       " 'tfidf_5',\n",
       " 'tfidf_6',\n",
       " 'tfidf_7',\n",
       " 'tfidf_8',\n",
       " 'tfidf_9',\n",
       " 'tfidf_10',\n",
       " 'tfidf_11',\n",
       " 'tfidf_12',\n",
       " 'tfidf_13',\n",
       " 'tfidf_14',\n",
       " 'tfidf_15',\n",
       " 'tfidf_16',\n",
       " 'tfidf_17',\n",
       " 'tfidf_18',\n",
       " 'tfidf_19',\n",
       " 'tfidf_20',\n",
       " 'tfidf_21',\n",
       " 'tfidf_22',\n",
       " 'tfidf_23',\n",
       " 'tfidf_24',\n",
       " 'tfidf_25',\n",
       " 'tfidf_26',\n",
       " 'tfidf_27',\n",
       " 'tfidf_28',\n",
       " 'tfidf_29',\n",
       " 'tfidf_30',\n",
       " 'tfidf_31',\n",
       " 'tfidf_32',\n",
       " 'tfidf_33',\n",
       " 'tfidf_34',\n",
       " 'tfidf_35',\n",
       " 'tfidf_36',\n",
       " 'tfidf_37',\n",
       " 'tfidf_38',\n",
       " 'tfidf_39',\n",
       " 'tfidf_40',\n",
       " 'tfidf_41',\n",
       " 'tfidf_42',\n",
       " 'tfidf_43',\n",
       " 'tfidf_44',\n",
       " 'tfidf_45',\n",
       " 'tfidf_46',\n",
       " 'tfidf_47',\n",
       " 'tfidf_48',\n",
       " 'tfidf_49',\n",
       " 'tfidf_50',\n",
       " 'tfidf_51',\n",
       " 'tfidf_52',\n",
       " 'tfidf_53',\n",
       " 'tfidf_54',\n",
       " 'tfidf_55',\n",
       " 'tfidf_56',\n",
       " 'tfidf_57',\n",
       " 'tfidf_58',\n",
       " 'tfidf_59',\n",
       " 'tfidf_60',\n",
       " 'tfidf_61',\n",
       " 'tfidf_62',\n",
       " 'tfidf_63',\n",
       " 'tfidf_64',\n",
       " 'tfidf_65',\n",
       " 'tfidf_66',\n",
       " 'tfidf_67',\n",
       " 'tfidf_68',\n",
       " 'tfidf_69',\n",
       " 'tfidf_70',\n",
       " 'tfidf_71',\n",
       " 'tfidf_72',\n",
       " 'tfidf_73',\n",
       " 'tfidf_74',\n",
       " 'tfidf_75',\n",
       " 'tfidf_76',\n",
       " 'tfidf_77',\n",
       " 'tfidf_78',\n",
       " 'tfidf_79',\n",
       " 'tfidf_80',\n",
       " 'tfidf_81',\n",
       " 'tfidf_82',\n",
       " 'tfidf_83',\n",
       " 'tfidf_84',\n",
       " 'tfidf_85',\n",
       " 'tfidf_86',\n",
       " 'tfidf_87',\n",
       " 'tfidf_88',\n",
       " 'tfidf_89',\n",
       " 'tfidf_90',\n",
       " 'tfidf_91',\n",
       " 'tfidf_92',\n",
       " 'tfidf_93',\n",
       " 'tfidf_94',\n",
       " 'tfidf_95',\n",
       " 'tfidf_96',\n",
       " 'tfidf_97',\n",
       " 'tfidf_98',\n",
       " 'tfidf_99',\n",
       " 'tfidf_100',\n",
       " 'tfidf_101',\n",
       " 'tfidf_102',\n",
       " 'tfidf_103',\n",
       " 'tfidf_104',\n",
       " 'tfidf_105',\n",
       " 'tfidf_106',\n",
       " 'tfidf_107',\n",
       " 'tfidf_108',\n",
       " 'tfidf_109',\n",
       " 'tfidf_110',\n",
       " 'tfidf_111',\n",
       " 'tfidf_112',\n",
       " 'tfidf_113',\n",
       " 'tfidf_114',\n",
       " 'tfidf_115',\n",
       " 'tfidf_116',\n",
       " 'tfidf_117',\n",
       " 'tfidf_118',\n",
       " 'tfidf_119',\n",
       " 'tfidf_120',\n",
       " 'tfidf_121',\n",
       " 'tfidf_122',\n",
       " 'tfidf_123',\n",
       " 'tfidf_124',\n",
       " 'tfidf_125',\n",
       " 'tfidf_126',\n",
       " 'tfidf_127',\n",
       " 'tfidf_128',\n",
       " 'tfidf_129',\n",
       " 'tfidf_130',\n",
       " 'tfidf_131',\n",
       " 'tfidf_132',\n",
       " 'tfidf_133',\n",
       " 'tfidf_134',\n",
       " 'tfidf_135',\n",
       " 'tfidf_136',\n",
       " 'tfidf_137',\n",
       " 'tfidf_138',\n",
       " 'tfidf_139',\n",
       " 'tfidf_140',\n",
       " 'tfidf_141',\n",
       " 'tfidf_142',\n",
       " 'tfidf_143',\n",
       " 'tfidf_144',\n",
       " 'tfidf_145',\n",
       " 'tfidf_146',\n",
       " 'tfidf_147',\n",
       " 'tfidf_148',\n",
       " 'tfidf_149',\n",
       " 'tfidf_150',\n",
       " 'tfidf_151',\n",
       " 'tfidf_152',\n",
       " 'tfidf_153',\n",
       " 'tfidf_154',\n",
       " 'tfidf_155',\n",
       " 'tfidf_156',\n",
       " 'tfidf_157',\n",
       " 'tfidf_158',\n",
       " 'tfidf_159',\n",
       " 'tfidf_160',\n",
       " 'tfidf_161',\n",
       " 'tfidf_162',\n",
       " 'tfidf_163',\n",
       " 'tfidf_164',\n",
       " 'tfidf_165',\n",
       " 'tfidf_166',\n",
       " 'tfidf_167',\n",
       " 'tfidf_168',\n",
       " 'tfidf_169',\n",
       " 'tfidf_170',\n",
       " 'tfidf_171',\n",
       " 'tfidf_172',\n",
       " 'tfidf_173',\n",
       " 'tfidf_174',\n",
       " 'tfidf_175',\n",
       " 'tfidf_176',\n",
       " 'tfidf_177',\n",
       " 'tfidf_178',\n",
       " 'tfidf_179',\n",
       " 'tfidf_180',\n",
       " 'tfidf_181',\n",
       " 'tfidf_182',\n",
       " 'tfidf_183',\n",
       " 'tfidf_184',\n",
       " 'tfidf_185',\n",
       " 'tfidf_186',\n",
       " 'tfidf_187',\n",
       " 'tfidf_188',\n",
       " 'tfidf_189',\n",
       " 'tfidf_190',\n",
       " 'tfidf_191',\n",
       " 'tfidf_192',\n",
       " 'tfidf_193',\n",
       " 'tfidf_194',\n",
       " 'tfidf_195',\n",
       " 'tfidf_196',\n",
       " 'tfidf_197',\n",
       " 'tfidf_198',\n",
       " 'tfidf_199',\n",
       " 'tfidf_200',\n",
       " 'tfidf_201',\n",
       " 'tfidf_202',\n",
       " 'tfidf_203',\n",
       " 'tfidf_204',\n",
       " 'tfidf_205',\n",
       " 'tfidf_206',\n",
       " 'tfidf_207',\n",
       " 'tfidf_208',\n",
       " 'tfidf_209',\n",
       " 'tfidf_210',\n",
       " 'tfidf_211',\n",
       " 'tfidf_212',\n",
       " 'tfidf_213',\n",
       " 'tfidf_214',\n",
       " 'tfidf_215',\n",
       " 'tfidf_216',\n",
       " 'tfidf_217',\n",
       " 'tfidf_218',\n",
       " 'tfidf_219',\n",
       " 'tfidf_220',\n",
       " 'tfidf_221',\n",
       " 'tfidf_222',\n",
       " 'tfidf_223',\n",
       " 'tfidf_224',\n",
       " 'tfidf_225',\n",
       " 'tfidf_226',\n",
       " 'tfidf_227',\n",
       " 'tfidf_228',\n",
       " 'tfidf_229',\n",
       " 'tfidf_230',\n",
       " 'tfidf_231',\n",
       " 'tfidf_232',\n",
       " 'tfidf_233',\n",
       " 'tfidf_234',\n",
       " 'tfidf_235',\n",
       " 'tfidf_236',\n",
       " 'tfidf_237',\n",
       " 'tfidf_238',\n",
       " 'tfidf_239',\n",
       " 'tfidf_240',\n",
       " 'tfidf_241',\n",
       " 'tfidf_242',\n",
       " 'tfidf_243',\n",
       " 'tfidf_244',\n",
       " 'tfidf_245',\n",
       " 'tfidf_246',\n",
       " 'tfidf_247',\n",
       " 'tfidf_248',\n",
       " 'tfidf_249',\n",
       " 'tfidf_250',\n",
       " 'tfidf_251',\n",
       " 'tfidf_252',\n",
       " 'tfidf_253',\n",
       " 'tfidf_254',\n",
       " 'tfidf_255',\n",
       " 'tfidf_256',\n",
       " 'tfidf_257',\n",
       " 'tfidf_258',\n",
       " 'tfidf_259',\n",
       " 'tfidf_260',\n",
       " 'tfidf_261',\n",
       " 'tfidf_262',\n",
       " 'tfidf_263',\n",
       " 'tfidf_264',\n",
       " 'tfidf_265',\n",
       " 'tfidf_266',\n",
       " 'tfidf_267',\n",
       " 'tfidf_268',\n",
       " 'tfidf_269',\n",
       " 'tfidf_270',\n",
       " 'tfidf_271',\n",
       " 'tfidf_272',\n",
       " 'tfidf_273',\n",
       " 'tfidf_274',\n",
       " 'tfidf_275',\n",
       " 'tfidf_276',\n",
       " 'tfidf_277',\n",
       " 'tfidf_278',\n",
       " 'tfidf_279',\n",
       " 'tfidf_280',\n",
       " 'tfidf_281',\n",
       " 'tfidf_282',\n",
       " 'tfidf_283',\n",
       " 'tfidf_284',\n",
       " 'tfidf_285',\n",
       " 'tfidf_286',\n",
       " 'tfidf_287',\n",
       " 'tfidf_288',\n",
       " 'tfidf_289',\n",
       " 'tfidf_290',\n",
       " 'tfidf_291',\n",
       " 'tfidf_292',\n",
       " 'tfidf_293',\n",
       " 'tfidf_294',\n",
       " 'tfidf_295',\n",
       " 'tfidf_296',\n",
       " 'tfidf_297',\n",
       " 'tfidf_298',\n",
       " 'tfidf_299',\n",
       " 'tfidf_300',\n",
       " 'tfidf_301',\n",
       " 'tfidf_302',\n",
       " 'tfidf_303',\n",
       " 'tfidf_304',\n",
       " 'tfidf_305',\n",
       " 'tfidf_306',\n",
       " 'tfidf_307',\n",
       " 'tfidf_308',\n",
       " 'tfidf_309',\n",
       " 'tfidf_310',\n",
       " 'tfidf_311',\n",
       " 'tfidf_312',\n",
       " 'tfidf_313',\n",
       " 'tfidf_314',\n",
       " 'tfidf_315',\n",
       " 'tfidf_316',\n",
       " 'tfidf_317',\n",
       " 'tfidf_318',\n",
       " 'tfidf_319',\n",
       " 'tfidf_320',\n",
       " 'tfidf_321',\n",
       " 'tfidf_322',\n",
       " 'tfidf_323',\n",
       " 'tfidf_324',\n",
       " 'tfidf_325',\n",
       " 'tfidf_326',\n",
       " 'tfidf_327',\n",
       " 'tfidf_328',\n",
       " 'tfidf_329',\n",
       " 'tfidf_330',\n",
       " 'tfidf_331',\n",
       " 'tfidf_332',\n",
       " 'tfidf_333',\n",
       " 'tfidf_334',\n",
       " 'tfidf_335',\n",
       " 'tfidf_336',\n",
       " 'tfidf_337',\n",
       " 'tfidf_338',\n",
       " 'tfidf_339',\n",
       " 'tfidf_340',\n",
       " 'tfidf_341',\n",
       " 'tfidf_342',\n",
       " 'tfidf_343',\n",
       " 'tfidf_344',\n",
       " 'tfidf_345',\n",
       " 'tfidf_346',\n",
       " 'tfidf_347',\n",
       " 'tfidf_348',\n",
       " 'tfidf_349',\n",
       " 'tfidf_350',\n",
       " 'tfidf_351',\n",
       " 'tfidf_352',\n",
       " 'tfidf_353',\n",
       " 'tfidf_354',\n",
       " 'tfidf_355',\n",
       " 'tfidf_356',\n",
       " 'tfidf_357',\n",
       " 'tfidf_358',\n",
       " 'tfidf_359',\n",
       " 'tfidf_360',\n",
       " 'tfidf_361',\n",
       " 'tfidf_362',\n",
       " 'tfidf_363',\n",
       " 'tfidf_364',\n",
       " 'tfidf_365',\n",
       " 'tfidf_366',\n",
       " 'tfidf_367',\n",
       " 'tfidf_368',\n",
       " 'tfidf_369',\n",
       " 'tfidf_370',\n",
       " 'tfidf_371',\n",
       " 'tfidf_372',\n",
       " 'tfidf_373',\n",
       " 'tfidf_374',\n",
       " 'tfidf_375',\n",
       " 'tfidf_376',\n",
       " 'tfidf_377',\n",
       " 'tfidf_378',\n",
       " 'tfidf_379',\n",
       " 'tfidf_380',\n",
       " 'tfidf_381',\n",
       " 'tfidf_382',\n",
       " 'tfidf_383',\n",
       " 'tfidf_384',\n",
       " 'tfidf_385',\n",
       " 'tfidf_386',\n",
       " 'tfidf_387',\n",
       " 'hybrid',\n",
       " 'indica',\n",
       " 'sativa',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'aroused',\n",
       " 'arthritis',\n",
       " 'creative',\n",
       " 'depression',\n",
       " 'dizzy',\n",
       " 'dry eyes',\n",
       " 'dry mouth',\n",
       " 'energetic',\n",
       " 'epilepsy',\n",
       " 'euphoric',\n",
       " 'eye pressure',\n",
       " 'fatigue',\n",
       " 'focused',\n",
       " 'giggly',\n",
       " 'happy',\n",
       " 'headache',\n",
       " 'hungry',\n",
       " 'migraines',\n",
       " 'pain',\n",
       " 'paranoid',\n",
       " 'relaxed',\n",
       " 'seizures',\n",
       " 'sleepy',\n",
       " 'spasticity',\n",
       " 'stress',\n",
       " 'talkative',\n",
       " 'tingly',\n",
       " 'uplifted',\n",
       " 'ammonia',\n",
       " 'apple',\n",
       " 'apricot',\n",
       " 'berry',\n",
       " 'blue cheese',\n",
       " 'blueberry',\n",
       " 'butter',\n",
       " 'cheese',\n",
       " 'chemical',\n",
       " 'chestnut',\n",
       " 'citrus',\n",
       " 'coffee',\n",
       " 'diesel',\n",
       " 'earthy',\n",
       " 'flowery',\n",
       " 'fruit',\n",
       " 'grape',\n",
       " 'grapefruit',\n",
       " 'honey',\n",
       " 'lavender',\n",
       " 'lemon',\n",
       " 'lime',\n",
       " 'mango',\n",
       " 'menthol',\n",
       " 'mint',\n",
       " 'nutty',\n",
       " 'orange',\n",
       " 'peach',\n",
       " 'pear',\n",
       " 'pepper',\n",
       " 'pine',\n",
       " 'pineapple',\n",
       " 'plum',\n",
       " 'pungent',\n",
       " 'rose',\n",
       " 'sage',\n",
       " 'skunk',\n",
       " 'spicy/herbal',\n",
       " 'strawberry',\n",
       " 'sweet',\n",
       " 'tar',\n",
       " 'tea',\n",
       " 'tobacco',\n",
       " 'tree',\n",
       " 'tropical',\n",
       " 'vanilla',\n",
       " 'violet',\n",
       " 'woody',\n",
       " 'X..Eucalyptol']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_rf.drop(['index', 'X..Eucalyptol'], axis = 1)\n",
    "y = df_rf[['X..Eucalyptol']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting histograms on target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array = y.to_numpy()\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(y, bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF modeling (before Feature selection and Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_9602/350139188.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg = RandomForestRegressor()\n",
    "rfreg.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028400597217101618"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009778243005595223"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09888499889060637"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9840910151467912"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9229808716912571"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots for each target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame({\n",
    "    \"features\": X_train1.columns,\n",
    "    \"score\": rfreg.feature_importances_\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.001125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.001255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     features     score\n",
       "0     tfidf_0  0.001125\n",
       "1     tfidf_1  0.000681\n",
       "2     tfidf_2  0.001247\n",
       "3     tfidf_3  0.000293\n",
       "4     tfidf_4  0.000900\n",
       "..        ...       ...\n",
       "464      tree  0.000326\n",
       "465  tropical  0.001255\n",
       "466   vanilla  0.000145\n",
       "467    violet  0.000175\n",
       "468     woody  0.001779\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_ranked = df_feat.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>diesel</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>lemon</td>\n",
       "      <td>0.070346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>indica</td>\n",
       "      <td>0.067042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>0.047650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>tfidf_253</td>\n",
       "      <td>0.027234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>sativa</td>\n",
       "      <td>0.021555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>tfidf_329</td>\n",
       "      <td>0.013845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tfidf_11</td>\n",
       "      <td>0.012009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>tfidf_82</td>\n",
       "      <td>0.011263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>earthy</td>\n",
       "      <td>0.009342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>tfidf_168</td>\n",
       "      <td>0.009313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>tfidf_162</td>\n",
       "      <td>0.008232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>tfidf_145</td>\n",
       "      <td>0.008177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>tfidf_312</td>\n",
       "      <td>0.008138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>tfidf_285</td>\n",
       "      <td>0.007513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>tfidf_245</td>\n",
       "      <td>0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>strawberry</td>\n",
       "      <td>0.007237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>tfidf_345</td>\n",
       "      <td>0.006717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>tfidf_149</td>\n",
       "      <td>0.006058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>tfidf_199</td>\n",
       "      <td>0.006014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>euphoric</td>\n",
       "      <td>0.005429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tfidf_58</td>\n",
       "      <td>0.005225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>relaxed</td>\n",
       "      <td>0.005069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>sweet</td>\n",
       "      <td>0.005062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>tfidf_121</td>\n",
       "      <td>0.004954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>tfidf_239</td>\n",
       "      <td>0.004904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>tfidf_223</td>\n",
       "      <td>0.004660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.004639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>tfidf_119</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>tfidf_167</td>\n",
       "      <td>0.004530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>tfidf_210</td>\n",
       "      <td>0.004530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>tfidf_309</td>\n",
       "      <td>0.004377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>tfidf_207</td>\n",
       "      <td>0.004238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>blueberry</td>\n",
       "      <td>0.004223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>tfidf_124</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>tfidf_141</td>\n",
       "      <td>0.004176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>tfidf_128</td>\n",
       "      <td>0.004171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_43</td>\n",
       "      <td>0.004113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>tfidf_93</td>\n",
       "      <td>0.004058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>tfidf_158</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>orange</td>\n",
       "      <td>0.003890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>tfidf_281</td>\n",
       "      <td>0.003783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>energetic</td>\n",
       "      <td>0.003650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>citrus</td>\n",
       "      <td>0.003649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>tfidf_197</td>\n",
       "      <td>0.003579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>tfidf_230</td>\n",
       "      <td>0.003524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>uplifted</td>\n",
       "      <td>0.003411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>tfidf_357</td>\n",
       "      <td>0.003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>tfidf_376</td>\n",
       "      <td>0.003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>tfidf_362</td>\n",
       "      <td>0.003108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfidf_6</td>\n",
       "      <td>0.003099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>cheese</td>\n",
       "      <td>0.003080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>tfidf_144</td>\n",
       "      <td>0.003069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>tfidf_355</td>\n",
       "      <td>0.003046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>tfidf_151</td>\n",
       "      <td>0.002982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>dry mouth</td>\n",
       "      <td>0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tfidf_36</td>\n",
       "      <td>0.002759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>tfidf_200</td>\n",
       "      <td>0.002758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>tfidf_73</td>\n",
       "      <td>0.002705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>tfidf_325</td>\n",
       "      <td>0.002690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>tfidf_216</td>\n",
       "      <td>0.002669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>tfidf_220</td>\n",
       "      <td>0.002655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>grape</td>\n",
       "      <td>0.002622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tfidf_37</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>tfidf_166</td>\n",
       "      <td>0.002547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>tfidf_161</td>\n",
       "      <td>0.002529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>tfidf_337</td>\n",
       "      <td>0.002520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tfidf_35</td>\n",
       "      <td>0.002505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>tfidf_286</td>\n",
       "      <td>0.002495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tfidf_30</td>\n",
       "      <td>0.002479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>tfidf_290</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>tfidf_126</td>\n",
       "      <td>0.002409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>tingly</td>\n",
       "      <td>0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>tfidf_342</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>focused</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>tfidf_157</td>\n",
       "      <td>0.002263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>tfidf_251</td>\n",
       "      <td>0.002186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>tfidf_194</td>\n",
       "      <td>0.002174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_5</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>tfidf_272</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>tfidf_323</td>\n",
       "      <td>0.002113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tfidf_51</td>\n",
       "      <td>0.002085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tfidf_34</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>creative</td>\n",
       "      <td>0.002039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tfidf_45</td>\n",
       "      <td>0.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tfidf_20</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>pungent</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>giggly</td>\n",
       "      <td>0.001977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>tfidf_303</td>\n",
       "      <td>0.001974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>tfidf_340</td>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>tfidf_115</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>hungry</td>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>tfidf_78</td>\n",
       "      <td>0.001915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>tfidf_248</td>\n",
       "      <td>0.001890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tfidf_46</td>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>tfidf_270</td>\n",
       "      <td>0.001862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>tfidf_321</td>\n",
       "      <td>0.001860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>tfidf_123</td>\n",
       "      <td>0.001855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>tfidf_90</td>\n",
       "      <td>0.001853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>tfidf_291</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>tfidf_258</td>\n",
       "      <td>0.001841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tfidf_32</td>\n",
       "      <td>0.001809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>talkative</td>\n",
       "      <td>0.001806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>woody</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>tfidf_154</td>\n",
       "      <td>0.001769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>tfidf_235</td>\n",
       "      <td>0.001756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>tfidf_209</td>\n",
       "      <td>0.001754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_7</td>\n",
       "      <td>0.001753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>tfidf_283</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>tfidf_336</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>tfidf_372</td>\n",
       "      <td>0.001702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>tfidf_382</td>\n",
       "      <td>0.001688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>tfidf_366</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tfidf_19</td>\n",
       "      <td>0.001643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>tfidf_371</td>\n",
       "      <td>0.001632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tfidf_25</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>tfidf_96</td>\n",
       "      <td>0.001577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>tfidf_205</td>\n",
       "      <td>0.001563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>tfidf_367</td>\n",
       "      <td>0.001547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>tfidf_300</td>\n",
       "      <td>0.001533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>tfidf_173</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>sleepy</td>\n",
       "      <td>0.001524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>tfidf_249</td>\n",
       "      <td>0.001522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>spicy/herbal</td>\n",
       "      <td>0.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>tfidf_110</td>\n",
       "      <td>0.001494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>tfidf_61</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>tfidf_221</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>tfidf_277</td>\n",
       "      <td>0.001476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>tfidf_186</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tfidf_21</td>\n",
       "      <td>0.001448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>tfidf_314</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tfidf_9</td>\n",
       "      <td>0.001444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>tfidf_232</td>\n",
       "      <td>0.001437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>tfidf_338</td>\n",
       "      <td>0.001428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>tfidf_264</td>\n",
       "      <td>0.001428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>tfidf_319</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>tfidf_350</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>tfidf_150</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>pine</td>\n",
       "      <td>0.001357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>tfidf_225</td>\n",
       "      <td>0.001356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>tfidf_164</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tfidf_54</td>\n",
       "      <td>0.001341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>tfidf_217</td>\n",
       "      <td>0.001328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>tfidf_294</td>\n",
       "      <td>0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tfidf_99</td>\n",
       "      <td>0.001319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>tfidf_263</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>tfidf_353</td>\n",
       "      <td>0.001296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>tfidf_368</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tfidf_41</td>\n",
       "      <td>0.001268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>tfidf_198</td>\n",
       "      <td>0.001259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>tropical</td>\n",
       "      <td>0.001255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>tfidf_163</td>\n",
       "      <td>0.001252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>tfidf_369</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>tfidf_215</td>\n",
       "      <td>0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>tfidf_316</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>tfidf_278</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>tfidf_265</td>\n",
       "      <td>0.001203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tfidf_94</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>tfidf_343</td>\n",
       "      <td>0.001184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tfidf_208</td>\n",
       "      <td>0.001176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>tfidf_289</td>\n",
       "      <td>0.001169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tfidf_98</td>\n",
       "      <td>0.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>tfidf_101</td>\n",
       "      <td>0.001163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>tfidf_381</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tfidf_26</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>tfidf_81</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>tfidf_190</td>\n",
       "      <td>0.001152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>tfidf_117</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>tfidf_103</td>\n",
       "      <td>0.001136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>ammonia</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.001125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>tfidf_192</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>tfidf_178</td>\n",
       "      <td>0.001121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>tfidf_219</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>tfidf_243</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>tfidf_373</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>tfidf_240</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>tfidf_344</td>\n",
       "      <td>0.001087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>tfidf_183</td>\n",
       "      <td>0.001086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tfidf_38</td>\n",
       "      <td>0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>tfidf_88</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>tfidf_297</td>\n",
       "      <td>0.001060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>dry eyes</td>\n",
       "      <td>0.001059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>tfidf_75</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>flowery</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>tfidf_379</td>\n",
       "      <td>0.001039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tfidf_48</td>\n",
       "      <td>0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tfidf_17</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>tfidf_260</td>\n",
       "      <td>0.001027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>tfidf_385</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>tfidf_130</td>\n",
       "      <td>0.001014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>tfidf_237</td>\n",
       "      <td>0.001014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>tfidf_308</td>\n",
       "      <td>0.001014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>tfidf_328</td>\n",
       "      <td>0.001013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>chemical</td>\n",
       "      <td>0.001003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>tfidf_140</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>tfidf_246</td>\n",
       "      <td>0.000985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>tfidf_271</td>\n",
       "      <td>0.000980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>tfidf_206</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tfidf_52</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tfidf_53</td>\n",
       "      <td>0.000974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>tfidf_348</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tfidf_12</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>tfidf_107</td>\n",
       "      <td>0.000955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>tfidf_244</td>\n",
       "      <td>0.000953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>tfidf_267</td>\n",
       "      <td>0.000952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>tfidf_86</td>\n",
       "      <td>0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>tfidf_97</td>\n",
       "      <td>0.000933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>mango</td>\n",
       "      <td>0.000920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>tfidf_276</td>\n",
       "      <td>0.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>tfidf_360</td>\n",
       "      <td>0.000907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>tfidf_80</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>tfidf_305</td>\n",
       "      <td>0.000903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>lavender</td>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tfidf_106</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_4</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>tfidf_129</td>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>tfidf_255</td>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>tfidf_113</td>\n",
       "      <td>0.000881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tfidf_332</td>\n",
       "      <td>0.000881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>coffee</td>\n",
       "      <td>0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>tfidf_295</td>\n",
       "      <td>0.000874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>tfidf_374</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tfidf_69</td>\n",
       "      <td>0.000860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>tfidf_170</td>\n",
       "      <td>0.000851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tfidf_44</td>\n",
       "      <td>0.000848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>tfidf_104</td>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>tfidf_169</td>\n",
       "      <td>0.000842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>tfidf_135</td>\n",
       "      <td>0.000839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tfidf_22</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>tfidf_118</td>\n",
       "      <td>0.000814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>tfidf_83</td>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>tfidf_380</td>\n",
       "      <td>0.000805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>tfidf_59</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>tfidf_231</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>tfidf_125</td>\n",
       "      <td>0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>tfidf_266</td>\n",
       "      <td>0.000784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>aroused</td>\n",
       "      <td>0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>tfidf_361</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>tfidf_100</td>\n",
       "      <td>0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tfidf_175</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>tfidf_331</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>tfidf_363</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>tfidf_127</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>tfidf_334</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>tfidf_341</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>tfidf_326</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>lime</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>tfidf_386</td>\n",
       "      <td>0.000740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>tfidf_256</td>\n",
       "      <td>0.000739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>tar</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>tfidf_146</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>tfidf_152</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tfidf_15</td>\n",
       "      <td>0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>tfidf_211</td>\n",
       "      <td>0.000708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>skunk</td>\n",
       "      <td>0.000706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>tfidf_275</td>\n",
       "      <td>0.000704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>tfidf_387</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>tfidf_189</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>tfidf_193</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tfidf_8</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>tfidf_317</td>\n",
       "      <td>0.000688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tfidf_10</td>\n",
       "      <td>0.000683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>tfidf_299</td>\n",
       "      <td>0.000683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_1</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>tfidf_133</td>\n",
       "      <td>0.000676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>tfidf_224</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>dizzy</td>\n",
       "      <td>0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>headache</td>\n",
       "      <td>0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>tfidf_122</td>\n",
       "      <td>0.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>tfidf_85</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>tfidf_301</td>\n",
       "      <td>0.000652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>tfidf_356</td>\n",
       "      <td>0.000650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>tfidf_64</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>tfidf_298</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>tfidf_228</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tfidf_39</td>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>tfidf_156</td>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tfidf_28</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>paranoid</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>tfidf_203</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tfidf_56</td>\n",
       "      <td>0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>tfidf_238</td>\n",
       "      <td>0.000623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>tfidf_131</td>\n",
       "      <td>0.000622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>tfidf_132</td>\n",
       "      <td>0.000621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>tfidf_139</td>\n",
       "      <td>0.000613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tfidf_67</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>tfidf_137</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>mint</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>tfidf_346</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>tfidf_313</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>tfidf_318</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>tfidf_233</td>\n",
       "      <td>0.000598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>tfidf_109</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tfidf_29</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>tfidf_142</td>\n",
       "      <td>0.000588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>tfidf_257</td>\n",
       "      <td>0.000581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>tfidf_171</td>\n",
       "      <td>0.000581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>tfidf_288</td>\n",
       "      <td>0.000573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tfidf_50</td>\n",
       "      <td>0.000571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>tfidf_71</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>tfidf_120</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>tfidf_184</td>\n",
       "      <td>0.000565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>tfidf_364</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>tfidf_226</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>tfidf_159</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>tfidf_143</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>tfidf_112</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>tfidf_177</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>tfidf_234</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>tfidf_195</td>\n",
       "      <td>0.000521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>tfidf_160</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>tfidf_354</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>tfidf_352</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>tfidf_375</td>\n",
       "      <td>0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>tfidf_76</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tfidf_16</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>tfidf_185</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>tfidf_273</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>tfidf_165</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>tfidf_57</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>tfidf_322</td>\n",
       "      <td>0.000477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>tfidf_172</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>tfidf_247</td>\n",
       "      <td>0.000474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>tfidf_310</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>nutty</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>tfidf_63</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>sage</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tfidf_24</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>tfidf_147</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>pepper</td>\n",
       "      <td>0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>tfidf_148</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>tfidf_370</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>tfidf_351</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tfidf_27</td>\n",
       "      <td>0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>tfidf_74</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>tfidf_269</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>tfidf_268</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tfidf_31</td>\n",
       "      <td>0.000442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tfidf_47</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>tfidf_153</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>tfidf_214</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>tfidf_262</td>\n",
       "      <td>0.000425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>tfidf_254</td>\n",
       "      <td>0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tfidf_66</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>tfidf_84</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>tfidf_227</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>tfidf_320</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>tfidf_204</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>tfidf_213</td>\n",
       "      <td>0.000398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>tfidf_222</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>tfidf_111</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tfidf_33</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>tfidf_134</td>\n",
       "      <td>0.000376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tfidf_105</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>tfidf_182</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>tfidf_91</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>tfidf_304</td>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>tfidf_307</td>\n",
       "      <td>0.000367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>tfidf_384</td>\n",
       "      <td>0.000367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>tfidf_242</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>tfidf_365</td>\n",
       "      <td>0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>tfidf_116</td>\n",
       "      <td>0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>tfidf_287</td>\n",
       "      <td>0.000359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>tfidf_102</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tfidf_13</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tfidf_23</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>tfidf_138</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>tfidf_92</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>tfidf_180</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>tfidf_155</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>tfidf_293</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>fruit</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>tfidf_89</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>tfidf_274</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tfidf_79</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>tfidf_292</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>anxious</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>tfidf_259</td>\n",
       "      <td>0.000316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>tfidf_377</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>tfidf_108</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>tfidf_95</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tfidf_236</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>tfidf_179</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>tfidf_188</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tfidf_40</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tfidf_18</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>tfidf_174</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_3</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>tfidf_282</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>tfidf_201</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>tfidf_62</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>tfidf_202</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>tfidf_347</td>\n",
       "      <td>0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>tfidf_359</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>tfidf_284</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>tfidf_315</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>tfidf_335</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>tfidf_241</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>tfidf_87</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>tfidf_70</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tfidf_68</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>tfidf_330</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>tfidf_136</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>tfidf_77</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>tfidf_339</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tfidf_49</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>tfidf_212</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tfidf_65</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>tfidf_378</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>tfidf_327</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>tfidf_181</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>tfidf_176</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>tfidf_311</td>\n",
       "      <td>0.000228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>tfidf_280</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tfidf_55</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tfidf_302</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>chestnut</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>apricot</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>tfidf_296</td>\n",
       "      <td>0.000207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>tfidf_72</td>\n",
       "      <td>0.000205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>tfidf_306</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>tfidf_349</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tfidf_14</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>tea</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>rose</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>tfidf_252</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>tfidf_187</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>tfidf_196</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>tfidf_358</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>tfidf_229</td>\n",
       "      <td>0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>tfidf_261</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>peach</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>apple</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>tfidf_114</td>\n",
       "      <td>0.000146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>tfidf_218</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>tfidf_383</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tfidf_191</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>tfidf_333</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>grapefruit</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tfidf_42</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>tfidf_60</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>tfidf_250</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>honey</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>plum</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>pineapple</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>tfidf_324</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>butter</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>menthol</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>blue cheese</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>tfidf_279</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>migraines</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>depression</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>pear</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>eye pressure</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>arthritis</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>stress</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>spasticity</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>seizures</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>pain</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>epilepsy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>fatigue</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         features     score\n",
       "433        diesel  0.141800\n",
       "441         lemon  0.070346\n",
       "389        indica  0.067042\n",
       "388        hybrid  0.047650\n",
       "253     tfidf_253  0.027234\n",
       "390        sativa  0.021555\n",
       "329     tfidf_329  0.013845\n",
       "11       tfidf_11  0.012009\n",
       "82       tfidf_82  0.011263\n",
       "434        earthy  0.009342\n",
       "168     tfidf_168  0.009313\n",
       "162     tfidf_162  0.008232\n",
       "145     tfidf_145  0.008177\n",
       "312     tfidf_312  0.008138\n",
       "285     tfidf_285  0.007513\n",
       "245     tfidf_245  0.007254\n",
       "459    strawberry  0.007237\n",
       "345     tfidf_345  0.006717\n",
       "149     tfidf_149  0.006058\n",
       "199     tfidf_199  0.006014\n",
       "402      euphoric  0.005429\n",
       "58       tfidf_58  0.005225\n",
       "413       relaxed  0.005069\n",
       "460         sweet  0.005062\n",
       "121     tfidf_121  0.004954\n",
       "239     tfidf_239  0.004904\n",
       "223     tfidf_223  0.004660\n",
       "407         happy  0.004639\n",
       "119     tfidf_119  0.004541\n",
       "167     tfidf_167  0.004530\n",
       "210     tfidf_210  0.004530\n",
       "309     tfidf_309  0.004377\n",
       "207     tfidf_207  0.004238\n",
       "426     blueberry  0.004223\n",
       "124     tfidf_124  0.004188\n",
       "141     tfidf_141  0.004176\n",
       "128     tfidf_128  0.004171\n",
       "43       tfidf_43  0.004113\n",
       "93       tfidf_93  0.004058\n",
       "158     tfidf_158  0.004024\n",
       "447        orange  0.003890\n",
       "281     tfidf_281  0.003783\n",
       "400     energetic  0.003650\n",
       "431        citrus  0.003649\n",
       "197     tfidf_197  0.003579\n",
       "230     tfidf_230  0.003524\n",
       "420      uplifted  0.003411\n",
       "357     tfidf_357  0.003329\n",
       "376     tfidf_376  0.003276\n",
       "362     tfidf_362  0.003108\n",
       "6         tfidf_6  0.003099\n",
       "428        cheese  0.003080\n",
       "144     tfidf_144  0.003069\n",
       "355     tfidf_355  0.003046\n",
       "151     tfidf_151  0.002982\n",
       "399     dry mouth  0.002851\n",
       "36       tfidf_36  0.002759\n",
       "200     tfidf_200  0.002758\n",
       "73       tfidf_73  0.002705\n",
       "325     tfidf_325  0.002690\n",
       "216     tfidf_216  0.002669\n",
       "220     tfidf_220  0.002655\n",
       "437         grape  0.002622\n",
       "37       tfidf_37  0.002575\n",
       "166     tfidf_166  0.002547\n",
       "161     tfidf_161  0.002529\n",
       "337     tfidf_337  0.002520\n",
       "35       tfidf_35  0.002505\n",
       "286     tfidf_286  0.002495\n",
       "424         berry  0.002484\n",
       "30       tfidf_30  0.002479\n",
       "290     tfidf_290  0.002446\n",
       "126     tfidf_126  0.002409\n",
       "419        tingly  0.002388\n",
       "342     tfidf_342  0.002306\n",
       "405       focused  0.002305\n",
       "157     tfidf_157  0.002263\n",
       "251     tfidf_251  0.002186\n",
       "194     tfidf_194  0.002174\n",
       "5         tfidf_5  0.002139\n",
       "272     tfidf_272  0.002125\n",
       "323     tfidf_323  0.002113\n",
       "51       tfidf_51  0.002085\n",
       "34       tfidf_34  0.002073\n",
       "395      creative  0.002039\n",
       "45       tfidf_45  0.002035\n",
       "20       tfidf_20  0.002025\n",
       "454       pungent  0.002005\n",
       "406        giggly  0.001977\n",
       "303     tfidf_303  0.001974\n",
       "340     tfidf_340  0.001949\n",
       "115     tfidf_115  0.001945\n",
       "409        hungry  0.001928\n",
       "78       tfidf_78  0.001915\n",
       "248     tfidf_248  0.001890\n",
       "46       tfidf_46  0.001866\n",
       "270     tfidf_270  0.001862\n",
       "321     tfidf_321  0.001860\n",
       "123     tfidf_123  0.001855\n",
       "90       tfidf_90  0.001853\n",
       "291     tfidf_291  0.001844\n",
       "258     tfidf_258  0.001841\n",
       "32       tfidf_32  0.001809\n",
       "418     talkative  0.001806\n",
       "468         woody  0.001779\n",
       "154     tfidf_154  0.001769\n",
       "235     tfidf_235  0.001756\n",
       "209     tfidf_209  0.001754\n",
       "7         tfidf_7  0.001753\n",
       "283     tfidf_283  0.001747\n",
       "336     tfidf_336  0.001715\n",
       "372     tfidf_372  0.001702\n",
       "382     tfidf_382  0.001688\n",
       "366     tfidf_366  0.001673\n",
       "19       tfidf_19  0.001643\n",
       "371     tfidf_371  0.001632\n",
       "25       tfidf_25  0.001584\n",
       "96       tfidf_96  0.001577\n",
       "205     tfidf_205  0.001563\n",
       "367     tfidf_367  0.001547\n",
       "300     tfidf_300  0.001533\n",
       "173     tfidf_173  0.001527\n",
       "415        sleepy  0.001524\n",
       "249     tfidf_249  0.001522\n",
       "458  spicy/herbal  0.001520\n",
       "110     tfidf_110  0.001494\n",
       "61       tfidf_61  0.001490\n",
       "221     tfidf_221  0.001487\n",
       "277     tfidf_277  0.001476\n",
       "186     tfidf_186  0.001465\n",
       "21       tfidf_21  0.001448\n",
       "314     tfidf_314  0.001445\n",
       "9         tfidf_9  0.001444\n",
       "232     tfidf_232  0.001437\n",
       "338     tfidf_338  0.001428\n",
       "264     tfidf_264  0.001428\n",
       "319     tfidf_319  0.001422\n",
       "350     tfidf_350  0.001421\n",
       "150     tfidf_150  0.001379\n",
       "451          pine  0.001357\n",
       "225     tfidf_225  0.001356\n",
       "164     tfidf_164  0.001345\n",
       "54       tfidf_54  0.001341\n",
       "217     tfidf_217  0.001328\n",
       "294     tfidf_294  0.001326\n",
       "99       tfidf_99  0.001319\n",
       "263     tfidf_263  0.001312\n",
       "353     tfidf_353  0.001296\n",
       "368     tfidf_368  0.001269\n",
       "41       tfidf_41  0.001268\n",
       "198     tfidf_198  0.001259\n",
       "465      tropical  0.001255\n",
       "163     tfidf_163  0.001252\n",
       "2         tfidf_2  0.001247\n",
       "369     tfidf_369  0.001244\n",
       "215     tfidf_215  0.001239\n",
       "316     tfidf_316  0.001238\n",
       "278     tfidf_278  0.001224\n",
       "265     tfidf_265  0.001203\n",
       "94       tfidf_94  0.001202\n",
       "343     tfidf_343  0.001184\n",
       "208     tfidf_208  0.001176\n",
       "289     tfidf_289  0.001169\n",
       "98       tfidf_98  0.001168\n",
       "101     tfidf_101  0.001163\n",
       "381     tfidf_381  0.001157\n",
       "26       tfidf_26  0.001154\n",
       "81       tfidf_81  0.001153\n",
       "190     tfidf_190  0.001152\n",
       "117     tfidf_117  0.001139\n",
       "103     tfidf_103  0.001136\n",
       "421       ammonia  0.001130\n",
       "0         tfidf_0  0.001125\n",
       "192     tfidf_192  0.001123\n",
       "178     tfidf_178  0.001121\n",
       "219     tfidf_219  0.001108\n",
       "243     tfidf_243  0.001107\n",
       "373     tfidf_373  0.001091\n",
       "240     tfidf_240  0.001091\n",
       "344     tfidf_344  0.001087\n",
       "183     tfidf_183  0.001086\n",
       "38       tfidf_38  0.001067\n",
       "88       tfidf_88  0.001064\n",
       "297     tfidf_297  0.001060\n",
       "398      dry eyes  0.001059\n",
       "75       tfidf_75  0.001057\n",
       "435       flowery  0.001054\n",
       "379     tfidf_379  0.001039\n",
       "48       tfidf_48  0.001037\n",
       "17       tfidf_17  0.001028\n",
       "260     tfidf_260  0.001027\n",
       "385     tfidf_385  0.001026\n",
       "130     tfidf_130  0.001014\n",
       "237     tfidf_237  0.001014\n",
       "308     tfidf_308  0.001014\n",
       "328     tfidf_328  0.001013\n",
       "429      chemical  0.001003\n",
       "140     tfidf_140  0.000996\n",
       "246     tfidf_246  0.000985\n",
       "271     tfidf_271  0.000980\n",
       "206     tfidf_206  0.000977\n",
       "52       tfidf_52  0.000977\n",
       "53       tfidf_53  0.000974\n",
       "348     tfidf_348  0.000972\n",
       "12       tfidf_12  0.000968\n",
       "107     tfidf_107  0.000955\n",
       "244     tfidf_244  0.000953\n",
       "267     tfidf_267  0.000952\n",
       "86       tfidf_86  0.000947\n",
       "97       tfidf_97  0.000933\n",
       "443         mango  0.000920\n",
       "276     tfidf_276  0.000915\n",
       "360     tfidf_360  0.000907\n",
       "80       tfidf_80  0.000904\n",
       "305     tfidf_305  0.000903\n",
       "440      lavender  0.000902\n",
       "106     tfidf_106  0.000900\n",
       "4         tfidf_4  0.000900\n",
       "129     tfidf_129  0.000886\n",
       "255     tfidf_255  0.000886\n",
       "113     tfidf_113  0.000881\n",
       "332     tfidf_332  0.000881\n",
       "432        coffee  0.000877\n",
       "295     tfidf_295  0.000874\n",
       "374     tfidf_374  0.000865\n",
       "69       tfidf_69  0.000860\n",
       "170     tfidf_170  0.000851\n",
       "44       tfidf_44  0.000848\n",
       "104     tfidf_104  0.000843\n",
       "169     tfidf_169  0.000842\n",
       "135     tfidf_135  0.000839\n",
       "22       tfidf_22  0.000823\n",
       "118     tfidf_118  0.000814\n",
       "83       tfidf_83  0.000807\n",
       "380     tfidf_380  0.000805\n",
       "59       tfidf_59  0.000800\n",
       "231     tfidf_231  0.000796\n",
       "125     tfidf_125  0.000791\n",
       "266     tfidf_266  0.000784\n",
       "393       aroused  0.000782\n",
       "361     tfidf_361  0.000777\n",
       "100     tfidf_100  0.000776\n",
       "175     tfidf_175  0.000762\n",
       "331     tfidf_331  0.000759\n",
       "363     tfidf_363  0.000758\n",
       "127     tfidf_127  0.000756\n",
       "334     tfidf_334  0.000750\n",
       "341     tfidf_341  0.000750\n",
       "463       tobacco  0.000750\n",
       "326     tfidf_326  0.000745\n",
       "442          lime  0.000744\n",
       "386     tfidf_386  0.000740\n",
       "256     tfidf_256  0.000739\n",
       "461           tar  0.000735\n",
       "146     tfidf_146  0.000734\n",
       "152     tfidf_152  0.000719\n",
       "15       tfidf_15  0.000709\n",
       "211     tfidf_211  0.000708\n",
       "457         skunk  0.000706\n",
       "275     tfidf_275  0.000704\n",
       "387     tfidf_387  0.000698\n",
       "189     tfidf_189  0.000698\n",
       "193     tfidf_193  0.000696\n",
       "8         tfidf_8  0.000696\n",
       "317     tfidf_317  0.000688\n",
       "10       tfidf_10  0.000683\n",
       "299     tfidf_299  0.000683\n",
       "1         tfidf_1  0.000681\n",
       "133     tfidf_133  0.000676\n",
       "224     tfidf_224  0.000672\n",
       "397         dizzy  0.000663\n",
       "408      headache  0.000663\n",
       "122     tfidf_122  0.000661\n",
       "85       tfidf_85  0.000654\n",
       "301     tfidf_301  0.000652\n",
       "356     tfidf_356  0.000650\n",
       "64       tfidf_64  0.000648\n",
       "298     tfidf_298  0.000645\n",
       "228     tfidf_228  0.000644\n",
       "39       tfidf_39  0.000642\n",
       "156     tfidf_156  0.000642\n",
       "28       tfidf_28  0.000639\n",
       "412      paranoid  0.000637\n",
       "203     tfidf_203  0.000633\n",
       "56       tfidf_56  0.000628\n",
       "238     tfidf_238  0.000623\n",
       "131     tfidf_131  0.000622\n",
       "132     tfidf_132  0.000621\n",
       "139     tfidf_139  0.000613\n",
       "67       tfidf_67  0.000612\n",
       "137     tfidf_137  0.000608\n",
       "445          mint  0.000606\n",
       "346     tfidf_346  0.000606\n",
       "313     tfidf_313  0.000606\n",
       "318     tfidf_318  0.000601\n",
       "233     tfidf_233  0.000598\n",
       "109     tfidf_109  0.000592\n",
       "29       tfidf_29  0.000589\n",
       "142     tfidf_142  0.000588\n",
       "257     tfidf_257  0.000581\n",
       "171     tfidf_171  0.000581\n",
       "288     tfidf_288  0.000573\n",
       "50       tfidf_50  0.000571\n",
       "71       tfidf_71  0.000569\n",
       "120     tfidf_120  0.000566\n",
       "184     tfidf_184  0.000565\n",
       "364     tfidf_364  0.000563\n",
       "226     tfidf_226  0.000560\n",
       "159     tfidf_159  0.000551\n",
       "143     tfidf_143  0.000541\n",
       "112     tfidf_112  0.000537\n",
       "177     tfidf_177  0.000527\n",
       "234     tfidf_234  0.000525\n",
       "195     tfidf_195  0.000521\n",
       "160     tfidf_160  0.000514\n",
       "354     tfidf_354  0.000512\n",
       "352     tfidf_352  0.000512\n",
       "375     tfidf_375  0.000503\n",
       "76       tfidf_76  0.000500\n",
       "16       tfidf_16  0.000491\n",
       "185     tfidf_185  0.000491\n",
       "273     tfidf_273  0.000486\n",
       "165     tfidf_165  0.000485\n",
       "57       tfidf_57  0.000478\n",
       "322     tfidf_322  0.000477\n",
       "172     tfidf_172  0.000476\n",
       "247     tfidf_247  0.000474\n",
       "310     tfidf_310  0.000473\n",
       "446         nutty  0.000469\n",
       "63       tfidf_63  0.000465\n",
       "456          sage  0.000465\n",
       "24       tfidf_24  0.000457\n",
       "147     tfidf_147  0.000456\n",
       "450        pepper  0.000455\n",
       "148     tfidf_148  0.000453\n",
       "370     tfidf_370  0.000453\n",
       "351     tfidf_351  0.000451\n",
       "27       tfidf_27  0.000448\n",
       "74       tfidf_74  0.000446\n",
       "269     tfidf_269  0.000445\n",
       "268     tfidf_268  0.000444\n",
       "31       tfidf_31  0.000442\n",
       "47       tfidf_47  0.000437\n",
       "153     tfidf_153  0.000437\n",
       "214     tfidf_214  0.000430\n",
       "262     tfidf_262  0.000425\n",
       "254     tfidf_254  0.000422\n",
       "66       tfidf_66  0.000421\n",
       "84       tfidf_84  0.000415\n",
       "227     tfidf_227  0.000413\n",
       "320     tfidf_320  0.000412\n",
       "204     tfidf_204  0.000404\n",
       "213     tfidf_213  0.000398\n",
       "222     tfidf_222  0.000390\n",
       "111     tfidf_111  0.000385\n",
       "33       tfidf_33  0.000377\n",
       "134     tfidf_134  0.000376\n",
       "105     tfidf_105  0.000375\n",
       "182     tfidf_182  0.000375\n",
       "91       tfidf_91  0.000374\n",
       "304     tfidf_304  0.000368\n",
       "307     tfidf_307  0.000367\n",
       "384     tfidf_384  0.000367\n",
       "242     tfidf_242  0.000365\n",
       "365     tfidf_365  0.000363\n",
       "116     tfidf_116  0.000363\n",
       "287     tfidf_287  0.000359\n",
       "102     tfidf_102  0.000358\n",
       "13       tfidf_13  0.000349\n",
       "23       tfidf_23  0.000343\n",
       "138     tfidf_138  0.000340\n",
       "92       tfidf_92  0.000337\n",
       "180     tfidf_180  0.000336\n",
       "155     tfidf_155  0.000336\n",
       "293     tfidf_293  0.000335\n",
       "436         fruit  0.000335\n",
       "89       tfidf_89  0.000333\n",
       "274     tfidf_274  0.000332\n",
       "79       tfidf_79  0.000330\n",
       "292     tfidf_292  0.000326\n",
       "464          tree  0.000326\n",
       "392       anxious  0.000318\n",
       "259     tfidf_259  0.000316\n",
       "377     tfidf_377  0.000312\n",
       "108     tfidf_108  0.000310\n",
       "95       tfidf_95  0.000308\n",
       "236     tfidf_236  0.000306\n",
       "179     tfidf_179  0.000305\n",
       "188     tfidf_188  0.000305\n",
       "40       tfidf_40  0.000302\n",
       "18       tfidf_18  0.000300\n",
       "174     tfidf_174  0.000295\n",
       "3         tfidf_3  0.000293\n",
       "282     tfidf_282  0.000293\n",
       "201     tfidf_201  0.000293\n",
       "62       tfidf_62  0.000290\n",
       "202     tfidf_202  0.000285\n",
       "347     tfidf_347  0.000284\n",
       "359     tfidf_359  0.000282\n",
       "284     tfidf_284  0.000280\n",
       "315     tfidf_315  0.000279\n",
       "335     tfidf_335  0.000275\n",
       "241     tfidf_241  0.000273\n",
       "87       tfidf_87  0.000267\n",
       "70       tfidf_70  0.000267\n",
       "68       tfidf_68  0.000264\n",
       "330     tfidf_330  0.000256\n",
       "136     tfidf_136  0.000256\n",
       "77       tfidf_77  0.000254\n",
       "339     tfidf_339  0.000249\n",
       "49       tfidf_49  0.000245\n",
       "212     tfidf_212  0.000243\n",
       "65       tfidf_65  0.000240\n",
       "378     tfidf_378  0.000239\n",
       "327     tfidf_327  0.000238\n",
       "181     tfidf_181  0.000235\n",
       "176     tfidf_176  0.000232\n",
       "311     tfidf_311  0.000228\n",
       "280     tfidf_280  0.000224\n",
       "55       tfidf_55  0.000222\n",
       "302     tfidf_302  0.000220\n",
       "430      chestnut  0.000212\n",
       "423       apricot  0.000208\n",
       "296     tfidf_296  0.000207\n",
       "72       tfidf_72  0.000205\n",
       "306     tfidf_306  0.000203\n",
       "349     tfidf_349  0.000200\n",
       "14       tfidf_14  0.000197\n",
       "462           tea  0.000194\n",
       "455          rose  0.000193\n",
       "252     tfidf_252  0.000185\n",
       "187     tfidf_187  0.000184\n",
       "196     tfidf_196  0.000182\n",
       "358     tfidf_358  0.000182\n",
       "229     tfidf_229  0.000178\n",
       "261     tfidf_261  0.000177\n",
       "467        violet  0.000175\n",
       "448         peach  0.000156\n",
       "422         apple  0.000148\n",
       "114     tfidf_114  0.000146\n",
       "466       vanilla  0.000145\n",
       "218     tfidf_218  0.000141\n",
       "383     tfidf_383  0.000140\n",
       "191     tfidf_191  0.000140\n",
       "333     tfidf_333  0.000133\n",
       "438    grapefruit  0.000118\n",
       "42       tfidf_42  0.000113\n",
       "60       tfidf_60  0.000110\n",
       "250     tfidf_250  0.000102\n",
       "439         honey  0.000098\n",
       "453          plum  0.000098\n",
       "452     pineapple  0.000093\n",
       "324     tfidf_324  0.000082\n",
       "427        butter  0.000077\n",
       "444       menthol  0.000064\n",
       "425   blue cheese  0.000061\n",
       "279     tfidf_279  0.000046\n",
       "391       anxiety  0.000033\n",
       "410     migraines  0.000026\n",
       "396    depression  0.000024\n",
       "449          pear  0.000013\n",
       "403  eye pressure  0.000000\n",
       "394     arthritis  0.000000\n",
       "417        stress  0.000000\n",
       "416    spasticity  0.000000\n",
       "414      seizures  0.000000\n",
       "411          pain  0.000000\n",
       "401      epilepsy  0.000000\n",
       "404       fatigue  0.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', df_feat_ranked.shape[0]+1)\n",
    "df_feat_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py:357: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "selector = SelectFromModel(rfreg).fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.13185352e-03, 7.67493365e-04, 1.20554132e-03, 3.63387499e-04,\n",
       "       1.01028440e-03, 2.31863716e-03, 3.14092621e-03, 1.61936255e-03,\n",
       "       6.21800720e-04, 1.45484069e-03, 7.89348834e-04, 1.20348766e-02,\n",
       "       9.40060006e-04, 3.01206256e-04, 2.44897386e-04, 6.36686636e-04,\n",
       "       5.12133810e-04, 9.77689092e-04, 3.15734770e-04, 1.87159085e-03,\n",
       "       2.09435043e-03, 1.67023370e-03, 8.33819149e-04, 3.45050062e-04,\n",
       "       5.85272264e-04, 1.55774833e-03, 1.04779969e-03, 4.90769164e-04,\n",
       "       6.47095078e-04, 7.23301850e-04, 2.67255281e-03, 3.87144637e-04,\n",
       "       1.84239356e-03, 3.32586098e-04, 1.89891751e-03, 2.63695674e-03,\n",
       "       2.93219656e-03, 2.71405019e-03, 1.07585036e-03, 8.28187958e-04,\n",
       "       2.67033773e-04, 1.11934189e-03, 8.63240962e-05, 3.89133813e-03,\n",
       "       8.47019551e-04, 2.06476633e-03, 1.91894274e-03, 3.66143923e-04,\n",
       "       1.07489307e-03, 2.80939007e-04, 6.20583141e-04, 2.15050442e-03,\n",
       "       9.41168423e-04, 8.12033583e-04, 1.34428533e-03, 2.16647522e-04,\n",
       "       6.91664951e-04, 4.48421489e-04, 5.13258594e-03, 7.70845312e-04,\n",
       "       1.07104737e-04, 1.40847928e-03, 2.92688187e-04, 4.54333762e-04,\n",
       "       7.51533362e-04, 3.09484134e-04, 4.30861794e-04, 6.16215139e-04,\n",
       "       2.81072277e-04, 8.95742994e-04, 2.19172875e-04, 5.33812617e-04,\n",
       "       1.64238008e-04, 2.84725704e-03, 3.65197750e-04, 1.16774512e-03,\n",
       "       3.64578043e-04, 2.10407343e-04, 1.92672599e-03, 3.11404775e-04,\n",
       "       8.55579667e-04, 1.13964771e-03, 1.12544455e-02, 7.00908499e-04,\n",
       "       3.96317220e-04, 7.54269368e-04, 1.05673478e-03, 3.82990491e-04,\n",
       "       1.16934092e-03, 3.15490600e-04, 2.06667695e-03, 3.52739216e-04,\n",
       "       3.23287235e-04, 4.45180616e-03, 1.35765406e-03, 2.58295259e-04,\n",
       "       1.58340047e-03, 9.24197437e-04, 1.17076366e-03, 1.25748095e-03,\n",
       "       8.96109524e-04, 1.31898619e-03, 4.11433451e-04, 1.16256919e-03,\n",
       "       7.86327073e-04, 3.67544496e-04, 8.01879024e-04, 9.59505739e-04,\n",
       "       2.12318067e-04, 6.23900156e-04, 1.47110447e-03, 4.14099204e-04,\n",
       "       6.24633175e-04, 9.94063210e-04, 1.50914139e-04, 1.88828442e-03,\n",
       "       3.03163427e-04, 1.14625196e-03, 1.08902818e-03, 4.57998382e-03,\n",
       "       5.78359383e-04, 4.56127600e-03, 6.15297735e-04, 1.94643395e-03,\n",
       "       4.09219231e-03, 8.15011098e-04, 2.47290676e-03, 6.85558292e-04,\n",
       "       4.49456713e-03, 1.16787083e-03, 9.34325278e-04, 5.08605255e-04,\n",
       "       6.34741211e-04, 6.95225282e-04, 3.56816327e-04, 8.32748576e-04,\n",
       "       2.21157904e-04, 5.84135138e-04, 3.45990602e-04, 6.22797666e-04,\n",
       "       1.06659641e-03, 4.02308240e-03, 6.28295666e-04, 4.73182566e-04,\n",
       "       3.24732078e-03, 8.37368210e-03, 7.42077664e-04, 6.12447713e-04,\n",
       "       3.98119936e-04, 6.35801010e-03, 1.25518264e-03, 2.99332304e-03,\n",
       "       7.13343270e-04, 4.43944919e-04, 1.99164706e-03, 3.39069647e-04,\n",
       "       7.53331761e-04, 2.17640503e-03, 3.89782768e-03, 5.65050043e-04,\n",
       "       4.60749733e-04, 2.52598896e-03, 8.17240677e-03, 1.35206133e-03,\n",
       "       1.28534126e-03, 5.11142478e-04, 2.37620043e-03, 5.02982524e-03,\n",
       "       8.93609086e-03, 8.64097820e-04, 8.68657131e-04, 4.33028517e-04,\n",
       "       5.46479315e-04, 1.29448734e-03, 2.72741716e-04, 7.22027142e-04,\n",
       "       2.68623533e-04, 4.23159893e-04, 1.18243403e-03, 3.19002502e-04,\n",
       "       3.35208900e-04, 2.40587744e-04, 3.68374565e-04, 1.09080438e-03,\n",
       "       7.05306099e-04, 4.83537901e-04, 1.45365413e-03, 1.86895747e-04,\n",
       "       3.39463507e-04, 6.94963642e-04, 1.16930937e-03, 1.56003410e-04,\n",
       "       1.08019672e-03, 7.63149558e-04, 2.24531410e-03, 5.03912830e-04,\n",
       "       1.48430391e-04, 3.58936296e-03, 1.39409015e-03, 6.32769920e-03,\n",
       "       2.50054610e-03, 2.54481287e-04, 3.07990488e-04, 5.80353959e-04,\n",
       "       4.19473895e-04, 1.48191832e-03, 9.55726732e-04, 4.18598338e-03,\n",
       "       1.12051202e-03, 1.78919871e-03, 4.83836623e-03, 6.40976467e-04,\n",
       "       2.45140438e-04, 1.95147780e-04, 4.41685565e-04, 1.18859116e-03,\n",
       "       2.66656350e-03, 1.34849363e-03, 1.41153042e-04, 1.27682293e-03,\n",
       "       2.50924233e-03, 1.40295911e-03, 3.70271287e-04, 4.60325489e-03,\n",
       "       6.29053452e-04, 1.30541513e-03, 3.91978948e-04, 4.01211986e-04,\n",
       "       6.29337800e-04, 1.63035390e-04, 3.50966452e-03, 9.01313920e-04,\n",
       "       1.36123034e-03, 5.52379374e-04, 5.10362226e-04, 1.82913898e-03,\n",
       "       2.58629932e-04, 7.34858323e-04, 5.90694866e-04, 4.71501930e-03,\n",
       "       1.19172631e-03, 1.89403940e-04, 4.39246438e-04, 9.15747909e-04,\n",
       "       7.93004081e-04, 7.09248679e-03, 1.09234785e-03, 4.65120476e-04,\n",
       "       1.97805450e-03, 1.67402650e-03, 2.01905178e-04, 2.27595654e-03,\n",
       "       2.25595419e-04, 2.70066586e-02, 3.37745113e-04, 8.58995620e-04,\n",
       "       7.29578735e-04, 5.58932755e-04, 1.76792544e-03, 3.34683701e-04,\n",
       "       1.10069273e-03, 2.49726891e-04, 4.39075380e-04, 1.18692029e-03,\n",
       "       1.35102882e-03, 1.20974179e-03, 9.57088794e-04, 1.05295020e-03,\n",
       "       4.66600956e-04, 5.14960445e-04, 1.86781461e-03, 8.58869748e-04,\n",
       "       2.17456701e-03, 6.17982593e-04, 3.38715357e-04, 7.49258188e-04,\n",
       "       9.08303263e-04, 1.52021292e-03, 1.16142964e-03, 4.14929327e-05,\n",
       "       2.31607991e-04, 4.03818735e-03, 2.77244220e-04, 1.63180004e-03,\n",
       "       2.34121254e-04, 8.06640995e-03, 2.59155877e-03, 4.23364132e-04,\n",
       "       5.18395514e-04, 1.14423869e-03, 2.52474911e-03, 1.61812382e-03,\n",
       "       2.44199424e-04, 2.41710724e-04, 1.22046299e-03, 7.69269342e-04,\n",
       "       1.79539856e-04, 1.03323053e-03, 6.33876524e-04, 7.87137884e-04,\n",
       "       1.54107961e-03, 6.78036842e-04, 1.94266568e-04, 1.87247039e-03,\n",
       "       2.73618511e-04, 8.74507655e-04, 2.06083134e-04, 2.79979208e-04,\n",
       "       9.82441832e-04, 4.16853300e-03, 5.48514944e-04, 2.43986403e-04,\n",
       "       8.03723664e-03, 7.56903077e-04, 1.49923066e-03, 2.86405476e-04,\n",
       "       1.18157331e-03, 6.73919428e-04, 6.46904064e-04, 1.34340252e-03,\n",
       "       3.62690075e-04, 2.17382908e-03, 4.16611441e-04, 1.87907345e-03,\n",
       "       9.00471425e-05, 2.76396836e-03, 7.28326442e-04, 3.46189958e-04,\n",
       "       1.12671808e-03, 1.34463087e-02, 2.45477202e-04, 6.11167291e-04,\n",
       "       9.86871625e-04, 1.04989524e-04, 7.24812024e-04, 3.34281498e-04,\n",
       "       1.63904456e-03, 2.57096652e-03, 1.34166422e-03, 2.51047979e-04,\n",
       "       1.99807725e-03, 6.42570847e-04, 2.27929243e-03, 1.21487538e-03,\n",
       "       1.03244809e-03, 6.50185887e-03, 5.77536713e-04, 2.70544914e-04,\n",
       "       9.30586603e-04, 2.06120748e-04, 1.39558057e-03, 4.29084932e-04,\n",
       "       4.98364148e-04, 1.36872782e-03, 5.49208814e-04, 3.09204588e-03,\n",
       "       6.64965042e-04, 3.32127040e-03, 1.89069291e-04, 4.12094948e-04,\n",
       "       7.76594009e-04, 8.93797399e-04, 2.77687010e-03, 5.68642466e-04,\n",
       "       5.97238302e-04, 3.31587416e-04, 1.78251668e-03, 1.48025640e-03,\n",
       "       1.26752356e-03, 1.23769757e-03, 5.00476859e-04, 1.78928196e-03,\n",
       "       1.80339280e-03, 1.16254042e-03, 8.92504798e-04, 4.94786840e-04,\n",
       "       3.19410983e-03, 2.88993796e-04, 3.30675843e-04, 1.02761110e-03,\n",
       "       9.10719358e-04, 1.00941211e-03, 1.47415444e-03, 1.30061923e-04,\n",
       "       4.08012235e-04, 1.10165716e-03, 7.25381928e-04, 5.83872350e-04,\n",
       "       5.04241121e-02, 6.74261652e-02, 1.86237139e-02, 2.56645615e-05,\n",
       "       3.17184564e-04, 7.64494879e-04, 0.00000000e+00, 2.14040187e-03,\n",
       "       2.08080175e-05, 6.55960705e-04, 9.97544369e-04, 2.94253742e-03,\n",
       "       4.02913605e-03, 0.00000000e+00, 5.50574711e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.20667232e-03, 1.94797868e-03, 4.41218320e-03,\n",
       "       5.91532548e-04, 2.16222897e-03, 3.02172916e-05, 0.00000000e+00,\n",
       "       7.49798870e-04, 4.97348602e-03, 0.00000000e+00, 1.41813823e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.59039197e-03, 2.25679360e-03,\n",
       "       3.21771912e-03, 1.08723613e-03, 6.41585740e-05, 9.83810674e-05,\n",
       "       2.37383948e-03, 3.27479203e-05, 4.27446677e-03, 6.17366240e-05,\n",
       "       2.98257252e-03, 9.73337965e-04, 2.64639068e-04, 3.46309394e-03,\n",
       "       8.80759600e-04, 1.41672369e-01, 9.05796863e-03, 9.91548497e-04,\n",
       "       3.94477379e-04, 2.79730638e-03, 1.01735012e-04, 9.68633031e-05,\n",
       "       9.20882879e-04, 7.06737448e-02, 7.82749180e-04, 9.64200184e-04,\n",
       "       5.53348851e-05, 6.96701526e-04, 3.88784473e-04, 4.01160681e-03,\n",
       "       1.36129832e-04, 3.00272004e-06, 4.56176796e-04, 1.11987767e-03,\n",
       "       1.12350350e-04, 1.18873707e-04, 2.00107830e-03, 2.47294617e-04,\n",
       "       4.61536515e-04, 9.85514448e-04, 1.47765215e-03, 7.24211992e-03,\n",
       "       4.87377166e-03, 7.47485849e-04, 2.04382920e-04, 6.89123776e-04,\n",
       "       2.03361866e-04, 1.11939714e-03, 1.11834731e-04, 1.09059157e-04,\n",
       "       1.59321045e-03])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021321961620469083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True,  True, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False, False,  True, False,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "       False, False, False, False,  True,  True, False, False,  True,\n",
       "        True, False, False, False,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False,  True,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "        True, False, False, False,  True, False, False,  True, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True,  True, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False,  True, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False,  True, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False, False,  True,\n",
       "       False, False, False,  True,  True, False,  True, False, False,\n",
       "        True, False,  True, False,  True, False, False, False,  True,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "       False,  True, False,  True, False,  True, False, False,  True,\n",
       "       False,  True,  True, False, False,  True, False, False, False,\n",
       "        True, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_11</th>\n",
       "      <th>tfidf_30</th>\n",
       "      <th>tfidf_35</th>\n",
       "      <th>tfidf_36</th>\n",
       "      <th>tfidf_37</th>\n",
       "      <th>tfidf_43</th>\n",
       "      <th>tfidf_51</th>\n",
       "      <th>tfidf_58</th>\n",
       "      <th>...</th>\n",
       "      <th>blueberry</th>\n",
       "      <th>cheese</th>\n",
       "      <th>citrus</th>\n",
       "      <th>diesel</th>\n",
       "      <th>earthy</th>\n",
       "      <th>grape</th>\n",
       "      <th>lemon</th>\n",
       "      <th>orange</th>\n",
       "      <th>strawberry</th>\n",
       "      <th>sweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tfidf_5  tfidf_6  tfidf_11  tfidf_30  tfidf_35  tfidf_36  tfidf_37  \\\n",
       "0      0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "1      0.145484      0.0       0.0       0.0  0.213037       0.0       0.0   \n",
       "2      0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "3      0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "4      0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "...         ...      ...       ...       ...       ...       ...       ...   \n",
       "74995  0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "74996  0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "74997  0.000000      0.0       0.0       0.0  0.182564       0.0       0.0   \n",
       "74998  0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "74999  0.000000      0.0       0.0       0.0  0.000000       0.0       0.0   \n",
       "\n",
       "       tfidf_43  tfidf_51  tfidf_58  ...  blueberry  cheese  citrus  diesel  \\\n",
       "0      0.198545       0.0       0.0  ...          0       1       0       0   \n",
       "1      0.000000       0.0       0.0  ...          0       1       0       0   \n",
       "2      0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "3      0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "4      0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "...         ...       ...       ...  ...        ...     ...     ...     ...   \n",
       "74995  0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "74996  0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "74997  0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "74998  0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "74999  0.000000       0.0       0.0  ...          0       0       0       0   \n",
       "\n",
       "       earthy  grape  lemon  orange  strawberry  sweet  \n",
       "0           0      0      0       0           0      1  \n",
       "1           0      1      0       0           0      0  \n",
       "2           0      0      0       0           0      0  \n",
       "3           0      0      0       0           0      0  \n",
       "4           0      0      0       0           0      0  \n",
       "...       ...    ...    ...     ...         ...    ...  \n",
       "74995       0      0      0       0           0      0  \n",
       "74996       0      0      0       0           0      0  \n",
       "74997       0      0      0       0           0      0  \n",
       "74998       0      0      0       0           0      0  \n",
       "74999       0      0      0       0           0      0  \n",
       "\n",
       "[75000 rows x 85 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_X = df_rf[selected_features]\n",
    "selected_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split (after Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['selected_X_rf_tfidf_euca.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(selector, \"selector_rf_tfidf_euca.pkl\")\n",
    "joblib.dump(selected_X, \"selected_X_rf_tfidf_euca.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selected_X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_9602/3758305.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg.fit(X_train1, y_train1)\n"
     ]
    }
   ],
   "source": [
    "rfreg.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028087505418454537"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009585083147977128"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09790343787619069"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979305339495647"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241105439595334"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [None, 10, 50, 100],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [100, 300, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv = RandomizedSearchCV(rfreg,  \n",
    "                     parameters,   \n",
    "                     cv=5, \n",
    "                     scoring='neg_mean_absolute_error',\n",
    "                     n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [None, 10, 50, 100],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 300, 500]},\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [None, 10, 50, 100],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 300, 500]},\n",
       "                   scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 100}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rscv_rf_tfidf_best_params_euca.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rscv, \"rscv_rf_tfidf_euca.pkl\")\n",
    "joblib.dump(rscv.best_params_, \"rscv_rf_tfidf_best_params_euca.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF (after Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pjvjlkjn5gl846rnyzr53p340000gn/T/ipykernel_9602/1949321143.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfreg_ht.fit(X_train1, y_train1)\n",
      "/Users/andalanputra/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "rfreg_ht = RandomForestRegressor(n_estimators = 100, min_samples_split = 2, min_samples_leaf = 1, max_features = 'auto', max_depth = 100)\n",
    "rfreg_ht.fit(X_train1, y_train1)\n",
    "y_pred_rfreg = rfreg_ht.predict(X_val)\n",
    "y_pred_rfreg_r2 = rfreg_ht.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028061839238556947"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009525744767279295"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09759992196348978"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred_rfreg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979299060947347"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "r2_score(y_train1, y_pred_rfreg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245803528661396"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "r2_score(y_val, y_pred_rfreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual plots after Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfreg_test = rfreg_ht.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_rfreg_tfidf_euca.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_pred_rfreg_test, \"y_pred_rfreg_test_tfidf_euca.pkl\")\n",
    "joblib.dump(y_test, \"y_test_rfreg_tfidf_euca.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026537717853275026"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008833536604313174"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09398689591806494"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred_rfreg_test, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9296677597515033"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred_rfreg_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assume y_pred is a numpy array and y_true is a pandas dataframe\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "column = \"X..Eucalyptol\"  # specify the target variable name\n",
    "ax.hist(y_pred_rfreg_test, alpha=0.5, label='y_pred', bins=20)\n",
    "ax.hist(y_test[column], alpha=0.5, label='y_true', bins=20)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title(column)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('error_hist_knn_tfidf_euca.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.967\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "corr_coef, p_value = pearsonr(y_pred_rfreg_test.flatten(), y_test.values.ravel())\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr_coef:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix = y_test.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
